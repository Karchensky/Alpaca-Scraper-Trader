{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpaca Trading Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data & API packages\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import TimeFrame, TimeFrameUnit\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import ta\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "#modeling packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import tensorflow as tf \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "scikeras.wrappers.KerasClassifier doesn't work?  or, at least it got too annoying to try. Making a custom keras model wrapper so we can do  hyper parameter tuning...\n",
    "\n",
    "\"\"\"\n",
    "class KerasModelWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model_type='LSTM', units=50, layers=2, dropout_rate=0.2, learning_rate=0.001, input_shape=(30, 12), epochs=10, batch_size=32):\n",
    "        self.model_type = model_type\n",
    "        self.units = units\n",
    "        self.layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_shape = input_shape\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self):\n",
    "        sequence_input = tf.keras.Input(shape=self.input_shape)\n",
    "        x = sequence_input\n",
    "        for _ in range(self.layers):\n",
    "            if self.model_type == 'LSTM':\n",
    "                x = tf.keras.layers.LSTM(self.units, return_sequences=True if _ < self.layers - 1 else False)(x)\n",
    "            elif self.model_type == 'RNN':\n",
    "                x = tf.keras.layers.SimpleRNN(self.units, return_sequences=True if _ < self.layers - 1 else False)(x)\n",
    "            elif self.model_type == 'CNN':\n",
    "                x = tf.keras.layers.Conv1D(filters=self.units, kernel_size=3, activation='relu')(x)\n",
    "                x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "            x = tf.keras.layers.Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs=sequence_input, outputs=outputs)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = self.build_model()\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2, callbacks=[early_stopping], verbose=3)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.model.predict(X) > 0.5).astype(\"int32\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return f1_score(y, y_pred, average='weighted')\n",
    "\n",
    "class AlpacaTradingBot:\n",
    "    \"\"\"\n",
    "\n",
    "    This class can be used to scrape stock data from the Alpaca API, store it locally in a SQLite Database,\n",
    "    add indicators & perform predective analytics.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, keys_file_path='alpaca_keys.txt', base_url='https://paper-api.alpaca.markets', database_path=r'D:\\Scripts\\alpaca\\alpaca_algo_trading\\alpaca_data.db'):\n",
    "        with open(keys_file_path, 'r') as file:\n",
    "            self.api_key = file.readline().strip()\n",
    "            self.api_secret = file.readline().strip()\n",
    "        self.base_url = base_url\n",
    "        self.api = tradeapi.REST(self.api_key, self.api_secret, base_url=base_url)\n",
    "        self.database_path = database_path\n",
    "        self.api_call_count = 0\n",
    "\n",
    "    ######################################################  API Scraping methods ######################################################\n",
    "    def download_bar_data(self, stock, timeframe, start_date, end_date, pause=True):\n",
    "        \"\"\"\n",
    "        \n",
    "        This method will be used to scrape bar data from the Alpaca API. \n",
    "        Provide a ticker symbol, an interval, and a beginning and end date.\n",
    "\n",
    "        \"\"\"\n",
    "        all_data = []\n",
    "\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            # Check if it's a weekend (Saturday or Sunday) & skip, if so\n",
    "            # if  datetime.strptime(current_date, \"%Y-%m-%d\").weekday() >= 5:\n",
    "            #     current_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "            #     continue\n",
    "\n",
    "            # Split the day into two segments: midnight to noon, and noon to end of day because the API can only return 1,000 data points a day\n",
    "            segments = [\n",
    "                (f\"{current_date}T00:00:00Z\", f\"{current_date}T11:59:59Z\"),\n",
    "                (f\"{current_date}T12:00:00Z\", f\"{current_date}T23:59:59Z\")\n",
    "            ]\n",
    "\n",
    "            for start_time, end_time in segments:\n",
    "                try:\n",
    "                    # Get data for the current day segment\n",
    "                    bars = self.api.get_bars(stock, timeframe, start=start_time, end=end_time, limit=1000).df\n",
    "                    self.api_call_count += 1  # Increment the API call counter\n",
    "                    if not bars.empty:\n",
    "                        bars['symbol'] = stock  # Add the stock symbol column\n",
    "                        all_data.append(bars)\n",
    "                        print(f\"Data scraped for {current_date} through {end_time.split('T')[1]}...\")\n",
    "                    else:\n",
    "                        print(f\"No data available for segment: {current_date}: {start_time.split('T')[1]} - {end_time.split('T')[1]}\")\n",
    "                except tradeapi.rest.APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    pause_duration = random.uniform(120, 240)  # Longer pause if an API error occurs\n",
    "                    time.sleep(pause_duration)\n",
    "                    continue\n",
    "                \n",
    "            # Random pause after each scrape.. API rate limits need to be considered. Only a problem for the main original pull to populate the database.\n",
    "            if pause and (datetime.strptime(current_date, \"%Y-%m-%d\") - datetime.strptime(start_date, \"%Y-%m-%d\")).days % 1 == 0:\n",
    "                print(f\"Total API calls made so far: {self.api_call_count}\")\n",
    "                pause_duration = random.uniform(10, 30)\n",
    "                print(f\"Pausing for {pause_duration} after scraping data for {current_date}...\")\n",
    "                print(f\"__________________________________\")\n",
    "                time.sleep(pause_duration)\n",
    "\n",
    "            # Move to the next day\n",
    "            current_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        if all_data:\n",
    "            combined_data = pd.concat(all_data)\n",
    "            combined_data = combined_data.reset_index()  # Ensure the index is reset to have 'timestamp' as a column\n",
    "            print(\"Scraping Bar Data complete for timerange:\", combined_data['timestamp'].min(), \" - \", combined_data['timestamp'].max())\n",
    "            return combined_data[['symbol'] + [col for col in combined_data.columns if col not in ['symbol']]]\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    ######################################################  Modeling methods ######################################################     \n",
    "    def sequence_modeling_prep(self, input_table, symbol, output_table=None):\n",
    "        \"\"\"\n",
    "\n",
    "        This method cleans the scraped raw stage data, adds indicator calculations on it inserts the otuput into a fresh table for modeling.\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Read the entire table\n",
    "            input_data = pd.read_sql(f\"SELECT distinct * FROM {input_table} WHERE SYMBOL = '{symbol}'\" , conn)\n",
    "\n",
    "            # Check if the input data is empty\n",
    "            if input_data.empty:\n",
    "                raise ValueError(f\"No data found for symbol {symbol} in table {input_table}\")\n",
    "            \n",
    "            # Convert 'TIMESTAMP' to datetime and localize it to UTC\n",
    "            input_data['timestamp_utc'] = pd.to_datetime(input_data['TIMESTAMP'], utc=True)\n",
    "            input_data['timestamp_est'] = input_data['timestamp_utc'].dt.tz_convert('US/Eastern')\n",
    "            input_data['trading_hours_ind'] = (input_data['timestamp_est'].dt.time >= datetime.strptime('09:30', '%H:%M').time()) & \\\n",
    "                                            (input_data['timestamp_est'].dt.time <= datetime.strptime('16:00', '%H:%M').time())\n",
    "            input_data['trading_hours_ind_ext'] = (input_data['timestamp_est'].dt.time >= datetime.strptime('10:30', '%H:%M').time()) & \\\n",
    "                                                    (input_data['timestamp_est'].dt.time <= datetime.strptime('15:00', '%H:%M').time())\n",
    "            input_data['date'] = input_data['timestamp_est'].dt.date\n",
    "\n",
    "            # Technical Indicators\n",
    "            input_data['rsi'] = ta.momentum.rsi(input_data['CLOSE'], window=60)\n",
    "            input_data['atr'] = ta.volatility.average_true_range(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=60)\n",
    "            input_data['cci'] = ta.trend.cci(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=60)\n",
    "            bb_indicator = ta.volatility.BollingerBands(input_data['CLOSE'], window=60, window_dev=2)\n",
    "            input_data['bollinger_width_percent'] = (bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()) / input_data['CLOSE']\n",
    "            input_data['bollinger_position'] = bb_indicator.bollinger_pband()\n",
    "            input_data['sma_30'] = ta.trend.sma_indicator(input_data['CLOSE'], window=30)\n",
    "            input_data['sma_120'] = ta.trend.sma_indicator(input_data['CLOSE'], window=120)\n",
    "            input_data['ema_30'] = ta.trend.ema_indicator(input_data['CLOSE'], window=30)\n",
    "            input_data['ema_120'] = ta.trend.ema_indicator(input_data['CLOSE'], window=120)\n",
    "            input_data['sma_30_120_pct_diff'] = ((input_data['sma_30'] - input_data['sma_120']) / input_data['sma_120']) * 100\n",
    "            input_data['ema_30_120_pct_diff'] = ((input_data['ema_30'] - input_data['ema_120']) / input_data['ema_120']) * 100\n",
    "            \n",
    "            # Drop some columns and reorder\n",
    "            input_data = input_data.drop(columns=['TIMESTAMP', 'timestamp_utc'])\n",
    "            input_data = input_data[['SYMBOL', 'timestamp_est', 'date', 'trading_hours_ind', 'trading_hours_ind_ext'] + [col for col in input_data.columns if col not in ['SYMBOL', 'timestamp_est', 'date', 'trading_hours_ind', 'trading_hours_ind_ext']]]\n",
    "\n",
    "            # Step 4: Store the final DataFrame into the output table\n",
    "            if output_table is not None:\n",
    "                # Truncate the existing table\n",
    "                self.db_write(f\"DELETE FROM {output_table}\")\n",
    "                print(f\"Truncated table {output_table}.\")\n",
    "\n",
    "                # Store the DataFrame in the output table\n",
    "                self.db_append(output_table, input_data)\n",
    "                print(f\"Prepared sequence model data stored in {output_table}. Number of rows inserted: {len(input_data)}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing symbol {symbol}: {e}\")\n",
    "    \n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        return input_data\n",
    "    \n",
    "    def sequence_modeling_build(self, features, symbol, sequence_window, target_threshold):\n",
    "        \"\"\"\n",
    "\n",
    "        This method runs our LSTM/RNN/CNN models on our prepped-data\n",
    "\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.database_path)\n",
    "        df = pd.read_sql(f\"SELECT * FROM SYMBOL_SEQUENCE_MODELING WHERE SYMBOL = '{symbol}'\", conn)\n",
    "        conn.close()\n",
    "        print(f\"Dataset generated from SYMBOL_SEQUENCE_MODELING table.\")\n",
    "\n",
    "        # Ensure data is sorted by time\n",
    "        df.sort_values('TIMESTAMP_EST', inplace=True)\n",
    "        print(f\"Data sorted by timestamp.\")\n",
    "\n",
    "        # Create new targets\n",
    "        df = self.create_targets(df, target_threshold)\n",
    "        print(f\"Price movement target created for target_threshold of {target_threshold}.\")\n",
    "\n",
    "        # Split data by unique dates\n",
    "        unique_dates = df['DATE'].unique()\n",
    "        train_dates, val_dates = train_test_split(unique_dates, test_size=0.3, random_state=42)\n",
    "        print(f\"Random sets of dates generated to split training & validation.\")\n",
    "\n",
    "        train_data = df[df['DATE'].isin(train_dates)].copy()\n",
    "        val_data = df[df['DATE'].isin(val_dates)].copy()\n",
    "        print(f\"Data split into training & validation.\")\n",
    "\n",
    "        # Create the sequences for each of our features\n",
    "        X_train, y_train, train_targets = self.create_sequences(train_data, features, 'TARGET_MOVEMENT_DIRECTION', int(sequence_window))\n",
    "        X_val, y_val, val_targets = self.create_sequences(val_data, features, 'TARGET_MOVEMENT_DIRECTION', int(sequence_window))\n",
    "        print(f\"Sequences created for training & validation data.\")\n",
    "\n",
    "        # Filter the sequences to include only those where the target row has TRADING_HOURS_IND_EXT == 1\n",
    "        X_train_filtered = X_train[train_targets == 1]\n",
    "        y_train_filtered = y_train[train_targets == 1]\n",
    "        X_val_filtered = X_val[val_targets == 1]\n",
    "        y_val_filtered = y_val[val_targets == 1]\n",
    "        print(f\"Data filtered to trading hours only.\")\n",
    "\n",
    "        # Apply SMOTE\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_train_smoted, y_train_smoted = sm.fit_resample(X_train_filtered.reshape(X_train_filtered.shape[0], -1), y_train_filtered)\n",
    "        X_train_smoted = X_train_smoted.reshape(X_train_smoted.shape[0], *X_train_filtered.shape[1:])\n",
    "        print(f\"Data smoted.\")\n",
    "\n",
    "        # Normalize\n",
    "        scaler = StandardScaler()\n",
    "        X_train_smoted = scaler.fit_transform(X_train_smoted.reshape(-1, X_train_smoted.shape[-1])).reshape(X_train_smoted.shape)\n",
    "        X_val_filtered = scaler.transform(X_val_filtered.reshape(-1, X_val_filtered.shape[-1])).reshape(X_val_filtered.shape)\n",
    "        print(f\"Data normalized. Modeling data is prepared in full. \")\n",
    "\n",
    "        # Define the parameter grid for RandomizedSearchCV\n",
    "        model_types = ['LSTM', 'RNN', 'CNN']\n",
    "        for model_type in model_types:\n",
    "            print(f\"Starting grid search for {model_type} model.\")\n",
    "            param_grid = {\n",
    "                'model_type': [model_type],\n",
    "                'units': [50, 100, 150],\n",
    "                'learning_rate': [0.001, 0.01, 0.1],\n",
    "                'batch_size': [32, 64, 128],\n",
    "                'epochs': [10, 20, 30],\n",
    "                'input_shape': [X_train_filtered.shape[1:]]\n",
    "            }\n",
    "\n",
    "            # Create the KerasClassifier wrapper\n",
    "            model = KerasModelWrapper()\n",
    "\n",
    "            # Perform hyperparameter tuning using RandomizedSearchCV\n",
    "            grid_search = RandomizedSearchCV(\n",
    "                estimator=model,\n",
    "                param_distributions=param_grid,\n",
    "                n_iter=20,\n",
    "                cv=3,\n",
    "                n_jobs=-1,\n",
    "                scoring='f1_weighted',\n",
    "                random_state=42,\n",
    "                verbose=3\n",
    "            )\n",
    "\n",
    "            # Fit the model\n",
    "            grid_search.fit(X_train_smoted, y_train_smoted)\n",
    "\n",
    "            # Evaluate the best model\n",
    "            best_model = grid_search.best_estimator_\n",
    "            self.evaluate_model(best_model, model_type, X_val_filtered, y_val_filtered, symbol, features)\n",
    "            print(f\"Completed training for {model_type} model\\n\")\n",
    "            \n",
    "    def create_targets(self, input_data, threshold=0.001):\n",
    "        \"\"\"\n",
    "\n",
    "        This method creates our target variable. We want to see which direction the stock is going to break first.\n",
    "\n",
    "        \"\"\"\n",
    "        input_data['TARGET_MOVEMENT_DIRECTION'] = np.nan\n",
    "\n",
    "        for i in range(len(input_data)):\n",
    "            for j in range(i + 1, len(input_data)):\n",
    "                price_diff = input_data['CLOSE'].iloc[j] - input_data['CLOSE'].iloc[i]\n",
    "                price_change_pct = price_diff / input_data['CLOSE'].iloc[i]\n",
    "\n",
    "                if price_change_pct >= threshold:\n",
    "                    input_data.at[i, 'TARGET_MOVEMENT_DIRECTION'] = 1  # Up\n",
    "                    break\n",
    "                elif price_change_pct <= -threshold:\n",
    "                    input_data.at[i, 'TARGET_MOVEMENT_DIRECTION'] = 0  # Down\n",
    "                    break\n",
    "\n",
    "        input_data.dropna(subset=['TARGET_MOVEMENT_DIRECTION'], inplace=True)\n",
    "        input_data['TARGET_MOVEMENT_DIRECTION'] = input_data['TARGET_MOVEMENT_DIRECTION'].astype(int)\n",
    "        return input_data\n",
    "    \n",
    "    # Support Function to create sequences\n",
    "    def create_sequences(self, df, feature_columns, target_column, sequence_length):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        trading_hours = []\n",
    "        for i in range(len(df) - sequence_length):\n",
    "            seq_features = df[feature_columns].iloc[i:i+sequence_length].values\n",
    "            seq_target = df[target_column].iloc[i + sequence_length]\n",
    "            seq_trading_hours = df['TRADING_HOURS_IND_EXT'].iloc[i + sequence_length]  # The trading hours indicator for the target row\n",
    "            sequences.append(seq_features)\n",
    "            targets.append(seq_target)\n",
    "            trading_hours.append(seq_trading_hours)\n",
    "        return np.array(sequences), np.array(targets), np.array(trading_hours)\n",
    "    \n",
    "    # Model evaluation\n",
    "    def evaluate_model(self, model, model_name, X_val, y_val, symbol, features):\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
    "        f1 = f1_score(y_val, y_pred_classes, average='weighted', zero_division=0)\n",
    "        cm = confusion_matrix(y_val, y_pred_classes)\n",
    "        cr = classification_report(y_val, y_pred_classes, zero_division=0)\n",
    "\n",
    "        print(f\"{model_name} - F1 Score: {f1}\")\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "        print(\"Classification Report:\\n\", cr)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.title(f'Confusion Matrix: {model_name}')\n",
    "        plt.show()\n",
    "        print(f\"__________________________________\")\n",
    "\n",
    "        # Save the model with a unique filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        if model_name in ['LSTM', 'RNN', 'CNN']:\n",
    "            model.model.save(f'{symbol}_{model_name}_{timestamp}.keras')\n",
    "        else:\n",
    "            model.model.save(f'{symbol}_{model_name}_{timestamp}.pkl')\n",
    "            \n",
    "        # Log results to the database\n",
    "        evaluation_results = {\n",
    "            'SYMBOL': symbol,\n",
    "            'MODEL_NAME': model_name,\n",
    "            'F1_SCORE': f1,\n",
    "            'CONFUSION_MATRIX': str(cm),\n",
    "            'CLASSIFICATION_REPORT': cr,\n",
    "            'BEST_PARAMS': str(model.get_params())\n",
    "        }\n",
    "        results_df = pd.DataFrame([evaluation_results])\n",
    "        self.db_append('CLASSIFICATION_MODEL_RESULTS', results_df)\n",
    "\n",
    "        # Feature Importance or Coefficients\n",
    "        if hasattr(model, \"feature_importances_\"):\n",
    "            importances = model.feature_importances_\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'feature': features,\n",
    "                'importance': importances\n",
    "            }).sort_values(by='importance', ascending=False)\n",
    "            print(f\"Feature importances for {model_name}:\")\n",
    "            print(feature_importance_df)\n",
    "        elif hasattr(model, \"coef_\"):\n",
    "            coefs = model.coef_[0]\n",
    "            coef_df = pd.DataFrame({\n",
    "                'feature': features,\n",
    "                'coefficient': coefs\n",
    "            }).sort_values(by='coefficient', ascending=False)\n",
    "            print(f\"Coefficients for {model_name}:\")\n",
    "            print(coef_df)\n",
    "        else:\n",
    "            print(f\"No feature importance or coefficients for {model_name}\")\n",
    "        print(f\"____________________________________________\")\n",
    "        print(f\"____________________________________________\")\n",
    "\n",
    "    ######################################################  (Old) Modeling methods.. these didn't produce great results ######################################################  \n",
    "    def classification_modeling_prep(self, input_table, symbol, output_table=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        This method cleans the scraped raw stage data, adds indicator calculations on it inserts the otuput into a fresh table.\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Read the entire table\n",
    "            input_data = pd.read_sql(f\"SELECT distinct * FROM {input_table} WHERE SYMBOL = '{symbol}'\" , conn)\n",
    "\n",
    "            # Check if the input data is empty\n",
    "            if input_data.empty:\n",
    "                raise ValueError(f\"No data found for symbol {symbol} in table {input_table}\")\n",
    "\n",
    "            # Convert 'TIMESTAMP' to datetime and localize it to UTC\n",
    "            input_data['timestamp_utc'] = pd.to_datetime(input_data['TIMESTAMP'], utc=True)\n",
    "            input_data['timestamp_est'] = input_data['timestamp_utc'].dt.tz_convert('US/Eastern')\n",
    "            input_data['trading_hours_ind'] = (input_data['timestamp_est'].dt.time >= datetime.strptime('09:30', '%H:%M').time()) & \\\n",
    "                                            (input_data['timestamp_est'].dt.time <= datetime.strptime('16:00', '%H:%M').time())\n",
    "            input_data['trading_hours_ind_ext'] = (input_data['timestamp_est'].dt.time >= datetime.strptime('10:30', '%H:%M').time()) & \\\n",
    "                                                    (input_data['timestamp_est'].dt.time <= datetime.strptime('15:00', '%H:%M').time())\n",
    "\n",
    "            # Calculate indicators\n",
    "            # Trend Indicators\n",
    "            input_data['sma_15'] = ta.trend.sma_indicator(input_data['CLOSE'], window=15)\n",
    "            input_data['sma_30'] = ta.trend.sma_indicator(input_data['CLOSE'], window=30)\n",
    "            input_data['sma_120'] = ta.trend.sma_indicator(input_data['CLOSE'], window=120)\n",
    "            input_data['ema_15'] = ta.trend.ema_indicator(input_data['CLOSE'], window=15)\n",
    "            input_data['ema_30'] = ta.trend.ema_indicator(input_data['CLOSE'], window=30)\n",
    "            input_data['ema_120'] = ta.trend.ema_indicator(input_data['CLOSE'], window=120)\n",
    "            input_data['sma_15_120_pct_diff'] = ((input_data['sma_15'] - input_data['sma_120']) / input_data['sma_120']) * 100\n",
    "            input_data['ema_15_120_pct_diff'] = ((input_data['ema_15'] - input_data['ema_120']) / input_data['ema_120']) * 100\n",
    "        \n",
    "            # 30-minute SMA (Simple Moving Average) for medium-term trend analysis\n",
    "            # 30-minute EMA for a responsive short to medium-term trend\n",
    "\n",
    "            input_data['macd'] = ta.trend.macd(input_data['CLOSE'], window_slow=26, window_fast=12)\n",
    "            input_data['macd_signal'] = ta.trend.macd_signal(input_data['CLOSE'], window_slow=26, window_fast=12, window_sign=9)\n",
    "            input_data['macd_diff'] = input_data['macd'] - input_data['macd_signal']  # MACD Histogram\n",
    "            input_data['adx'] = ta.trend.adx(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=14)\n",
    "            # Standard MACD (Moving Average Convergence Divergence) settings for capturing trend and momentum changes\n",
    "\n",
    "            # Momentum Indicators\n",
    "            input_data['rsi'] = ta.momentum.rsi(input_data['CLOSE'], window=14)\n",
    "            input_data['stochastic_k'] = ta.momentum.stoch(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=14, smooth_window=3)\n",
    "            input_data['stochastic_d'] = ta.momentum.stoch_signal(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=14, smooth_window=3)\n",
    "            input_data['cci'] = ta.trend.cci(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=20)\n",
    "            input_data['williams_r'] = ta.momentum.williams_r(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], lbp=14)\n",
    "            # 14-minute RSI (Relative Strength Index) for overbought/oversold conditions\n",
    "            # 14-minute Stochastic Oscillator for short-term momentum\n",
    "            # 20-minute CCI for identifying cyclical trends\n",
    "\n",
    "            # Volatility Indicators\n",
    "            bb_indicator = ta.volatility.BollingerBands(input_data['CLOSE'], window=20, window_dev=2)\n",
    "            input_data['bollinger_hband'] = bb_indicator.bollinger_hband()\n",
    "            input_data['bollinger_lband'] = bb_indicator.bollinger_lband()\n",
    "            input_data['bollinger_mband'] = bb_indicator.bollinger_mavg()\n",
    "            input_data['bollinger_width_percent'] = (input_data['bollinger_hband'] - input_data['bollinger_lband']) / input_data['CLOSE']\n",
    "            input_data['bollinger_position'] = bb_indicator.bollinger_pband()\n",
    "            input_data['atr'] = ta.volatility.average_true_range(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=14)\n",
    "            # Bollinger Bands with standard deviation score for precise volatility measure\n",
    "            # 14-minute ATR (Average True Range) for short-term volatility\n",
    "\n",
    "            # Volume-Based Indicators\n",
    "            input_data['obv'] = ta.volume.on_balance_volume(input_data['CLOSE'], input_data['VOLUME'])\n",
    "            # OBV (On-Balance Volume) for volume-based trend prediction\n",
    "\n",
    "            # Calculate VWAP for each day\n",
    "            input_data['vwap'] = input_data.groupby(input_data['timestamp_est'].dt.date).apply(\n",
    "                lambda x: (x['CLOSE'] * x['VOLUME']).cumsum() / x['VOLUME'].cumsum()\n",
    "            ).reset_index(level=0, drop=True)\n",
    "\n",
    "            # Additional Technical Indicators\n",
    "            input_data['ichimoku_tenkan_sen'] = ta.trend.ichimoku_conversion_line(input_data['HIGH'], input_data['LOW'], window1=9, window2=26)\n",
    "            input_data['ichimoku_senkou_span_a'] = ta.trend.ichimoku_a(input_data['HIGH'], input_data['LOW'])\n",
    "            input_data['ichimoku_senkou_span_b'] = ta.trend.ichimoku_b(input_data['HIGH'], input_data['LOW'])\n",
    "            input_data['rvi'] = ta.trend.stc(input_data['CLOSE'])\n",
    "            input_data['force_index'] = ta.volume.force_index(input_data['CLOSE'], input_data['VOLUME'])\n",
    "            ao_indicator = ta.momentum.AwesomeOscillatorIndicator(input_data['HIGH'], input_data['LOW'])\n",
    "            input_data['ao_ind'] = ao_indicator.awesome_oscillator()  # Extract the values and store them in the DataFrame\n",
    "            # Ichimoku Tenkan-sen (Conversion Line): The average of the highest high and the lowest low over the last 9 periods.\n",
    "            # Ichimoku Senkou Span A (Leading Span A): The average of the Tenkan-sen and the Kijun-sen, plotted 26 periods ahead.\n",
    "            # Ichimoku Senkou Span B (Leading Span B): The average of the highest high and the lowest low over the last 52 periods, plotted 26 periods ahead.\n",
    "            # Relative Vigor Index (RVI): Measures the conviction of a recent price action and the likelihood that it will continue.\n",
    "            # Force Index: Combines price change and volume to measure the strength of bulls and bears in the market.\n",
    "            # The Awesome Oscillator is an indicator used to measure market momentum. AO calculates the difference of a 34 Period and 5 Period Simple Moving Averages.\n",
    "\n",
    "            # Lagged Features\n",
    "            lagged_features = []\n",
    "\n",
    "            for lag in [1, 2, 3, 4, 5, 10, 20, 30]:\n",
    "                lagged_features.append(input_data['rsi'].shift(lag))\n",
    "                lagged_features.append(input_data['macd_diff'].shift(lag))\n",
    "                lagged_features.append(input_data['cci'].shift(lag))\n",
    "                lagged_features.append(input_data['adx'].shift(lag))\n",
    "                lagged_features.append(input_data['CLOSE'].shift(lag))\n",
    "                lagged_features.append(input_data['VOLUME'].shift(lag))\n",
    "                lagged_features.append(input_data['vwap'].shift(lag))\n",
    "                lagged_features.append(input_data['atr'].shift(lag))\n",
    "\n",
    "            # Concatenate all lagged features\n",
    "            lagged_features_df = pd.concat(lagged_features, axis=1)\n",
    "\n",
    "            # Rename columns appropriately\n",
    "            lagged_feature_columns = [f'{col}_lag{lag}' for lag in [1, 2, 3, 4, 5, 10, 20, 30] for col in ['rsi', 'macd_diff', 'cci', 'adx', 'CLOSE', 'VOLUME', 'vwap', 'atr']]\n",
    "            lagged_features_df.columns = lagged_feature_columns\n",
    "\n",
    "            # Concatenate lagged features with input_data\n",
    "            input_data = pd.concat([input_data, lagged_features_df], axis=1)\n",
    "\n",
    "            # Polynomial Features (example for RSI and CCI)\n",
    "            # You can concatenate them like this:\n",
    "            squared_columns = pd.concat([\n",
    "                input_data['rsi'] ** 2,\n",
    "                input_data['cci'] ** 2,\n",
    "                input_data['macd_diff'] ** 2,\n",
    "                input_data['adx'] ** 2,\n",
    "                input_data['bollinger_width_percent'] ** 2\n",
    "            ], axis=1)\n",
    "            squared_columns.columns = ['rsi_squared', 'cci_squared', 'macd_squared', 'adx_squared', 'bollinger_width_percent_squared']\n",
    "            input_data = pd.concat([input_data, squared_columns], axis=1)\n",
    "\n",
    "            # Interaction Features\n",
    "            input_data['sma_30_rsi'] = input_data['sma_30'] * input_data['rsi']\n",
    "            input_data['macd_stochastic_k'] = input_data['macd_diff'] * input_data['stochastic_k']\n",
    "            input_data['ema_30_atr'] = input_data['ema_30'] * input_data['atr']\n",
    "            input_data['rsi_macd_diff_interaction'] = input_data['rsi'] * input_data['macd_diff']\n",
    "            input_data['sma_30_adx_interaction'] = input_data['sma_30'] * input_data['adx']\n",
    "            input_data['bollinger_position_atr_interaction'] = input_data['bollinger_position'] * input_data['atr']\n",
    "\n",
    "            # Calculate additional features\n",
    "            input_data['date'] = input_data['timestamp_est'].dt.date\n",
    "            input_data['day_of_week'] = input_data['timestamp_est'].dt.dayofweek\n",
    "            input_data['hour_of_day'] = input_data['timestamp_est'].dt.hour\n",
    "            input_data['time_of_day'] = input_data['timestamp_est'].dt.time\n",
    "\n",
    "            # Movement in-day & in-week as percentage\n",
    "            input_data['daily_opening_price'] = input_data.groupby(input_data['timestamp_est'].dt.date)['OPEN'].transform('first')\n",
    "            input_data['daily_movement_since_open'] = (input_data['CLOSE'] - input_data['daily_opening_price']) / input_data['daily_opening_price']\n",
    "\n",
    "            input_data['weekly_opening_price'] = input_data.groupby(input_data['timestamp_est'].dt.isocalendar().week)['OPEN'].transform('first')\n",
    "            input_data['weekly_movement_since_open'] = (input_data['CLOSE'] - input_data['weekly_opening_price']) / input_data['weekly_opening_price']\n",
    "\n",
    "            # Calculate percentage movement over the last 20 periods (20 minutes)\n",
    "            input_data['pct_movement_20_periods'] = input_data['CLOSE'].pct_change(periods=20)\n",
    "\n",
    "            # Calculate percentage movement over the last 60 periods (1 hour)\n",
    "            input_data['pct_movement_60_periods'] = input_data['CLOSE'].pct_change(periods=60)\n",
    "\n",
    "            # Targets\n",
    "            input_data['target_30_min'] = input_data['CLOSE'].shift(-30)\n",
    "            # Target for the closing price 30 minutes from the current time, aligning with the 30 minute trading strategy\n",
    "            # Calculate movement_pct and signal\n",
    "            input_data['target_movement_pct'] = ((input_data['target_30_min'] - input_data['CLOSE']).abs() / input_data['CLOSE']) * 100\n",
    "            input_data['target_movement_signal'] = input_data.apply(\n",
    "                lambda row: 1 if (row['target_30_min'] - row['CLOSE']) > 0.001 * row['CLOSE'] else \n",
    "                            (-1 if (row['target_30_min'] - row['CLOSE']) <= -0.001 * row['CLOSE'] else 0), \n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Drop some columns and reorder\n",
    "            input_data = input_data.drop(columns=['TIMESTAMP', 'timestamp_utc'])\n",
    "            input_data = input_data[['SYMBOL', 'timestamp_est', 'date', 'trading_hours_ind', 'trading_hours_ind_ext'] + [col for col in input_data.columns if col not in ['SYMBOL', 'timestamp_est', 'date', 'trading_hours_ind', 'trading_hours_ind_ext']]]\n",
    "\n",
    "            # Step 4: Store the final DataFrame into the output table\n",
    "            if output_table is not None:\n",
    "                # Truncate the existing table\n",
    "                self.db_write(f\"DELETE FROM {output_table}\")\n",
    "                print(f\"Truncated table {output_table}.\")\n",
    "\n",
    "                # Store the DataFrame in the output table\n",
    "                self.db_append(output_table, input_data)\n",
    "                print(f\"Prepared model data stored in {output_table}. Number of rows inserted: {len(input_data)}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing symbol {symbol}: {e}\")\n",
    "    \n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        return input_data\n",
    "\n",
    "    def classification_modeling_build(self, features, target, symbol):\n",
    "        \"\"\"\n",
    "        Splits the data into training and testing sets based on provided features and target variable for a specific symbol.\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.database_path)\n",
    "        df = pd.read_sql(f\"SELECT * FROM SYMBOL_DATA_MODELING WHERE SYMBOL = '{symbol}' AND TRADING_HOURS_IND_EXT = 1\", conn)\n",
    "        conn.close()\n",
    "\n",
    "        unique_dates = df['DATE'].unique()\n",
    "        train_dates, val_dates = train_test_split(unique_dates, test_size=0.3, random_state=42)\n",
    "\n",
    "        train_data = df[df['DATE'].isin(train_dates)]\n",
    "        val_data = df[df['DATE'].isin(val_dates)]\n",
    "\n",
    "        X_train = train_data[features]\n",
    "        y_train = train_data[target]\n",
    "        X_val = val_data[features]\n",
    "        y_val = val_data[target]\n",
    "\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        models = {\n",
    "            \"K-Nearest Neighbors\": (KNeighborsClassifier(), {\n",
    "                'n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "            }),\n",
    "            \"XGBoost\": (XGBClassifier(eval_metric='mlogloss'), {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'reg_alpha': [0, 0.1, 1],\n",
    "                'reg_lambda': [1, 1.5, 2]\n",
    "            }),\n",
    "            \"Logistic Regression\": (LogisticRegression(max_iter=1000, random_state=42), {\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'C': [0.1, 1, 10],\n",
    "                'solver': ['liblinear', 'saga'],\n",
    "                'class_weight': ['balanced', None]\n",
    "            }),\n",
    "            \"Random Forest\": (RandomForestClassifier(), {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [None, 10, 20, 30],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'bootstrap': [True, False],\n",
    "                'class_weight': ['balanced', 'balanced_subsample', None]\n",
    "            }),\n",
    "            \"Neural Network\": (MLPClassifier(max_iter=1000, random_state=42), {\n",
    "                'hidden_layer_sizes': [(50,), (100,), (150,), (100, 50), (100, 100)],\n",
    "                'activation': ['relu', 'tanh', 'logistic'],\n",
    "                'solver': ['adam', 'sgd', 'lbfgs'],\n",
    "                'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "                'learning_rate': ['constant', 'adaptive'],\n",
    "                'learning_rate_init': [0.001, 0.01, 0.1]\n",
    "            }),\n",
    "            \"Gradient Boosting\": (GradientBoostingClassifier(), {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 4, 5, 6],\n",
    "                'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }),\n",
    "            \"Support Vector Machine\": (SVC(), {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'class_weight': ['balanced', None]\n",
    "            })\n",
    "        }\n",
    "\n",
    "        for model_name, (model, params) in models.items():\n",
    "            print(f\"Processing {model_name} for symbol: {symbol}\")\n",
    "\n",
    "            # Apply standardization only for SVM and Neural Network\n",
    "            if model_name in [\"Support Vector Machine\", \"Neural Network\"]:\n",
    "                scaler = StandardScaler()\n",
    "                X_train_smote = scaler.fit_transform(X_train_smote)\n",
    "                X_val = scaler.transform(X_val)\n",
    "\n",
    "            # These are mostly performance modifications by model type... faster = can do more combos of parameters\n",
    "            grid_search = RandomizedSearchCV(estimator=model, param_distributions=params, n_iter=10, cv=2, n_jobs=-1, scoring='f1_weighted', random_state=42,verbose=3)\n",
    "\n",
    "            if model_name == \"XGBoost\":\n",
    "                y_train_transformed = y_train_smote.replace({-1: 0, 0: 1, 1: 2})\n",
    "                grid_search.fit(X_train_smote, y_train_transformed)\n",
    "                best_model = grid_search.best_estimator_\n",
    "                y_pred_transformed = best_model.predict(X_val)\n",
    "                y_pred = pd.Series(y_pred_transformed).replace({0: -1, 1: 0, 2: 1})\n",
    "            else:\n",
    "                grid_search.fit(X_train_smote, y_train_smote)\n",
    "                best_model = grid_search.best_estimator_\n",
    "                y_pred = best_model.predict(X_val)\n",
    "\n",
    "            f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "            cm = confusion_matrix(y_val, y_pred)\n",
    "            cr = classification_report(y_val, y_pred, zero_division=0)\n",
    "\n",
    "            # visualize & print results\n",
    "            print(f\"{model_name} - F1 Score: {f1}\")\n",
    "            print(\"Confusion Matrix:\\n\", cm)\n",
    "            print(\"Classification Report:\\n\", cr)\n",
    "            print(\"Best Hyperparameters:\\n\", grid_search.best_params_)\n",
    "            sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='d')\n",
    "            plt.title(f'Confusion Matrix: {model_name}')\n",
    "            plt.show()\n",
    "            print(f\"__________________________________\")\n",
    "            # Save the model with a unique filename\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            joblib.dump(best_model, f'{symbol}_{model_name}_{timestamp}.pkl')\n",
    "\n",
    "            evaluation_results = {\n",
    "                'SYMBOL': symbol,\n",
    "                'MODEL_NAME': model_name,\n",
    "                'F1_SCORE': f1,\n",
    "                'CONFUSION_MATRIX': str(cm),\n",
    "                'CLASSIFICATION_REPORT': cr,\n",
    "                'BEST_PARAMS': str(grid_search.best_params_)\n",
    "            }\n",
    "            results_df = pd.DataFrame([evaluation_results])\n",
    "            self.db_append('CLASSIFICATION_MODEL_RESULTS', results_df)\n",
    "\n",
    "            # Feature Importance or Coefficients\n",
    "            if hasattr(best_model, \"feature_importances_\"):\n",
    "                importances = best_model.feature_importances_\n",
    "                feature_importance_df = pd.DataFrame({\n",
    "                    'feature': features,\n",
    "                    'importance': importances\n",
    "                }).sort_values(by='importance', ascending=False)\n",
    "                print(f\"Feature importances for {model_name}:\")\n",
    "                print(feature_importance_df)\n",
    "            elif hasattr(best_model, \"coef_\"):\n",
    "                coefs = best_model.coef_[0]\n",
    "                coef_df = pd.DataFrame({\n",
    "                    'feature': features,\n",
    "                    'coefficient': coefs\n",
    "                }).sort_values(by='coefficient', ascending=False)\n",
    "                print(f\"Coefficients for {model_name}:\")\n",
    "                print(coef_df)\n",
    "            else:\n",
    "                print(f\"No feature importance or coefficients for {model_name}\")\n",
    "            print(f\"____________________________________________\")\n",
    "            print(f\"____________________________________________\")\n",
    "\n",
    "    def classification_modeling_apply(self, input_table, symbol, output_table=None):\n",
    "        \"\"\"\n",
    "        This method reads data from the stage table, adds indicators, makes predictions using models,\n",
    "        and stores the results in a new table.\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        try:\n",
    "            # Step 1: Transfer stage data and add indicators\n",
    "            indicators_data = self.transfer_stage_modeling(input_table, symbol)\n",
    "            indicators_data.columns = indicators_data.columns.str.upper()\n",
    "            indicators_data = indicators_data[indicators_data['TRADING_HOURS_IND_EXT'] == 1]\n",
    "\n",
    "            if indicators_data.empty:\n",
    "                raise ValueError(f\"No data found for symbol {symbol} in table {input_table}\")\n",
    "\n",
    "            # Extract feature columns\n",
    "            feature_columns = ['CLOSE','HIGH','LOW','TRADE_COUNT','OPEN','VOLUME','VWAP','SMA_15','SMA_30','SMA_120','EMA_15','EMA_30','EMA_120','SMA_15_120_PCT_DIFF','EMA_15_120_PCT_DIFF',\n",
    "            'MACD','MACD_SIGNAL','MACD_DIFF','ADX','RSI','STOCHASTIC_K','STOCHASTIC_D','CCI','WILLIAMS_R','BOLLINGER_WIDTH_PERCENT','BOLLINGER_POSITION','ATR','OBV',\n",
    "            'ICHIMOKU_TENKAN_SEN','ICHIMOKU_SENKOU_SPAN_A','ICHIMOKU_SENKOU_SPAN_B','RVI','FORCE_INDEX','AO_IND','RSI_LAG1','MACD_DIFF_LAG1','CCI_LAG1','ADX_LAG1','CLOSE_LAG1',\n",
    "            'VOLUME_LAG1','VWAP_LAG1','ATR_LAG1','RSI_LAG2','MACD_DIFF_LAG2','CCI_LAG2','ADX_LAG2','CLOSE_LAG2','VOLUME_LAG2','VWAP_LAG2','ATR_LAG2','RSI_LAG3','MACD_DIFF_LAG3','CCI_LAG3',\n",
    "            'ADX_LAG3','CLOSE_LAG3','VOLUME_LAG3','VWAP_LAG3','ATR_LAG3','RSI_LAG4','MACD_DIFF_LAG4','CCI_LAG4','ADX_LAG4','CLOSE_LAG4','VOLUME_LAG4','VWAP_LAG4','ATR_LAG4','RSI_LAG5',\n",
    "            'MACD_DIFF_LAG5','CCI_LAG5','ADX_LAG5','CLOSE_LAG5','VOLUME_LAG5','VWAP_LAG5','ATR_LAG5','RSI_LAG10','MACD_DIFF_LAG10','CCI_LAG10','ADX_LAG10','CLOSE_LAG10','VOLUME_LAG10',\n",
    "            'VWAP_LAG10','ATR_LAG10','RSI_LAG20','MACD_DIFF_LAG20','CCI_LAG20','ADX_LAG20','CLOSE_LAG20','VOLUME_LAG20','VWAP_LAG20','ATR_LAG20','RSI_LAG30','MACD_DIFF_LAG30','CCI_LAG30',\n",
    "            'ADX_LAG30','CLOSE_LAG30','VOLUME_LAG30','VWAP_LAG30','ATR_LAG30','RSI_SQUARED','CCI_SQUARED','MACD_SQUARED','ADX_SQUARED','BOLLINGER_WIDTH_PERCENT_SQUARED','SMA_30_RSI',\n",
    "            'MACD_STOCHASTIC_K','EMA_30_ATR','RSI_MACD_DIFF_INTERACTION','SMA_30_ADX_INTERACTION','BOLLINGER_POSITION_ATR_INTERACTION','DAY_OF_WEEK','HOUR_OF_DAY',\n",
    "            'DAILY_MOVEMENT_SINCE_OPEN','WEEKLY_MOVEMENT_SINCE_OPEN','PCT_MOVEMENT_20_PERIODS','PCT_MOVEMENT_60_PERIODS' ]\n",
    "            \n",
    "            # Ensure only the feature columns are used for prediction\n",
    "            feature_data = indicators_data[feature_columns]\n",
    "\n",
    "            # Step 2: Load models\n",
    "            models = {\n",
    "                \"Random_Forest\": joblib.load('SPY_Random Forest_20240603_103726.pkl'),\n",
    "                \"XGBoost\": joblib.load('SPY_XGBoost_20240603_135615.pkl'),\n",
    "                \"Gradient_Boosting\": joblib.load('SPY_Gradient Boosting_20240603_173344.pkl'),\n",
    "                \"Neural_Network\": joblib.load('SPY_Neural Network_20240604_104103.pkl'),\n",
    "            }\n",
    "\n",
    "            # Step 3: Make predictions using each model and store the results in the DataFrame\n",
    "            for model_name, model in models.items():\n",
    "                print(f\"Predicting with {model_name} model.\")\n",
    "                if model_name == \"XGBoost\":\n",
    "                    predictions = model.predict(feature_data)\n",
    "                    # Inverse transform the predictions for XGBoost\n",
    "                    predictions = pd.Series(predictions).replace({0: -1, 1: 0, 2: 1}).values\n",
    "                elif model_name == \"Neural_Network\":\n",
    "                    scaler = StandardScaler()\n",
    "                    X_scaled = scaler.fit_transform(feature_data)\n",
    "                    predictions = model.predict(X_scaled)\n",
    "                else:\n",
    "                    predictions = model.predict(feature_data)\n",
    "                \n",
    "                indicators_data[f'{model_name}_prediction'] = predictions\n",
    "                print(f\"{model_name} prediction computed for {symbol}.\")\n",
    "                print(f\"Unique predictions for {model_name}: {pd.Series(predictions).unique()}\")\n",
    "\n",
    "            # Step 4: Store the final DataFrame into the output table\n",
    "            if output_table is not None:\n",
    "                # Truncate the existing table\n",
    "                self.db_write(f\"DELETE FROM {output_table}\")\n",
    "                print(f\"Truncated table {output_table}.\")\n",
    "\n",
    "                # Store the DataFrame in the output table\n",
    "                self.db_append(output_table, indicators_data)\n",
    "                print(f\"Model predictions stored in {output_table}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing symbol {symbol}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        return indicators_data\n",
    "    \n",
    "    ######################################################  Database write/read methods ######################################################\n",
    "    def db_write(self, sql_statement):\n",
    "        \"\"\"\n",
    "\n",
    "        Basic functionality to execute an SQL statement against our database\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database & create a cursor object\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Execute the SQL statement\n",
    "        cur.execute(sql_statement)\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def db_append(self, table_name, data_frame):\n",
    "        \"\"\"\n",
    "\n",
    "        Basic functionality to append/insert a dataframe into a specified table in our database\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        # Append our dataframe into our table\n",
    "        data_frame.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "    def db_append_no_duplicates(self, table_name, data_frame):\n",
    "        \"\"\"\n",
    "\n",
    "        Variation of db_append that makes sure we aren't inserting duplicates by first checking against the primary key of the table records are being inserted into\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Get the primary key column names\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        table_info = cursor.fetchall()\n",
    "\n",
    "        # Identify columns that are part of the primary key\n",
    "        primary_key_columns = [column[1] for column in table_info if column[5] > 0]\n",
    "\n",
    "        # If no primary key columns found, fall back to db_append method\n",
    "        if not primary_key_columns:\n",
    "            data_frame.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "            \n",
    "            # Commit the changes & close the connection\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return\n",
    "    \n",
    "        # Convert primary key columns and DataFrame columns to uppercase for case-insensitive comparison\n",
    "        primary_key_columns_upper = [col.upper() for col in primary_key_columns]\n",
    "        data_frame.columns = data_frame.columns.str.upper()\n",
    "\n",
    "        # Ensure the DataFrame contains all primary key columns\n",
    "        for column in primary_key_columns_upper:\n",
    "            if column not in data_frame.columns:\n",
    "                raise KeyError(f\"Column '{column}' is missing in the DataFrame\")\n",
    "\n",
    "        # Standardize the timestamp format in the DataFrame\n",
    "        timestamp_column = None\n",
    "        for col in primary_key_columns_upper:\n",
    "            if 'TIMESTAMP' in col:\n",
    "                timestamp_column = col\n",
    "                break\n",
    "        \n",
    "        if timestamp_column:\n",
    "            data_frame[timestamp_column] = pd.to_datetime(data_frame[timestamp_column], utc=True)\n",
    "\n",
    "        # Construct the SELECT statement to fetch existing primary keys from the table\n",
    "        existing_keys_query = f\"SELECT {', '.join(primary_key_columns)} FROM {table_name}\"\n",
    "        existing_keys_df = pd.read_sql(existing_keys_query, conn)\n",
    "\n",
    "        # Standardize the timestamp format in the existing keys DataFrame\n",
    "        if timestamp_column:\n",
    "            existing_keys_df[timestamp_column] = pd.to_datetime(existing_keys_df[timestamp_column], utc=True)\n",
    "\n",
    "        # Construct the composite primary key from the existing keys DataFrame\n",
    "        existing_keys_df['COMPOSITE_KEY'] = existing_keys_df.apply(lambda row: tuple(row), axis=1)\n",
    "        existing_keys = set(existing_keys_df['COMPOSITE_KEY'])\n",
    "\n",
    "        # Print count of rows in the DataFrame before filtering\n",
    "        print(f\"Total count of data records queried: {len(data_frame)}\")\n",
    "\n",
    "        # Construct the composite primary key for new records\n",
    "        data_frame['COMPOSITE_KEY'] = data_frame.apply(lambda row: tuple(row[primary_key_columns_upper]), axis=1)\n",
    "\n",
    "        # Filter out rows with primary keys that already exist in the table\n",
    "        data_frame_new_records = data_frame[~data_frame['COMPOSITE_KEY'].isin(existing_keys)]\n",
    "\n",
    "        # Drop the composite key column\n",
    "        data_frame_new_records = data_frame_new_records.drop(columns=['COMPOSITE_KEY'])\n",
    "\n",
    "        # Append only the new rows into our table\n",
    "        if not data_frame_new_records.empty:\n",
    "            data_frame_new_records.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "\n",
    "        # Print count of rows that will be inserted\n",
    "        print(f\"Count of fresh data records actually inserted into {table_name}: {len(data_frame_new_records)}\")\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema DDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Schema DDL - creates the tables in our database that we need for storing the raw scraped data & analyzing it.\n",
    "\n",
    "\"\"\"\n",
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# This table will house the raw scraped data\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS STG_SYMBOL_DATA''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS STG_SYMBOL_DATA (\n",
    "#     SYMBOL                  TEXT,\n",
    "#     TIMESTAMP               TIMESTAMP,\n",
    "#     CLOSE                   DECIMAL,\n",
    "#     HIGH                    DECIMAL,\n",
    "#     LOW                     DECIMAL,\n",
    "#     TRADE_COUNT             INTEGER,\n",
    "#     OPEN                    DECIMAL,\n",
    "#     VOLUME                  INTEGER,\n",
    "#     VWAP                    DECIMAL,\n",
    "#     PRIMARY KEY (SYMBOL, TIMESTAMP)\n",
    "# )\n",
    "# ''')\n",
    "\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS SYMBOL_DATA_MODELING''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS SYMBOL_DATA_MODELING (\n",
    "#     SYMBOL                      TEXT,\n",
    "#     TIMESTAMP_EST               TIMESTAMP,\n",
    "#     DATE                        DATE,\n",
    "#     TRADING_HOURS_IND           BOOLEAN,\n",
    "#     TRADING_HOURS_IND_EXT       BOOLEAN,\n",
    "#     CLOSE                       DECIMAL,\n",
    "#     HIGH                        DECIMAL,\n",
    "#     LOW                         DECIMAL,\n",
    "#     TRADE_COUNT                 INTEGER,\n",
    "#     OPEN                        DECIMAL,\n",
    "#     VOLUME                      INTEGER,\n",
    "#     VWAP                        DECIMAL,\n",
    "#     SMA_15                      DECIMAL,\n",
    "#     SMA_30                      DECIMAL,\n",
    "#     SMA_120                     DECIMAL,\n",
    "#     EMA_15                      DECIMAL,\n",
    "#     EMA_30                      DECIMAL,\n",
    "#     EMA_120                     DECIMAL,\n",
    "#     SMA_15_120_PCT_DIFF         DECIMAL,                    \n",
    "#     EMA_15_120_PCT_DIFF         DECIMAL,\n",
    "#     MACD                        DECIMAL,\n",
    "#     MACD_SIGNAL                 DECIMAL,\n",
    "#     MACD_DIFF                   DECIMAL,\n",
    "#     ADX                         DECIMAL,\n",
    "#     RSI                         DECIMAL,                      \n",
    "#     STOCHASTIC_K                DECIMAL,\n",
    "#     STOCHASTIC_D                DECIMAL,\n",
    "#     CCI                         DECIMAL,\n",
    "#     WILLIAMS_R                  DECIMAL, \n",
    "#     BOLLINGER_HBAND             DECIMAL,\n",
    "#     BOLLINGER_LBAND             DECIMAL,\n",
    "#     BOLLINGER_MBAND             DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT     DECIMAL,\n",
    "#     BOLLINGER_POSITION          DECIMAL,\n",
    "#     ATR                         DECIMAL,\n",
    "#     OBV                         INTEGER, \n",
    "#     ICHIMOKU_TENKAN_SEN         DECIMAL, \n",
    "#     ICHIMOKU_SENKOU_SPAN_A      DECIMAL,\n",
    "#     ICHIMOKU_SENKOU_SPAN_B      DECIMAL,\n",
    "#     RVI                         DECIMAL,\n",
    "#     FORCE_INDEX                 DECIMAL,\n",
    "#     AO_IND                      DECIMAL,\n",
    "#     RSI_LAG1                    DECIMAL,\n",
    "#     MACD_DIFF_LAG1              DECIMAL,\n",
    "#     CCI_LAG1                    DECIMAL,\n",
    "#     ADX_LAG1                    DECIMAL, \n",
    "#     CLOSE_LAG1                  DECIMAL,\n",
    "#     VOLUME_LAG1                 DECIMAL,\n",
    "#     VWAP_LAG1                   DECIMAL,\n",
    "#     ATR_LAG1                    DECIMAL,\n",
    "#     RSI_LAG2                    DECIMAL,\n",
    "#     MACD_DIFF_LAG2              DECIMAL,\n",
    "#     CCI_LAG2                    DECIMAL,\n",
    "#     ADX_LAG2                    DECIMAL,\n",
    "#     CLOSE_LAG2                  DECIMAL,\n",
    "#     VOLUME_LAG2                 DECIMAL,\n",
    "#     VWAP_LAG2                   DECIMAL,\n",
    "#     ATR_LAG2                    DECIMAL,\n",
    "#     RSI_LAG3                    DECIMAL,\n",
    "#     MACD_DIFF_LAG3              DECIMAL,\n",
    "#     CCI_LAG3                    DECIMAL,\n",
    "#     ADX_LAG3                    DECIMAL,\n",
    "#     CLOSE_LAG3                  DECIMAL,\n",
    "#     VOLUME_LAG3                 DECIMAL,\n",
    "#     VWAP_LAG3                   DECIMAL,\n",
    "#     ATR_LAG3                    DECIMAL,                      \n",
    "#     RSI_LAG4                    DECIMAL,\n",
    "#     MACD_DIFF_LAG4              DECIMAL,\n",
    "#     CCI_LAG4                    DECIMAL,\n",
    "#     ADX_LAG4                    DECIMAL,\n",
    "#     CLOSE_LAG4                  DECIMAL,\n",
    "#     VOLUME_LAG4                 DECIMAL,\n",
    "#     VWAP_LAG4                   DECIMAL,\n",
    "#     ATR_LAG4                    DECIMAL,                           \n",
    "#     RSI_LAG5                    DECIMAL,\n",
    "#     MACD_DIFF_LAG5              DECIMAL,\n",
    "#     CCI_LAG5                    DECIMAL,\n",
    "#     ADX_LAG5                    DECIMAL,\n",
    "#     CLOSE_LAG5                  DECIMAL,\n",
    "#     VOLUME_LAG5                 DECIMAL,\n",
    "#     VWAP_LAG5                   DECIMAL,\n",
    "#     ATR_LAG5                    DECIMAL,\n",
    "#     RSI_LAG10                   DECIMAL,\n",
    "#     MACD_DIFF_LAG10             DECIMAL,\n",
    "#     CCI_LAG10                   DECIMAL,\n",
    "#     ADX_LAG10                   DECIMAL,\n",
    "#     CLOSE_LAG10                 DECIMAL,\n",
    "#     VOLUME_LAG10                DECIMAL,\n",
    "#     VWAP_LAG10                  DECIMAL,\n",
    "#     ATR_LAG10                   DECIMAL,    \n",
    "#     RSI_LAG20                   DECIMAL,\n",
    "#     MACD_DIFF_LAG20             DECIMAL,\n",
    "#     CCI_LAG20                   DECIMAL,\n",
    "#     ADX_LAG20                   DECIMAL,\n",
    "#     CLOSE_LAG20                 DECIMAL,\n",
    "#     VOLUME_LAG20                DECIMAL,\n",
    "#     VWAP_LAG20                  DECIMAL,\n",
    "#     ATR_LAG20                   DECIMAL,                     \n",
    "#     RSI_LAG30                   DECIMAL,\n",
    "#     MACD_DIFF_LAG30             DECIMAL,\n",
    "#     CCI_LAG30                   DECIMAL,\n",
    "#     ADX_LAG30                   DECIMAL,\n",
    "#     CLOSE_LAG30                 DECIMAL,\n",
    "#     VOLUME_LAG30                DECIMAL,\n",
    "#     VWAP_LAG30                  DECIMAL,\n",
    "#     ATR_LAG30                   DECIMAL,\n",
    "#     RSI_SQUARED                 DECIMAL,\n",
    "#     CCI_SQUARED                 DECIMAL,\n",
    "#     MACD_SQUARED                DECIMAL,\n",
    "#     ADX_SQUARED                 DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT_SQUARED         DECIMAL,\n",
    "#     SMA_30_RSI                  DECIMAL,\n",
    "#     MACD_STOCHASTIC_K           DECIMAL,\n",
    "#     EMA_30_ATR                  DECIMAL,\n",
    "#     RSI_MACD_DIFF_INTERACTION   DECIMAL,\n",
    "#     SMA_30_ADX_INTERACTION      DECIMAL,\n",
    "#     BOLLINGER_POSITION_ATR_INTERACTION      DECIMAL,\n",
    "#     DAY_OF_WEEK                 INTEGER,\n",
    "#     HOUR_OF_DAY                 INTEGER,\n",
    "#     TIME_OF_DAY                 TIME,                      \n",
    "#     DAILY_OPENING_PRICE         DECIMAL,\n",
    "#     DAILY_MOVEMENT_SINCE_OPEN   DECIMAL,\n",
    "#     WEEKLY_OPENING_PRICE        DECIMAL,\n",
    "#     WEEKLY_MOVEMENT_SINCE_OPEN  DECIMAL,\n",
    "#     PCT_MOVEMENT_20_PERIODS     DECIMAL,\n",
    "#     PCT_MOVEMENT_60_PERIODS     DECIMAL,\n",
    "#     TARGET_30_MIN               DECIMAL,\n",
    "#     TARGET_MOVEMENT_PCT         DECIMAL,\n",
    "#     TARGET_MOVEMENT_SIGNAL      INTEGER,\n",
    "#     PRIMARY KEY (SYMBOL, TIMESTAMP_EST)\n",
    "# )\n",
    "# ''')\n",
    "\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS CLASSIFICATION_MODEL_RESULTS''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS CLASSIFICATION_MODEL_RESULTS  (\n",
    "#     ID                      INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "#     SYMBOL                  TEXT NOT NULL,\n",
    "#     MODEL_NAME              TEXT NOT NULL,\n",
    "#     F1_SCORE                REAL NOT NULL,\n",
    "#     CONFUSION_MATRIX        TEXT NOT NULL,\n",
    "#     CLASSIFICATION_REPORT   TEXT NOT NULL,\n",
    "#     BEST_PARAMS             TEXT NOT NULL,                 \n",
    "#     EVALUATION_TIMESTAMP    DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "# )\n",
    "# ''')\n",
    "\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS CLASSIFICATION_MODEL_PREDICTIONS''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS CLASSIFICATION_MODEL_PREDICTIONS (\n",
    "#     SYMBOL                      TEXT,\n",
    "#     TIMESTAMP_EST               TIMESTAMP,\n",
    "#     DATE                        DATE,\n",
    "#     TRADING_HOURS_IND           BOOLEAN,\n",
    "#     TRADING_HOURS_IND_EXT       BOOLEAN,\n",
    "#     CLOSE                       DECIMAL,\n",
    "#     HIGH                        DECIMAL,\n",
    "#     LOW                         DECIMAL,\n",
    "#     TRADE_COUNT                 INTEGER,\n",
    "#     OPEN                        DECIMAL,\n",
    "#     VOLUME                      INTEGER,\n",
    "#     VWAP                        DECIMAL,\n",
    "#     SMA_15                      DECIMAL,\n",
    "#     SMA_30                      DECIMAL,\n",
    "#     SMA_120                     DECIMAL,\n",
    "#     EMA_15                      DECIMAL,\n",
    "#     EMA_30                      DECIMAL,\n",
    "#     EMA_120                     DECIMAL,\n",
    "#     SMA_15_120_PCT_DIFF         DECIMAL,                    \n",
    "#     EMA_15_120_PCT_DIFF         DECIMAL,\n",
    "#     MACD                        DECIMAL,\n",
    "#     MACD_SIGNAL                 DECIMAL,\n",
    "#     MACD_DIFF                   DECIMAL,\n",
    "#     ADX                         DECIMAL,\n",
    "#     RSI                         DECIMAL,                      \n",
    "#     STOCHASTIC_K                DECIMAL,\n",
    "#     STOCHASTIC_D                DECIMAL,\n",
    "#     CCI                         DECIMAL,\n",
    "#     WILLIAMS_R                  DECIMAL, \n",
    "#     BOLLINGER_HBAND             DECIMAL,\n",
    "#     BOLLINGER_LBAND             DECIMAL,\n",
    "#     BOLLINGER_MBAND             DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT     DECIMAL,\n",
    "#     BOLLINGER_POSITION          DECIMAL,\n",
    "#     ATR                         DECIMAL,\n",
    "#     OBV                         INTEGER, \n",
    "#     ICHIMOKU_TENKAN_SEN         DECIMAL, \n",
    "#     ICHIMOKU_SENKOU_SPAN_A      DECIMAL,\n",
    "#     ICHIMOKU_SENKOU_SPAN_B      DECIMAL,\n",
    "#     RVI                         DECIMAL,\n",
    "#     FORCE_INDEX                 DECIMAL,\n",
    "#     AO_IND                      DECIMAL,\n",
    "#     RSI_LAG1                    DECIMAL,\n",
    "#     MACD_DIFF_LAG1              DECIMAL,\n",
    "#     CCI_LAG1                    DECIMAL,\n",
    "#     ADX_LAG1                    DECIMAL, \n",
    "#     CLOSE_LAG1                  DECIMAL,\n",
    "#     VOLUME_LAG1                 DECIMAL,\n",
    "#     VWAP_LAG1                   DECIMAL,\n",
    "#     ATR_LAG1                    DECIMAL,\n",
    "#     RSI_LAG2                    DECIMAL,\n",
    "#     MACD_DIFF_LAG2              DECIMAL,\n",
    "#     CCI_LAG2                    DECIMAL,\n",
    "#     ADX_LAG2                    DECIMAL,\n",
    "#     CLOSE_LAG2                  DECIMAL,\n",
    "#     VOLUME_LAG2                 DECIMAL,\n",
    "#     VWAP_LAG2                   DECIMAL,\n",
    "#     ATR_LAG2                    DECIMAL,\n",
    "#     RSI_LAG3                    DECIMAL,\n",
    "#     MACD_DIFF_LAG3              DECIMAL,\n",
    "#     CCI_LAG3                    DECIMAL,\n",
    "#     ADX_LAG3                    DECIMAL,\n",
    "#     CLOSE_LAG3                  DECIMAL,\n",
    "#     VOLUME_LAG3                 DECIMAL,\n",
    "#     VWAP_LAG3                   DECIMAL,\n",
    "#     ATR_LAG3                    DECIMAL,                      \n",
    "#     RSI_LAG4                    DECIMAL,\n",
    "#     MACD_DIFF_LAG4              DECIMAL,\n",
    "#     CCI_LAG4                    DECIMAL,\n",
    "#     ADX_LAG4                    DECIMAL,\n",
    "#     CLOSE_LAG4                  DECIMAL,\n",
    "#     VOLUME_LAG4                 DECIMAL,\n",
    "#     VWAP_LAG4                   DECIMAL,\n",
    "#     ATR_LAG4                    DECIMAL,                           \n",
    "#     RSI_LAG5                    DECIMAL,\n",
    "#     MACD_DIFF_LAG5              DECIMAL,\n",
    "#     CCI_LAG5                    DECIMAL,\n",
    "#     ADX_LAG5                    DECIMAL,\n",
    "#     CLOSE_LAG5                  DECIMAL,\n",
    "#     VOLUME_LAG5                 DECIMAL,\n",
    "#     VWAP_LAG5                   DECIMAL,\n",
    "#     ATR_LAG5                    DECIMAL,\n",
    "#     RSI_LAG10                   DECIMAL,\n",
    "#     MACD_DIFF_LAG10             DECIMAL,\n",
    "#     CCI_LAG10                   DECIMAL,\n",
    "#     ADX_LAG10                   DECIMAL,\n",
    "#     CLOSE_LAG10                 DECIMAL,\n",
    "#     VOLUME_LAG10                DECIMAL,\n",
    "#     VWAP_LAG10                  DECIMAL,\n",
    "#     ATR_LAG10                   DECIMAL,    \n",
    "#     RSI_LAG20                   DECIMAL,\n",
    "#     MACD_DIFF_LAG20             DECIMAL,\n",
    "#     CCI_LAG20                   DECIMAL,\n",
    "#     ADX_LAG20                   DECIMAL,\n",
    "#     CLOSE_LAG20                 DECIMAL,\n",
    "#     VOLUME_LAG20                DECIMAL,\n",
    "#     VWAP_LAG20                  DECIMAL,\n",
    "#     ATR_LAG20                   DECIMAL,                     \n",
    "#     RSI_LAG30                   DECIMAL,\n",
    "#     MACD_DIFF_LAG30             DECIMAL,\n",
    "#     CCI_LAG30                   DECIMAL,\n",
    "#     ADX_LAG30                   DECIMAL,\n",
    "#     CLOSE_LAG30                 DECIMAL,\n",
    "#     VOLUME_LAG30                DECIMAL,\n",
    "#     VWAP_LAG30                  DECIMAL,\n",
    "#     ATR_LAG30                   DECIMAL,\n",
    "#     RSI_SQUARED                 DECIMAL,\n",
    "#     CCI_SQUARED                 DECIMAL,\n",
    "#     MACD_SQUARED                DECIMAL,\n",
    "#     ADX_SQUARED                 DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT_SQUARED         DECIMAL,\n",
    "#     SMA_30_RSI                  DECIMAL,\n",
    "#     MACD_STOCHASTIC_K           DECIMAL,\n",
    "#     EMA_30_ATR                  DECIMAL,\n",
    "#     RSI_MACD_DIFF_INTERACTION   DECIMAL,\n",
    "#     SMA_30_ADX_INTERACTION      DECIMAL,\n",
    "#     BOLLINGER_POSITION_ATR_INTERACTION      DECIMAL,\n",
    "#     DAY_OF_WEEK                 INTEGER,\n",
    "#     HOUR_OF_DAY                 INTEGER,\n",
    "#     TIME_OF_DAY                 TIME,                      \n",
    "#     DAILY_OPENING_PRICE         DECIMAL,\n",
    "#     DAILY_MOVEMENT_SINCE_OPEN   DECIMAL,\n",
    "#     WEEKLY_OPENING_PRICE        DECIMAL,\n",
    "#     WEEKLY_MOVEMENT_SINCE_OPEN  DECIMAL,\n",
    "#     PCT_MOVEMENT_20_PERIODS     DECIMAL,\n",
    "#     PCT_MOVEMENT_60_PERIODS     DECIMAL,\n",
    "#     TARGET_30_MIN               DECIMAL,\n",
    "#     TARGET_MOVEMENT_PCT         DECIMAL,\n",
    "#     TARGET_MOVEMENT_SIGNAL      INTEGER,\n",
    "#     RANDOM_FOREST_PREDICTION        INTEGER,                \n",
    "#     XGBOOST_PREDICTION              INTEGER,     \n",
    "#     GRADIENT_BOOSTING_PREDICTION    INTEGER,     \n",
    "#     NEURAL_NETWORK_PREDICTION       INTEGER,     \n",
    "#     PRIMARY KEY (SYMBOL, TIMESTAMP_EST)\n",
    "# )\n",
    "# ''')\n",
    "\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS SYMBOL_SEQUENCE_MODELING''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS SYMBOL_SEQUENCE_MODELING (\n",
    "#     SYMBOL                      TEXT,\n",
    "#     TIMESTAMP_EST               TIMESTAMP,\n",
    "#     DATE                        DATE,\n",
    "#     TRADING_HOURS_IND           BOOLEAN,\n",
    "#     TRADING_HOURS_IND_EXT       BOOLEAN,\n",
    "#     CLOSE                       DECIMAL,\n",
    "#     HIGH                        DECIMAL,\n",
    "#     LOW                         DECIMAL,\n",
    "#     TRADE_COUNT                 INTEGER,\n",
    "#     OPEN                        DECIMAL,\n",
    "#     VOLUME                      INTEGER,\n",
    "#     VWAP                        DECIMAL,\n",
    "#     RSI                         DECIMAL,\n",
    "#     ATR                         DECIMAL,\n",
    "#     CCI                         DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT     DECIMAL,\n",
    "#     BOLLINGER_POSITION          DECIMAL, \n",
    "#     SMA_30                      DECIMAL,\n",
    "#     SMA_120                     DECIMAL,\n",
    "#     EMA_30                      DECIMAL,\n",
    "#     EMA_120                     DECIMAL,\n",
    "#     SMA_30_120_PCT_DIFF         DECIMAL,\n",
    "#     EMA_30_120_PCT_DIFF         DECIMAL,\n",
    "#     PRIMARY KEY (SYMBOL, TIMESTAMP_EST)\n",
    "# )\n",
    "# ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape & Store Data In Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped for 2024-06-05 through 11:59:59Z...\n",
      "Data scraped for 2024-06-05 through 23:59:59Z...\n",
      "Total API calls made so far: 2\n",
      "Pausing for 16.4940822757135 after scraping data for 2024-06-05...\n",
      "__________________________________\n",
      "Scraping Bar Data complete for timerange: 2024-06-05 08:00:00+00:00  -  2024-06-05 23:59:00+00:00\n",
      "Total count of data records queried: 785\n",
      "Count of fresh data records actually inserted into STG_SYMBOL_DATA: 0\n"
     ]
    }
   ],
   "source": [
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# Define the list of stocks\n",
    "stocks = [\"SPY\"]\n",
    "\n",
    "# Define the time range for historical data\n",
    "start_date = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "end_date = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Download and store historical data in the staging table\n",
    "data = {}\n",
    "for stock in stocks:\n",
    "    # Download historical data\n",
    "    stock_data = atb.download_bar_data(stock, TimeFrame(1, TimeFrameUnit.Minute), start_date, end_date)\n",
    "    data[stock] = stock_data\n",
    "\n",
    "# Store all downloaded data in the staging table\n",
    "for stock, stock_data in data.items():\n",
    "    atb.db_append_no_duplicates('STG_SYMBOL_DATA', stock_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated table SYMBOL_SEQUENCE_MODELING.\n",
      "Prepared sequence model data stored in SYMBOL_SEQUENCE_MODELING. Number of rows inserted: 916895.\n"
     ]
    }
   ],
   "source": [
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# Define the list of stocks\n",
    "stocks = [\"SPY\"]\n",
    "\n",
    "# Step 2a: Transfer data from the staging table to the modeling table\n",
    "# for stock in stocks:\n",
    "#     atb.classification_modeling_prep('STG_SYMBOL_DATA', stock, 'SYMBOL_DATA_MODELING')\n",
    "\n",
    "# Step 2b: Transfer data from the staging table to the modeling table\n",
    "for stock in stocks:\n",
    "    atb.sequence_modeling_prep('STG_SYMBOL_DATA', stock, 'SYMBOL_SEQUENCE_MODELING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Classification Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing symbol: SPY\n",
      "Dataset generated from SYMBOL_SEQUENCE_MODELING table.\n",
      "Data sorted by timestamp.\n",
      "Price movement target created for target_threshold of 0.001.\n",
      "Random sets of dates generated to split training & validation.\n",
      "Data split into training & validation.\n",
      "Sequences created for training & validation data.\n",
      "Data filtered to trading hours only.\n",
      "Data smoted.\n",
      "Data normalized. Modeling data is prepared in full. \n",
      "Starting grid search for LSTM model.\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Scripts\\alpaca\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1052: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "LSTM - F1 Score: 0.541587605684943\n",
      "Confusion Matrix:\n",
      " [[1480 2232]\n",
      " [1826 3405]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.40      0.42      3712\n",
      "           1       0.60      0.65      0.63      5231\n",
      "\n",
      "    accuracy                           0.55      8943\n",
      "   macro avg       0.53      0.52      0.52      8943\n",
      "weighted avg       0.54      0.55      0.54      8943\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________\n",
      "No feature importance or coefficients for LSTM\n",
      "____________________________________________\n",
      "____________________________________________\n",
      "Completed training for LSTM model\n",
      "\n",
      "Starting grid search for RNN model.\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and use the trading bot\n",
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# Select symbol, features, target move %, and sequence_window (how far is our look-back period per observation) to run time-series models on\n",
    "symbols = ['SPY']\n",
    "features = ['CLOSE','TRADE_COUNT','VOLUME','VWAP','RSI','ATR','CCI','BOLLINGER_WIDTH_PERCENT','BOLLINGER_POSITION','EMA_30','EMA_120','EMA_30_120_PCT_DIFF']\n",
    "sequence_window = 60\n",
    "target_threshold = .002\n",
    "\n",
    "# Run models for each symbol\n",
    "for symbol in symbols:\n",
    "    print(f\"Processing symbol: {symbol}\")\n",
    "    atb.sequence_modeling_build(features, symbol, sequence_window, target_threshold)\n",
    "    print(f\"Completed processing for symbol: {symbol}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Query current day's data, insert to stage, add indicators & add model predictions based on already created models.\n",
    "\n",
    "\"\"\"\n",
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# Define the list of stocks\n",
    "stocks = [\"SPY\"]\n",
    "\n",
    "# Just query current day's data\n",
    "# start_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Step 1: Download ticks and add to stage table\n",
    "# data = {}\n",
    "# for stock in stocks:\n",
    "    # Download historical data\n",
    "    # stock_data = atb.download_bar_data(stock, TimeFrame(1, TimeFrameUnit.Minute), start_date, end_date)\n",
    "    # data[stock] = stock_data\n",
    "\n",
    "# Store all downloaded data in the staging table\n",
    "# for stock, stock_data in data.items():\n",
    "#     atb.db_append_no_duplicates('STG_SYMBOL_DATA', stock_data)\n",
    "#     print(f\"Stage table updated with data for {stock}\")\n",
    "\n",
    "# Step 2: Transfer data from the staging table to the modeling staging table\n",
    "for stock in stocks:\n",
    "    atb.classification_modeling_apply('STG_SYMBOL_DATA', stock, \"CLASSIFICATION_MODEL_PREDICTIONS\")\n",
    "    print(f\"Model predictions for {stock} stored in CLASSIFICATION_MODEL_PREDICTIONS.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
