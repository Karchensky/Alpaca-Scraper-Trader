{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpaca Trading Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data & API packages\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import TimeFrame, TimeFrameUnit\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import ta\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "#modeling packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import tensorflow as tf \n",
    "\n",
    "######################################################  Keras Wrapper Class (model build support) ######################################################\n",
    "class KerasModelWrapper(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    this class is used as a keras wrapper so we can run ouur sequential models with various hyperparameters \n",
    "    \"\"\"\n",
    "    def __init__(self, model_type='LSTM', units=50, layers=2, dropout_rate=0.2, learning_rate=0.001, input_shape=(30, 12), epochs=10, batch_size=32):\n",
    "        self.model_type = model_type\n",
    "        self.units = units\n",
    "        self.layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_shape = input_shape\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Builds and compiles the Keras model based on the specified hyperparameters.\n",
    "        \"\"\"\n",
    "        sequence_input = tf.keras.Input(shape=self.input_shape)\n",
    "        x = sequence_input\n",
    "        for _ in range(self.layers):\n",
    "            if self.model_type == 'LSTM':\n",
    "                x = tf.keras.layers.LSTM(self.units, return_sequences=True if _ < self.layers - 1 else False)(x)\n",
    "            elif self.model_type == 'RNN':\n",
    "                x = tf.keras.layers.SimpleRNN(self.units, return_sequences=True if _ < self.layers - 1 else False)(x)\n",
    "            elif self.model_type == 'CNN':\n",
    "                x = tf.keras.layers.Conv1D(filters=self.units, kernel_size=3, activation='relu')(x)\n",
    "                x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "            x = tf.keras.layers.Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs=sequence_input, outputs=outputs)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the model using the provided training data.\n",
    "        \"\"\"\n",
    "        self.model = self.build_model()\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, callbacks=[early_stopping], verbose=3)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Generates class predictions for the input samples.\n",
    "        \"\"\"\n",
    "        return (self.model.predict(X) > 0.5).astype(\"int32\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Generates probability estimates for the input samples.\n",
    "        \"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculates the F1 score of the model on the given test data and labels.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return f1_score(y, y_pred, average='weighted')\n",
    "\n",
    "######################################################  The meat ######################################################\n",
    "class AlpacaTradingBot:\n",
    "    \"\"\"\n",
    "    This class is used to scrape stock data from the Alpaca API, store it locally in a SQLite Database, add indicators & perform predective analytics.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys_file_path='alpaca_keys.txt', base_url='https://paper-api.alpaca.markets', database_path=r'D:\\Scripts\\alpaca\\alpaca_algo_trading\\alpaca_data.db'):\n",
    "        with open(keys_file_path, 'r') as file:\n",
    "            self.api_key = file.readline().strip()\n",
    "            self.api_secret = file.readline().strip()\n",
    "        self.base_url = base_url\n",
    "        self.api = tradeapi.REST(self.api_key, self.api_secret, base_url=base_url)\n",
    "        self.database_path = database_path\n",
    "        self.api  \n",
    "        # Define model types & provide dicationary of parameters\n",
    "        self.model_categories = {\n",
    "        'sequential': ['LSTM', 'RNN', 'CNN'],\n",
    "        'classification': ['K-Nearest Neighbors', 'XGBoost', 'Logistic Regression', 'Random Forest', 'Neural Network', 'Gradient Boosting', 'Support Vector Machine']\n",
    "        }\n",
    "        self.models = {\n",
    "            \"K-Nearest Neighbors\": (KNeighborsClassifier(), {\n",
    "                'n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "            }),\n",
    "            \"XGBoost\": (XGBClassifier(eval_metric='mlogloss'), {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'reg_alpha': [0, 0.1, 1],\n",
    "                'reg_lambda': [1, 1.5, 2]\n",
    "            }),\n",
    "            \"Logistic Regression\": (LogisticRegression(max_iter=1000, random_state=42), {\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'C': [0.1, 1, 10],\n",
    "                'solver': ['liblinear', 'saga'],\n",
    "                'class_weight': ['balanced', None]\n",
    "            }),\n",
    "            \"Random Forest\": (RandomForestClassifier(), {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [None, 10, 20, 30],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'bootstrap': [True, False],\n",
    "                'class_weight': ['balanced', 'balanced_subsample', None]\n",
    "            }),\n",
    "            \"Neural Network\": (MLPClassifier(max_iter=1000, random_state=42), {\n",
    "                'hidden_layer_sizes': [(50,), (100,), (150,), (100, 50), (100, 100)],\n",
    "                'activation': ['relu', 'tanh', 'logistic'],\n",
    "                'solver': ['adam', 'sgd', 'lbfgs'],\n",
    "                'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "                'learning_rate': ['constant', 'adaptive'],\n",
    "                'learning_rate_init': [0.001, 0.01, 0.1]\n",
    "            }),\n",
    "            \"Gradient Boosting\": (GradientBoostingClassifier(), {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 4, 5, 6],\n",
    "                'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }),\n",
    "            \"Support Vector Machine\": (SVC(), {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'class_weight': ['balanced', None]\n",
    "            }),\n",
    "            \"Sequential Model Parameters\": {\n",
    "                'units': [50, 100, 150],\n",
    "                'learning_rate': [0.001, 0.01, 0.1],\n",
    "                'batch_size': [32, 64, 128],\n",
    "                'epochs': [10, 30, 50]\n",
    "            }\n",
    "        }\n",
    "\n",
    "##### API Scraping methods\n",
    "\n",
    "    def download_bar_data(self, stock, timeframe, start_date, end_date, pause=True):\n",
    "        \"\"\"\n",
    "        This method is used to scrape bar data from the Alpaca API. \n",
    "        \"\"\"\n",
    "        all_data = []\n",
    "\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            # Check if it's a weekend (Saturday or Sunday) & skip, if so\n",
    "            # if  datetime.strptime(current_date, \"%Y-%m-%d\").weekday() >= 5:\n",
    "            #     current_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "            #     continue\n",
    "\n",
    "            # Split the day into two segments: midnight to noon, and noon to end of day because the API can only return 1,000 data points a day\n",
    "            segments = [\n",
    "                (f\"{current_date}T00:00:00Z\", f\"{current_date}T11:59:59Z\"),\n",
    "                (f\"{current_date}T12:00:00Z\", f\"{current_date}T23:59:59Z\")\n",
    "            ]\n",
    "\n",
    "            for start_time, end_time in segments:\n",
    "                try:\n",
    "                    # Get data for the current day segment\n",
    "                    bars = self.api.get_bars(stock, timeframe, start=start_time, end=end_time, limit=1000).df\n",
    "                    self.api_call_count += 1  # Increment the API call counter\n",
    "                    if not bars.empty:\n",
    "                        bars['symbol'] = stock  # Add the stock symbol column\n",
    "                        all_data.append(bars)\n",
    "                        print(f\"Data scraped for {current_date} through {end_time.split('T')[1]}...\")\n",
    "                    else:\n",
    "                        print(f\"No data available for segment: {current_date}: {start_time.split('T')[1]} - {end_time.split('T')[1]}\")\n",
    "                except tradeapi.rest.APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    pause_duration = random.uniform(120, 240)  # Longer pause if an API error occurs\n",
    "                    time.sleep(pause_duration)\n",
    "                    continue\n",
    "                \n",
    "            # Random pause after each scrape.. API rate limits need to be considered when populating historic data to the database.\n",
    "            if pause and (datetime.strptime(current_date, \"%Y-%m-%d\") - datetime.strptime(start_date, \"%Y-%m-%d\")).days % 2 == 0:\n",
    "                print(f\"Total API calls made so far: {self.api_call_count}\")\n",
    "                pause_duration = random.uniform(10, 30)\n",
    "                print(f\"Pausing for {pause_duration} after scraping data for {current_date}...\")\n",
    "                print(f\"__________________________________\")\n",
    "                time.sleep(pause_duration)\n",
    "\n",
    "            # Move to the next day\n",
    "            current_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        if all_data:\n",
    "            combined_data = pd.concat(all_data)\n",
    "            combined_data = combined_data.reset_index()  # Ensure the index is reset to have 'timestamp' as a column\n",
    "            print(\"Scraping Bar Data complete for timerange:\", combined_data['timestamp'].min(), \" - \", combined_data['timestamp'].max())\n",
    "            return combined_data[['symbol'] + [col for col in combined_data.columns if col not in ['symbol']]]\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "##### Model Preperation Methods\n",
    "\n",
    "    def fetch_data(self, input_table, symbol):\n",
    "        \"\"\"\n",
    "        This method grabs stock data from the stage table for model preperation\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        try:\n",
    "            input_data = pd.read_sql(f\"SELECT distinct * FROM {input_table} WHERE SYMBOL = '{symbol}'\", conn)\n",
    "            if input_data.empty:\n",
    "                raise ValueError(f\"No data found for symbol {symbol} in table {input_table}\")\n",
    "            else:\n",
    "                print(f\"Data fetched for {symbol} from table {input_table}.\")\n",
    "            return input_data\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def preprocess_data(self, input_data):\n",
    "        \"\"\"\n",
    "        This method gets our timestamps in EST\n",
    "        \"\"\"\n",
    "        input_data['timestamp_utc'] = pd.to_datetime(input_data['TIMESTAMP'], utc=True)\n",
    "        input_data['timestamp_est'] = input_data['timestamp_utc'].dt.tz_convert('US/Eastern')\n",
    "        input_data['trading_hours_ind'] = (input_data['timestamp_est'].dt.time >= datetime.strptime('09:30', '%H:%M').time()) & \\\n",
    "                                          (input_data['timestamp_est'].dt.time <= datetime.strptime('16:00', '%H:%M').time())\n",
    "        input_data['trading_hours_ind_ext'] = (input_data['timestamp_est'].dt.time >= datetime.strptime('10:30', '%H:%M').time()) & \\\n",
    "                                              (input_data['timestamp_est'].dt.time <= datetime.strptime('15:00', '%H:%M').time())\n",
    "        input_data['date'] = input_data['timestamp_est'].dt.date\n",
    "        print(f\"Timestamps converted to EST.\")\n",
    "        return input_data\n",
    "    \n",
    "    def structure_dataframe(self, data, table_name):\n",
    "        \"\"\"\n",
    "        This matches our columns to the database to make sure everything inserts correctly\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        try:\n",
    "            cursor = conn.execute(f\"PRAGMA table_info({table_name})\")\n",
    "            table_columns = [row[1].upper() for row in cursor.fetchall()]\n",
    "\n",
    "            # Match DataFrame columns to table columns (case insensitive)\n",
    "            data_columns_upper = {col.upper(): col for col in data.columns}\n",
    "            matched_columns = [data_columns_upper[col] for col in table_columns if col in data_columns_upper]\n",
    "\n",
    "            # Identify dropped columns\n",
    "            dropped_columns = set(data.columns) - set(matched_columns)\n",
    "            print(f\"Dropped columns: {dropped_columns}\")\n",
    "\n",
    "            # Reorder and drop columns\n",
    "            reordered_data = data[matched_columns]\n",
    "            print(f\"Dataframe columns reordered to match {table_name}.\")\n",
    "            return reordered_data\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def save_dataframe(self, output_data, output_table):\n",
    "        \"\"\"\n",
    "        This writes our table to the database.. truncates any existing data first\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        try:\n",
    "            # Truncate existing table\n",
    "            self.db_write(f\"DELETE FROM {output_table}\")\n",
    "            print(f\"Truncated table {output_table}.\")\n",
    "\n",
    "            # Insert into table\n",
    "            self.db_append(output_table, output_data)\n",
    "            print(f\"Prepared model data stored in {output_table}. Number of rows inserted: {len(output_data)}.\")\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def calculate_indicators_sequence(self, input_data):\n",
    "        \"\"\"\n",
    "        This method creates a bunch of technical indicators we will use for sequential modeling\n",
    "        \"\"\"\n",
    "        input_data['rsi'] = ta.momentum.rsi(input_data['CLOSE'], window=60)\n",
    "        input_data['atr'] = ta.volatility.average_true_range(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=60)\n",
    "        input_data['cci'] = ta.trend.cci(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=60)\n",
    "        bb_indicator = ta.volatility.BollingerBands(input_data['CLOSE'], window=60, window_dev=2)\n",
    "        input_data['bollinger_width_percent'] = (bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()) / input_data['CLOSE']\n",
    "        input_data['bollinger_position'] = bb_indicator.bollinger_pband()\n",
    "        input_data['sma_30'] = ta.trend.sma_indicator(input_data['CLOSE'], window=30)\n",
    "        input_data['sma_120'] = ta.trend.sma_indicator(input_data['CLOSE'], window=120)\n",
    "        input_data['ema_30'] = ta.trend.ema_indicator(input_data['CLOSE'], window=30)\n",
    "        input_data['ema_120'] = ta.trend.ema_indicator(input_data['CLOSE'], window=120)\n",
    "        input_data['sma_30_120_pct_diff'] = ((input_data['sma_30'] - input_data['sma_120']) / input_data['sma_120']) * 100\n",
    "        input_data['ema_30_120_pct_diff'] = ((input_data['ema_30'] - input_data['ema_120']) / input_data['ema_120']) * 100\n",
    "        print(f\"Technical indicators created.\")\n",
    "        return input_data\n",
    "\n",
    "    def calculate_indicators_classification(self, input_data):\n",
    "        \"\"\"\n",
    "        This method creates a bunch of technical indicators we will use for classification modeling\n",
    "        \"\"\"\n",
    "        input_data['sma_15'] = ta.trend.sma_indicator(input_data['CLOSE'], window=15)\n",
    "        input_data['sma_30'] = ta.trend.sma_indicator(input_data['CLOSE'], window=30)\n",
    "        input_data['sma_120'] = ta.trend.sma_indicator(input_data['CLOSE'], window=120)\n",
    "        input_data['ema_15'] = ta.trend.ema_indicator(input_data['CLOSE'], window=15)\n",
    "        input_data['ema_30'] = ta.trend.ema_indicator(input_data['CLOSE'], window=30)\n",
    "        input_data['ema_120'] = ta.trend.ema_indicator(input_data['CLOSE'], window=120)\n",
    "        input_data['sma_15_120_pct_diff'] = ((input_data['sma_15'] - input_data['sma_120']) / input_data['sma_120']) * 100\n",
    "        input_data['ema_15_120_pct_diff'] = ((input_data['ema_15'] - input_data['ema_120']) / input_data['ema_120']) * 100\n",
    "        input_data['macd'] = ta.trend.macd(input_data['CLOSE'], window_slow=26, window_fast=12)\n",
    "        input_data['macd_signal'] = ta.trend.macd_signal(input_data['CLOSE'], window_slow=26, window_fast=12, window_sign=9)\n",
    "        input_data['macd_diff'] = input_data['macd'] - input_data['macd_signal']\n",
    "        input_data['adx'] = ta.trend.adx(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=14)\n",
    "        input_data['rsi'] = ta.momentum.rsi(input_data['CLOSE'], window=14)\n",
    "        input_data['stochastic_k'] = ta.momentum.stoch(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=14, smooth_window=3)\n",
    "        input_data['stochastic_d'] = ta.momentum.stoch_signal(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=14, smooth_window=3)\n",
    "        input_data['cci'] = ta.trend.cci(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=20)\n",
    "        input_data['williams_r'] = ta.momentum.williams_r(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], lbp=14)\n",
    "        bb_indicator = ta.volatility.BollingerBands(input_data['CLOSE'], window=20, window_dev=2)\n",
    "        input_data['bollinger_hband'] = bb_indicator.bollinger_hband()\n",
    "        input_data['bollinger_lband'] = bb_indicator.bollinger_lband()\n",
    "        input_data['bollinger_mband'] = bb_indicator.bollinger_mavg()\n",
    "        input_data['bollinger_width_percent'] = (input_data['bollinger_hband'] - input_data['bollinger_lband']) / input_data['CLOSE']\n",
    "        input_data['bollinger_position'] = bb_indicator.bollinger_pband()\n",
    "        input_data['atr'] = ta.volatility.average_true_range(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=14)\n",
    "        input_data['obv'] = ta.volume.on_balance_volume(input_data['CLOSE'], input_data['VOLUME'])\n",
    "        input_data['vwap'] = input_data.groupby(input_data['timestamp_est'].dt.date).apply(\n",
    "            lambda x: (x['CLOSE'] * x['VOLUME']).cumsum() / x['VOLUME'].cumsum()\n",
    "        ).reset_index(level=0, drop=True)\n",
    "        input_data['ichimoku_tenkan_sen'] = ta.trend.ichimoku_conversion_line(input_data['HIGH'], input_data['LOW'], window1=9, window2=26)\n",
    "        input_data['ichimoku_senkou_span_a'] = ta.trend.ichimoku_a(input_data['HIGH'], input_data['LOW'])\n",
    "        input_data['ichimoku_senkou_span_b'] = ta.trend.ichimoku_b(input_data['HIGH'], input_data['LOW'])\n",
    "        input_data['rvi'] = ta.trend.stc(input_data['CLOSE'])\n",
    "        input_data['force_index'] = ta.volume.force_index(input_data['CLOSE'], input_data['VOLUME'])\n",
    "        ao_indicator = ta.momentum.AwesomeOscillatorIndicator(input_data['HIGH'], input_data['LOW'])\n",
    "        input_data['ao_ind'] = ao_indicator.awesome_oscillator()\n",
    "\n",
    "        # Lagged Features\n",
    "        lagged_features = []\n",
    "        for lag in [1, 2, 3, 4, 5, 10, 20, 30]:\n",
    "            lagged_features.append(input_data['rsi'].shift(lag))\n",
    "            lagged_features.append(input_data['macd_diff'].shift(lag))\n",
    "            lagged_features.append(input_data['cci'].shift(lag))\n",
    "            lagged_features.append(input_data['adx'].shift(lag))\n",
    "            lagged_features.append(input_data['CLOSE'].shift(lag))\n",
    "            lagged_features.append(input_data['VOLUME'].shift(lag))\n",
    "            lagged_features.append(input_data['vwap'].shift(lag))\n",
    "            lagged_features.append(input_data['atr'].shift(lag))\n",
    "        lagged_features_df = pd.concat(lagged_features, axis=1)\n",
    "        lagged_feature_columns = [f'{col}_lag{lag}' for lag in [1, 2, 3, 4, 5, 10, 20, 30] for col in ['rsi', 'macd_diff', 'cci', 'adx', 'CLOSE', 'VOLUME', 'vwap', 'atr']]\n",
    "        lagged_features_df.columns = lagged_feature_columns\n",
    "        input_data = pd.concat([input_data, lagged_features_df], axis=1)\n",
    "\n",
    "        # Polynomial Features\n",
    "        squared_columns = pd.concat([\n",
    "            input_data['rsi'] ** 2,\n",
    "            input_data['macd_diff'] ** 2,\n",
    "            input_data['cci'] ** 2,\n",
    "            input_data['adx'] ** 2,\n",
    "            input_data['bollinger_width_percent'] ** 2,\n",
    "            input_data['stochastic_k'] ** 2,\n",
    "            input_data['stochastic_d'] ** 2,\n",
    "            input_data['williams_r'] ** 2,\n",
    "            input_data['obv'] ** 2,\n",
    "        ], axis=1)\n",
    "        squared_columns.columns = [f'{col}_squared' for col in squared_columns.columns]\n",
    "        input_data = pd.concat([input_data, squared_columns], axis=1)\n",
    "\n",
    "        # Crossed indicators\n",
    "        input_data['SMA_30_RSI'] = input_data['sma_30'] * input_data['rsi']\n",
    "        input_data['MACD_STOCHASTIC_K'] = input_data['macd'] * input_data['stochastic_k']\n",
    "        input_data['EMA_30_ATR'] = input_data['ema_30'] * input_data['atr']\n",
    "        input_data['RSI_MACD_DIFF_INTERACTION'] = input_data['rsi'] * input_data['macd_diff']\n",
    "        input_data['SMA_30_ADX_INTERACTION'] = input_data['sma_30'] * input_data['adx']\n",
    "        input_data['BOLLINGER_POSITION_ATR_INTERACTION'] = input_data['bollinger_position'] * input_data['atr']\n",
    "        input_data['HIGH_LOW_RANGE'] = input_data['HIGH'] - input_data['LOW']\n",
    "        input_data['CLOSE_PERCENT_RANGE'] = ((input_data['CLOSE'] - input_data['LOW']) / input_data['HIGH_LOW_RANGE']) * 100\n",
    "\n",
    "        # Time-based features\n",
    "        input_data['DAY_OF_WEEK'] = input_data['timestamp_est'].dt.dayofweek\n",
    "        input_data['HOUR_OF_DAY'] = input_data['timestamp_est'].dt.hour\n",
    "        input_data['TIME_OF_DAY'] = input_data['timestamp_est'].dt.time\n",
    "\n",
    "        # Price-based features\n",
    "        input_data['DAILY_MOVEMENT_SINCE_OPEN'] = input_data['CLOSE'] - input_data.groupby(input_data['timestamp_est'].dt.date)['CLOSE'].transform('first')\n",
    "        input_data['WEEKLY_MOVEMENT_SINCE_OPEN'] = input_data['CLOSE'] - input_data.groupby(input_data['timestamp_est'].dt.to_period('W'))['CLOSE'].transform('first')\n",
    "\n",
    "        # Percentage movement features\n",
    "        input_data['PCT_MOVEMENT_20_PERIODS'] = input_data['CLOSE'].pct_change(periods=20) * 100\n",
    "        input_data['PCT_MOVEMENT_60_PERIODS'] = input_data['CLOSE'].pct_change(periods=60) * 100\n",
    "\n",
    "        print(f\"Technical indicators created.\")\n",
    "        return input_data\n",
    "    \n",
    "\n",
    "    def sequence_modeling_prep(self, input_table, output_table, symbol):\n",
    "        \"\"\"\n",
    "        This method cleans the scraped raw stage data, adds indicator calculations on it inserts the otuput into a fresh table for sequential modeling.\n",
    "        \"\"\"\n",
    "        print(f\"Preparing {symbol} data for sequential modeling.\")\n",
    "        input_data = self.fetch_data(input_table, symbol)\n",
    "        preprocessed_data = self.preprocess_data(input_data)\n",
    "        indicators_data = self.calculate_indicators_sequence(preprocessed_data)\n",
    "        reordered_data = self.structure_dataframe(indicators_data, output_table)\n",
    "        self.save_dataframe(reordered_data, output_table)\n",
    "        print(f\"__________________________________\")\n",
    "\n",
    "    def modeling_prepare(self, input_table, output_table, symbol):\n",
    "        \"\"\"\n",
    "        This method cleans the scraped raw stage data, adds indicator calculations on it inserts the otuput into a fresh table for classification modeling..\n",
    "        \"\"\"\n",
    "        print(f\"Preparing {symbol} data for classification modeling.\")\n",
    "        input_data = self.fetch_data(input_table, symbol)\n",
    "        preprocessed_data = self.preprocess_data(input_data)\n",
    "        indicators_data = self.calculate_indicators_classification(preprocessed_data)\n",
    "        reordered_data = self.structure_dataframe(indicators_data, output_table)\n",
    "        self.save_dataframe(reordered_data, output_table)\n",
    "        print(f\"__________________________________\")\n",
    "\n",
    "##### Model Building Methods\n",
    "\n",
    "    def prepare_data_for_modeling(self, database_path, symbol, table_name, start_date):\n",
    "        \"\"\"\n",
    "        This method grabs the prepared modeling data from our database and orders it\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(database_path)\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table_name} WHERE SYMBOL = '{symbol}' AND DATE >= '{start_date}'\", conn)\n",
    "        conn.close()\n",
    "        print(f\"Dataset generated from {table_name} table.\")\n",
    "        df.sort_values('TIMESTAMP_EST', inplace=True)\n",
    "        print(f\"Data sorted by timestamp.\")\n",
    "        return df\n",
    "\n",
    "    def split_data_by_dates(self, df):\n",
    "        \"\"\"\n",
    "        This method splits our data into training & validation by breaking out dates\n",
    "        \"\"\"\n",
    "        unique_dates = df['DATE'].unique()\n",
    "        train_dates, val_dates = train_test_split(unique_dates, test_size=0.3, random_state=42)\n",
    "        print(f\"Random sets of dates generated to split training & validation.\")\n",
    "        train_data = df[df['DATE'].isin(train_dates)].copy()\n",
    "        val_data = df[df['DATE'].isin(val_dates)].copy()\n",
    "        print(f\"Data split into training & validation.\")\n",
    "        return train_data, val_data\n",
    "    \n",
    "    def create_targets(self, input_data, threshold=0.001):\n",
    "        \"\"\"\n",
    "        This method creates our target variable. We want to see which direction the stock is going to break first.\n",
    "        \"\"\"\n",
    "        input_data['TARGET_MOVEMENT_DIRECTION'] = np.nan\n",
    "\n",
    "        for i in range(len(input_data)):\n",
    "            for j in range(i + 1, len(input_data)):\n",
    "                price_diff = input_data['CLOSE'].iloc[j] - input_data['CLOSE'].iloc[i]\n",
    "                price_change_pct = price_diff / input_data['CLOSE'].iloc[i]\n",
    "\n",
    "                if price_change_pct >= threshold:\n",
    "                    input_data.at[i, 'TARGET_MOVEMENT_DIRECTION'] = 1  # Up\n",
    "                    break\n",
    "                elif price_change_pct <= -threshold:\n",
    "                    input_data.at[i, 'TARGET_MOVEMENT_DIRECTION'] = 0  # Down\n",
    "                    break\n",
    "\n",
    "        input_data.dropna(subset=['TARGET_MOVEMENT_DIRECTION'], inplace=True)\n",
    "        input_data['TARGET_MOVEMENT_DIRECTION'] = input_data['TARGET_MOVEMENT_DIRECTION'].astype(int)\n",
    "        print(f\"Targets created at target thershold of: {threshold}.\")\n",
    "        return input_data\n",
    "    \n",
    "    def create_sequences(self, df, feature_columns, target_column, sequence_length):\n",
    "        \"\"\"\n",
    "        This method creates our sequences for our sequential modeling approaches\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        trading_hours = []\n",
    "        for i in range(len(df) - sequence_length):\n",
    "            seq_features = df[feature_columns].iloc[i:i+sequence_length].values\n",
    "            seq_target = df[target_column].iloc[i + sequence_length]\n",
    "            seq_trading_hours = df['TRADING_HOURS_IND_EXT'].iloc[i + sequence_length]  # The trading hours indicator for the target row\n",
    "            sequences.append(seq_features)\n",
    "            targets.append(seq_target)\n",
    "            trading_hours.append(seq_trading_hours)\n",
    "        return np.array(sequences), np.array(targets), np.array(trading_hours)\n",
    "\n",
    "    def smote_data(self, X_train, y_train, model_type=None):\n",
    "        \"\"\"\n",
    "        Apply SMOTE to the training data.\n",
    "        Sequential/classification models have different input datashapes\n",
    "        \"\"\"\n",
    "        sm = SMOTE(random_state=42)\n",
    "        \n",
    "        if model_type == 'sequential':\n",
    "            # Reshape for SMOTE, then reshape back for sequential models\n",
    "            X_train_smoted, y_train_smoted = sm.fit_resample(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "            X_train_smoted = X_train_smoted.reshape(X_train_smoted.shape[0], *X_train.shape[1:])\n",
    "        else:\n",
    "            # Directly apply SMOTE for classification models\n",
    "            X_train_smoted, y_train_smoted = sm.fit_resample(X_train, y_train)\n",
    "        \n",
    "        print(f\"Data smoted.\")\n",
    "        return X_train_smoted, y_train_smoted\n",
    "\n",
    "    def normalize_data(self, X_train, X_val, model_type=None):\n",
    "        \"\"\"\n",
    "        Normalize data\n",
    "        \"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        if model_type == 'sequential':\n",
    "            # Reshape for normalization, then reshape back for sequential models\n",
    "            X_train_normalized = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "            X_val_normalized = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "        else:\n",
    "            # Directly normalize classification models\n",
    "            X_train_normalized = scaler.fit_transform(X_train)\n",
    "            X_val_normalized = scaler.transform(X_val)\n",
    "        \n",
    "        print(f\"Data normalized.\")\n",
    "        return X_train_normalized, X_val_normalized\n",
    "\n",
    "    def run_model_grid_search(self, model_type, X_train, y_train, X_val, y_val, symbol, features, n_iter, cv):\n",
    "        \"\"\"\n",
    "        This method runs grid search & evaluates the outcome of various hyperparameters for both sequential and traditional classification models.\n",
    "        \"\"\"\n",
    "        if model_type in self.model_categories['sequential']:\n",
    "            param_grid = self.models[\"Sequential Model Parameters\"]\n",
    "            param_grid['model_type'] = [model_type]\n",
    "            param_grid['input_shape'] = [X_train.shape[1:]]\n",
    "            model = KerasModelWrapper()\n",
    "        else:\n",
    "            # For classification models, the model is selected from a predefined dictionary of models and their parameter grids\n",
    "            model, param_grid = self.models[model_type]\n",
    "\n",
    "        grid_search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            scoring='f1_weighted',\n",
    "            random_state=42,\n",
    "            verbose=3\n",
    "        )\n",
    "\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        self.evaluate_model(best_model, model_type, X_val, y_val, symbol, features)\n",
    "        print(f\"Completed training for {model_type} model\\n\")\n",
    "        print(f\"__________________________________\")\n",
    "\n",
    "    def modeling_build(self, model_types, table_name, features, symbol, target_threshold, sequence_window, n_iter=20, cv=3, start_date='2020-01-01'):\n",
    "        \"\"\"\n",
    "        This method executes our models\n",
    "        \"\"\"\n",
    "        # Do minor data cleaning & create our target variables\n",
    "        df = self.prepare_data_for_modeling(self.database_path, symbol, table_name, start_date)\n",
    "        df = self.create_targets(df, target_threshold)\n",
    "        \n",
    "        # Define model types as sequential or classification based on class dictionary\n",
    "        sequential_model_types = self.model_categories['sequential']\n",
    "        classification_model_types = self.model_categories['classification']\n",
    "\n",
    "        # Separate model types into sequential and classification\n",
    "        sequential_models = [m for m in model_types if m in sequential_model_types]\n",
    "        classification_models = [m for m in model_types if m in classification_model_types]\n",
    "\n",
    "        # If we have any sequential models, prepare for modeling by creating sequences, filtering, smoting, and normalizing\n",
    "        if sequential_models:\n",
    "            train_data, val_data = self.split_data_by_dates(df)\n",
    "            X_train, y_train, train_targets = self.create_sequences(train_data, features, 'TARGET_MOVEMENT_DIRECTION', sequence_window)\n",
    "            X_val, y_val, val_targets = self.create_sequences(val_data, features, 'TARGET_MOVEMENT_DIRECTION', sequence_window)\n",
    "            print(f\"Sequences created with window of: {sequence_window}.\")\n",
    "            X_train_filtered = X_train[train_targets == 1]\n",
    "            y_train_filtered = y_train[train_targets == 1]\n",
    "            X_val_filtered = X_val[val_targets == 1]\n",
    "            y_val_filtered = y_val[val_targets == 1]\n",
    "\n",
    "            X_train_smoted, y_train_smoted = self.smote_data(X_train_filtered, y_train_filtered, model_type='sequential')\n",
    "            X_train_normalized, X_val_normalized = self.normalize_data(X_train_smoted, X_val_filtered, model_type='sequential')\n",
    "\n",
    "            # For each sequential model run grid search & record results\n",
    "            for model_type in sequential_models:\n",
    "                print(f\"Beginning grid search for model: {model_type}.\")\n",
    "                self.run_model_grid_search(model_type, X_train_normalized, y_train_smoted, X_val_normalized, y_val_filtered, symbol, features, n_iter, cv)\n",
    "\n",
    "        # If we have any classification models, do the same less create sequences.. and only normmalize if it's SVM or NN\n",
    "        if classification_models:\n",
    "            df_classification = df[df['TRADING_HOURS_IND_EXT'] == 1]\n",
    "\n",
    "            train_data, val_data = self.split_data_by_dates(df_classification)\n",
    "            X_train = train_data[features]\n",
    "            y_train = train_data['TARGET_MOVEMENT_DIRECTION']\n",
    "            X_val = val_data[features]\n",
    "            y_val = val_data['TARGET_MOVEMENT_DIRECTION']\n",
    "            X_train_smoted, y_train_smoted = self.smote_data(X_train, y_train)\n",
    "\n",
    "            # For each classification model run grid search & record results\n",
    "            for model_type in classification_models:\n",
    "                \n",
    "                # For SVM & Neural Net we will normalize the data first\n",
    "                if model_type in ['Support Vector Machine', 'Neural Network']:\n",
    "                    X_train_normalized, X_val_normalized = self.normalize_data(X_train_smoted, X_val)\n",
    "                    self.run_model_grid_search(model_type, X_train_normalized, y_train_smoted, X_val_normalized, y_val, symbol, features, n_iter, cv)\n",
    "                else:\n",
    "                    self.run_model_grid_search(model_type, X_train_smoted, y_train_smoted, X_val, y_val, symbol, features, n_iter, cv)\n",
    "\n",
    "    def evaluate_model(self, model, model_name, X_val, y_val, symbol, features):\n",
    "        \"\"\"\n",
    "        This method saves our model output & evaluates its results\n",
    "        \"\"\"\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
    "        f1 = f1_score(y_val, y_pred_classes, average='weighted', zero_division=0)\n",
    "        cm = confusion_matrix(y_val, y_pred_classes)\n",
    "        cr = classification_report(y_val, y_pred_classes, zero_division=0)\n",
    "\n",
    "        print(f\"___\")\n",
    "        print(f\"{model_name} - F1 Score: {f1}\")\n",
    "        print(f\"___\")\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "        print(f\"___\")\n",
    "        print(\"Classification Report:\\n\", cr)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.title(f'Confusion Matrix: {model_name}')\n",
    "        print(f\"___\")\n",
    "        plt.show()\n",
    "        print(f\"___\")\n",
    "\n",
    "        # Save the model with a unique filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        if model_name in self.model_categories['sequential']:\n",
    "            model.model.save(f'{symbol}_{model_name}_{timestamp}.keras')\n",
    "        else:\n",
    "            joblib.dump(model, f'{symbol}_{model_name}_{timestamp}.pkl')\n",
    "\n",
    "        # Log results to the database\n",
    "        evaluation_results = {\n",
    "            'SYMBOL': symbol,\n",
    "            'MODEL_NAME': model_name,\n",
    "            'F1_SCORE': f1,\n",
    "            'CONFUSION_MATRIX': str(cm),\n",
    "            'CLASSIFICATION_REPORT': cr,\n",
    "            'BEST_PARAMS': str(model.get_params() if hasattr(model, \"get_params\") else {})\n",
    "        }\n",
    "        results_df = pd.DataFrame([evaluation_results])\n",
    "        self.db_append('CLASSIFICATION_MODEL_RESULTS', results_df)\n",
    "\n",
    "        # Feature Importance or Coefficients\n",
    "        if hasattr(model, \"feature_importances_\"):\n",
    "            importances = model.feature_importances_\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'feature': features,\n",
    "                'importance': importances\n",
    "            }).sort_values(by='importance', ascending=False)\n",
    "            print(f\"Feature importances for {model_name}:\")\n",
    "            print(feature_importance_df)\n",
    "        elif hasattr(model, \"coef_\"):\n",
    "            coefs = model.coef_[0]\n",
    "            coef_df = pd.DataFrame({\n",
    "                'feature': features,\n",
    "                'coefficient': coefs\n",
    "            }).sort_values(by='coefficient', ascending=False)\n",
    "            print(f\"Coefficients for {model_name}:\")\n",
    "            print(coef_df)\n",
    "        else:\n",
    "            print(f\"No feature importance or coefficients for {model_name}\")\n",
    "        print(f\"___\")\n",
    "\n",
    "##### Database write/read methods\n",
    "\n",
    "    def db_write(self, sql_statement):\n",
    "        \"\"\"\n",
    "        Basic functionality to execute an SQL statement against our database\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database & create a cursor object\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Execute the SQL statement\n",
    "        cur.execute(sql_statement)\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def db_append(self, table_name, data_frame):\n",
    "        \"\"\"\n",
    "        Basic functionality to append/insert a dataframe into a specified table in our database\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        # Append our dataframe into our table\n",
    "        data_frame.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "    def db_append_no_duplicates(self, table_name, data_frame):\n",
    "        \"\"\"\n",
    "        Variation of db_append that makes sure we aren't inserting duplicates by first checking against the primary key of the table records are being inserted into\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Get the primary key column names\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        table_info = cursor.fetchall()\n",
    "\n",
    "        # Identify columns that are part of the primary key\n",
    "        primary_key_columns = [column[1] for column in table_info if column[5] > 0]\n",
    "\n",
    "        # If no primary key columns found, fall back to db_append method\n",
    "        if not primary_key_columns:\n",
    "            data_frame.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "            \n",
    "            # Commit the changes & close the connection\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return\n",
    "    \n",
    "        # Convert primary key columns and DataFrame columns to uppercase for case-insensitive comparison\n",
    "        primary_key_columns_upper = [col.upper() for col in primary_key_columns]\n",
    "        data_frame.columns = data_frame.columns.str.upper()\n",
    "\n",
    "        # Ensure the DataFrame contains all primary key columns\n",
    "        for column in primary_key_columns_upper:\n",
    "            if column not in data_frame.columns:\n",
    "                raise KeyError(f\"Column '{column}' is missing in the DataFrame\")\n",
    "\n",
    "        # Standardize the timestamp format in the DataFrame\n",
    "        timestamp_column = None\n",
    "        for col in primary_key_columns_upper:\n",
    "            if 'TIMESTAMP' in col:\n",
    "                timestamp_column = col\n",
    "                break\n",
    "        \n",
    "        if timestamp_column:\n",
    "            data_frame[timestamp_column] = pd.to_datetime(data_frame[timestamp_column], utc=True)\n",
    "\n",
    "        # Construct the SELECT statement to fetch existing primary keys from the table\n",
    "        existing_keys_query = f\"SELECT {', '.join(primary_key_columns)} FROM {table_name}\"\n",
    "        existing_keys_df = pd.read_sql(existing_keys_query, conn)\n",
    "\n",
    "        # Standardize the timestamp format in the existing keys DataFrame\n",
    "        if timestamp_column:\n",
    "            existing_keys_df[timestamp_column] = pd.to_datetime(existing_keys_df[timestamp_column], utc=True)\n",
    "\n",
    "        # Construct the composite primary key from the existing keys DataFrame\n",
    "        existing_keys_df['COMPOSITE_KEY'] = existing_keys_df.apply(lambda row: tuple(row), axis=1)\n",
    "        existing_keys = set(existing_keys_df['COMPOSITE_KEY'])\n",
    "\n",
    "        # Print count of rows in the DataFrame before filtering\n",
    "        print(f\"Total count of data records queried: {len(data_frame)}\")\n",
    "\n",
    "        # Construct the composite primary key for new records\n",
    "        data_frame['COMPOSITE_KEY'] = data_frame.apply(lambda row: tuple(row[primary_key_columns_upper]), axis=1)\n",
    "\n",
    "        # Filter out rows with primary keys that already exist in the table\n",
    "        data_frame_new_records = data_frame[~data_frame['COMPOSITE_KEY'].isin(existing_keys)]\n",
    "\n",
    "        # Drop the composite key column\n",
    "        data_frame_new_records = data_frame_new_records.drop(columns=['COMPOSITE_KEY'])\n",
    "\n",
    "        # Append only the new rows into our table\n",
    "        if not data_frame_new_records.empty:\n",
    "            data_frame_new_records.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "\n",
    "        # Print count of rows that will be inserted\n",
    "        print(f\"Count of fresh data records actually inserted into {table_name}: {len(data_frame_new_records)}\")\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "##### work in progress model application (will adjust once we got a good one)\n",
    "    def classification_modeling_apply(self, input_table, symbol, output_table=None):\n",
    "        \"\"\"\n",
    "        This method reads data from the stage table, adds indicators, makes predictions using models,\n",
    "        and stores the results in a new table.\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        try:\n",
    "            # Step 1: Transfer stage data and add indicators\n",
    "            indicators_data = self.transfer_stage_modeling(input_table, symbol)\n",
    "            indicators_data.columns = indicators_data.columns.str.upper()\n",
    "            indicators_data = indicators_data[indicators_data['TRADING_HOURS_IND_EXT'] == 1]\n",
    "\n",
    "            if indicators_data.empty:\n",
    "                raise ValueError(f\"No data found for symbol {symbol} in table {input_table}\")\n",
    "\n",
    "            # Extract feature columns\n",
    "            feature_columns = ['CLOSE','HIGH','LOW','TRADE_COUNT','OPEN','VOLUME','VWAP','SMA_15','SMA_30','SMA_120','EMA_15','EMA_30','EMA_120','SMA_15_120_PCT_DIFF','EMA_15_120_PCT_DIFF',\n",
    "            'MACD','MACD_SIGNAL','MACD_DIFF','ADX','RSI','STOCHASTIC_K','STOCHASTIC_D','CCI','WILLIAMS_R','BOLLINGER_WIDTH_PERCENT','BOLLINGER_POSITION','ATR','OBV',\n",
    "            'ICHIMOKU_TENKAN_SEN','ICHIMOKU_SENKOU_SPAN_A','ICHIMOKU_SENKOU_SPAN_B','RVI','FORCE_INDEX','AO_IND','RSI_LAG1','MACD_DIFF_LAG1','CCI_LAG1','ADX_LAG1','CLOSE_LAG1',\n",
    "            'VOLUME_LAG1','VWAP_LAG1','ATR_LAG1','RSI_LAG2','MACD_DIFF_LAG2','CCI_LAG2','ADX_LAG2','CLOSE_LAG2','VOLUME_LAG2','VWAP_LAG2','ATR_LAG2','RSI_LAG3','MACD_DIFF_LAG3','CCI_LAG3',\n",
    "            'ADX_LAG3','CLOSE_LAG3','VOLUME_LAG3','VWAP_LAG3','ATR_LAG3','RSI_LAG4','MACD_DIFF_LAG4','CCI_LAG4','ADX_LAG4','CLOSE_LAG4','VOLUME_LAG4','VWAP_LAG4','ATR_LAG4','RSI_LAG5',\n",
    "            'MACD_DIFF_LAG5','CCI_LAG5','ADX_LAG5','CLOSE_LAG5','VOLUME_LAG5','VWAP_LAG5','ATR_LAG5','RSI_LAG10','MACD_DIFF_LAG10','CCI_LAG10','ADX_LAG10','CLOSE_LAG10','VOLUME_LAG10',\n",
    "            'VWAP_LAG10','ATR_LAG10','RSI_LAG20','MACD_DIFF_LAG20','CCI_LAG20','ADX_LAG20','CLOSE_LAG20','VOLUME_LAG20','VWAP_LAG20','ATR_LAG20','RSI_LAG30','MACD_DIFF_LAG30','CCI_LAG30',\n",
    "            'ADX_LAG30','CLOSE_LAG30','VOLUME_LAG30','VWAP_LAG30','ATR_LAG30','RSI_SQUARED','CCI_SQUARED','MACD_SQUARED','ADX_SQUARED','BOLLINGER_WIDTH_PERCENT_SQUARED','SMA_30_RSI',\n",
    "            'MACD_STOCHASTIC_K','EMA_30_ATR','RSI_MACD_DIFF_INTERACTION','SMA_30_ADX_INTERACTION','BOLLINGER_POSITION_ATR_INTERACTION','DAY_OF_WEEK','HOUR_OF_DAY',\n",
    "            'DAILY_MOVEMENT_SINCE_OPEN','WEEKLY_MOVEMENT_SINCE_OPEN','PCT_MOVEMENT_20_PERIODS','PCT_MOVEMENT_60_PERIODS' ]\n",
    "            \n",
    "            # Ensure only the feature columns are used for prediction\n",
    "            feature_data = indicators_data[feature_columns]\n",
    "\n",
    "            # Step 2: Load models\n",
    "            models = {\n",
    "                \"Random_Forest\": joblib.load('SPY_Random Forest_20240603_103726.pkl'),\n",
    "                \"XGBoost\": joblib.load('SPY_XGBoost_20240603_135615.pkl'),\n",
    "                \"Gradient_Boosting\": joblib.load('SPY_Gradient Boosting_20240603_173344.pkl'),\n",
    "                \"Neural_Network\": joblib.load('SPY_Neural Network_20240604_104103.pkl'),\n",
    "            }\n",
    "\n",
    "            # Step 3: Make predictions using each model and store the results in the DataFrame\n",
    "            for model_name, model in models.items():\n",
    "                print(f\"Predicting with {model_name} model.\")\n",
    "                if model_name == \"XGBoost\":\n",
    "                    predictions = model.predict(feature_data)\n",
    "                    # Inverse transform the predictions for XGBoost\n",
    "                    predictions = pd.Series(predictions).replace({0: -1, 1: 0, 2: 1}).values\n",
    "                elif model_name == \"Neural_Network\":\n",
    "                    scaler = StandardScaler()\n",
    "                    X_scaled = scaler.fit_transform(feature_data)\n",
    "                    predictions = model.predict(X_scaled)\n",
    "                else:\n",
    "                    predictions = model.predict(feature_data)\n",
    "                \n",
    "                indicators_data[f'{model_name}_prediction'] = predictions\n",
    "                print(f\"{model_name} prediction computed for {symbol}.\")\n",
    "                print(f\"Unique predictions for {model_name}: {pd.Series(predictions).unique()}\")\n",
    "\n",
    "            # Step 4: Store the final DataFrame into the output table\n",
    "            if output_table is not None:\n",
    "                # Truncate the existing table\n",
    "                self.db_write(f\"DELETE FROM {output_table}\")\n",
    "                print(f\"Truncated table {output_table}.\")\n",
    "\n",
    "                # Store the DataFrame in the output table\n",
    "                self.db_append(output_table, indicators_data)\n",
    "                print(f\"Model predictions stored in {output_table}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing symbol {symbol}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        return indicators_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema DDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Schema DDL - creates the tables in our database that we need for storing the raw scraped data & analyzing it.\n",
    "\n",
    "\"\"\"\n",
    "atb = AlpacaTradingBot()\n",
    "\n",
    "###### Raw Data ######\n",
    "# This table will house the raw scraped data\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS STG_SYMBOL_DATA''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS STG_SYMBOL_DATA (\n",
    "#     SYMBOL                  TEXT,\n",
    "#     TIMESTAMP               TIMESTAMP,\n",
    "#     CLOSE                   DECIMAL,\n",
    "#     HIGH                    DECIMAL,\n",
    "#     LOW                     DECIMAL,\n",
    "#     TRADE_COUNT             INTEGER,\n",
    "#     OPEN                    DECIMAL,\n",
    "#     VOLUME                  INTEGER,\n",
    "#     VWAP                    DECIMAL,\n",
    "#     PRIMARY KEY (SYMBOL, TIMESTAMP)\n",
    "# )\n",
    "# ''')\n",
    "\n",
    "##### Pre-Modeling Data ######\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS MODEL_TRAINING_DATA''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS MODEL_TRAINING_DATA (\n",
    "#     SYMBOL                      TEXT,\n",
    "#     TIMESTAMP_EST               TIMESTAMP,\n",
    "#     DATE                        DATE,\n",
    "#     TRADING_HOURS_IND           BOOLEAN,\n",
    "#     TRADING_HOURS_IND_EXT       BOOLEAN,\n",
    "#     CLOSE                       DECIMAL,\n",
    "#     HIGH                        DECIMAL,\n",
    "#     LOW                         DECIMAL,\n",
    "#     TRADE_COUNT                 INTEGER,\n",
    "#     OPEN                        DECIMAL,\n",
    "#     VOLUME                      INTEGER,\n",
    "#     VWAP                        DECIMAL,\n",
    "#     SMA_15                      DECIMAL,\n",
    "#     SMA_30                      DECIMAL,\n",
    "#     SMA_120                     DECIMAL,\n",
    "#     EMA_15                      DECIMAL,\n",
    "#     EMA_30                      DECIMAL,\n",
    "#     EMA_120                     DECIMAL,\n",
    "#     SMA_15_120_PCT_DIFF         DECIMAL, \n",
    "#     EMA_15_120_PCT_DIFF         DECIMAL,\n",
    "#     MACD                        DECIMAL,\n",
    "#     MACD_SIGNAL                 DECIMAL,\n",
    "#     MACD_DIFF                   DECIMAL,\n",
    "#     ADX                         DECIMAL,\n",
    "#     RSI                         DECIMAL, \n",
    "#     STOCHASTIC_K                DECIMAL,\n",
    "#     STOCHASTIC_D                DECIMAL,\n",
    "#     CCI                         DECIMAL,\n",
    "#     WILLIAMS_R                  DECIMAL, \n",
    "#     BOLLINGER_HBAND             DECIMAL,\n",
    "#     BOLLINGER_LBAND             DECIMAL,\n",
    "#     BOLLINGER_MBAND             DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT     DECIMAL,\n",
    "#     BOLLINGER_POSITION          DECIMAL,\n",
    "#     ATR                         DECIMAL,\n",
    "#     OBV                         INTEGER, \n",
    "#     ICHIMOKU_TENKAN_SEN         DECIMAL, \n",
    "#     ICHIMOKU_SENKOU_SPAN_A      DECIMAL,\n",
    "#     ICHIMOKU_SENKOU_SPAN_B      DECIMAL,\n",
    "#     RVI                         DECIMAL,\n",
    "#     FORCE_INDEX                 DECIMAL,\n",
    "#     AO_IND                      DECIMAL,\n",
    "#     RSI_LAG1                    DECIMAL,\n",
    "#     MACD_DIFF_LAG1              DECIMAL,\n",
    "#     CCI_LAG1                    DECIMAL,\n",
    "#     ADX_LAG1                    DECIMAL, \n",
    "#     CLOSE_LAG1                  DECIMAL,\n",
    "#     VOLUME_LAG1                 DECIMAL,\n",
    "#     VWAP_LAG1                   DECIMAL,\n",
    "#     ATR_LAG1                    DECIMAL,\n",
    "#     RSI_LAG2                    DECIMAL,\n",
    "#     MACD_DIFF_LAG2              DECIMAL,\n",
    "#     CCI_LAG2                    DECIMAL,\n",
    "#     ADX_LAG2                    DECIMAL,\n",
    "#     CLOSE_LAG2                  DECIMAL,\n",
    "#     VOLUME_LAG2                 DECIMAL,\n",
    "#     VWAP_LAG2                   DECIMAL,\n",
    "#     ATR_LAG2                    DECIMAL,\n",
    "#     RSI_LAG3                    DECIMAL,\n",
    "#     MACD_DIFF_LAG3              DECIMAL,\n",
    "#     CCI_LAG3                    DECIMAL,\n",
    "#     ADX_LAG3                    DECIMAL,\n",
    "#     CLOSE_LAG3                  DECIMAL,\n",
    "#     VOLUME_LAG3                 DECIMAL,\n",
    "#     VWAP_LAG3                   DECIMAL,\n",
    "#     ATR_LAG3                    DECIMAL, \n",
    "#     RSI_LAG4                    DECIMAL,\n",
    "#     MACD_DIFF_LAG4              DECIMAL,\n",
    "#     CCI_LAG4                    DECIMAL,\n",
    "#     ADX_LAG4                    DECIMAL,\n",
    "#     CLOSE_LAG4                  DECIMAL,\n",
    "#     VOLUME_LAG4                 DECIMAL,\n",
    "#     VWAP_LAG4                   DECIMAL,\n",
    "#     ATR_LAG4                    DECIMAL, \n",
    "#     RSI_LAG5                    DECIMAL,\n",
    "#     MACD_DIFF_LAG5              DECIMAL,\n",
    "#     CCI_LAG5                    DECIMAL,\n",
    "#     ADX_LAG5                    DECIMAL,\n",
    "#     CLOSE_LAG5                  DECIMAL,\n",
    "#     VOLUME_LAG5                 DECIMAL,\n",
    "#     VWAP_LAG5                   DECIMAL,\n",
    "#     ATR_LAG5                    DECIMAL,\n",
    "#     RSI_LAG10                   DECIMAL,\n",
    "#     MACD_DIFF_LAG10             DECIMAL,\n",
    "#     CCI_LAG10                   DECIMAL,\n",
    "#     ADX_LAG10                   DECIMAL,\n",
    "#     CLOSE_LAG10                 DECIMAL,\n",
    "#     VOLUME_LAG10                DECIMAL,\n",
    "#     VWAP_LAG10                  DECIMAL,\n",
    "#     ATR_LAG10                   DECIMAL, \n",
    "#     RSI_LAG20                   DECIMAL,\n",
    "#     MACD_DIFF_LAG20             DECIMAL,\n",
    "#     CCI_LAG20                   DECIMAL,\n",
    "#     ADX_LAG20                   DECIMAL,\n",
    "#     CLOSE_LAG20                 DECIMAL,\n",
    "#     VOLUME_LAG20                DECIMAL,\n",
    "#     VWAP_LAG20                  DECIMAL,\n",
    "#     ATR_LAG20                   DECIMAL, \n",
    "#     RSI_LAG30                   DECIMAL,\n",
    "#     MACD_DIFF_LAG30             DECIMAL,\n",
    "#     CCI_LAG30                   DECIMAL,\n",
    "#     ADX_LAG30                   DECIMAL,\n",
    "#     CLOSE_LAG30                 DECIMAL,\n",
    "#     VOLUME_LAG30                DECIMAL,\n",
    "#     VWAP_LAG30                  DECIMAL,\n",
    "#     ATR_LAG30                   DECIMAL,\n",
    "#     RSI_SQUARED                 DECIMAL,\n",
    "#     CCI_SQUARED                 DECIMAL,\n",
    "#     MACD_SQUARED                DECIMAL,\n",
    "#     ADX_SQUARED                 DECIMAL,\n",
    "#     MACD_DIFF_SQUARED           DECIMAL,\n",
    "#     STOCHASTIC_D_SQUARED        DECIMAL,\n",
    "#     STOCHASTIC_K_SQUARED        DECIMAL,\n",
    "#     WILLIAMS_R_SQUARED          DECIMAL,\n",
    "#     OBV_SQUARED                 DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT_SQUARED         DECIMAL,\n",
    "#     SMA_30_RSI                  DECIMAL,\n",
    "#     MACD_STOCHASTIC_K           DECIMAL,\n",
    "#     EMA_30_ATR                  DECIMAL,\n",
    "#     RSI_MACD_DIFF_INTERACTION   DECIMAL,\n",
    "#     SMA_30_ADX_INTERACTION      DECIMAL,\n",
    "#     BOLLINGER_POSITION_ATR_INTERACTION      DECIMAL,\n",
    "#     HIGH_LOW_RANGE              DECIMAL,\n",
    "#     CLOSE_PERCENT_RANGE         DECIMAL,                                        \n",
    "#     DAY_OF_WEEK                 INTEGER,\n",
    "#     HOUR_OF_DAY                 INTEGER,\n",
    "#     TIME_OF_DAY                 TIME,\n",
    "#     DAILY_MOVEMENT_SINCE_OPEN   DECIMAL,\n",
    "#     WEEKLY_MOVEMENT_SINCE_OPEN  DECIMAL,\n",
    "#     PCT_MOVEMENT_20_PERIODS     DECIMAL,\n",
    "#     PCT_MOVEMENT_60_PERIODS     DECIMAL,\n",
    "    PRIMARY KEY (SYMBOL, TIMESTAMP_EST)\n",
    ")\n",
    "''')\n",
    "\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS STG_SEQUENCE_MODELING''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS STG_SEQUENCE_MODELING (\n",
    "#     SYMBOL                      TEXT,\n",
    "#     TIMESTAMP_EST               TIMESTAMP,\n",
    "#     DATE                        DATE,\n",
    "#     TRADING_HOURS_IND           BOOLEAN,\n",
    "#     TRADING_HOURS_IND_EXT       BOOLEAN,\n",
    "#     CLOSE                       DECIMAL,\n",
    "#     HIGH                        DECIMAL,\n",
    "#     LOW                         DECIMAL,\n",
    "#     TRADE_COUNT                 INTEGER,\n",
    "#     OPEN                        DECIMAL,\n",
    "#     VOLUME                      INTEGER,\n",
    "#     VWAP                        DECIMAL,\n",
    "#     RSI                         DECIMAL,\n",
    "#     ATR                         DECIMAL,\n",
    "#     CCI                         DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT     DECIMAL,\n",
    "#     BOLLINGER_POSITION          DECIMAL, \n",
    "#     SMA_30                      DECIMAL,\n",
    "#     SMA_120                     DECIMAL,\n",
    "#     EMA_30                      DECIMAL,\n",
    "#     EMA_120                     DECIMAL,\n",
    "#     SMA_30_120_PCT_DIFF         DECIMAL,\n",
    "#     EMA_30_120_PCT_DIFF         DECIMAL,\n",
    "#     PRIMARY KEY (SYMBOL, TIMESTAMP_EST)\n",
    "# )\n",
    "# ''')\n",
    "\n",
    "###### Model Results ######\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS CLASSIFICATION_MODEL_RESULTS''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS CLASSIFICATION_MODEL_RESULTS  (\n",
    "#     ID                      INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "#     SYMBOL                  TEXT NOT NULL,\n",
    "#     MODEL_NAME              TEXT NOT NULL,\n",
    "#     F1_SCORE                REAL NOT NULL,\n",
    "#     CONFUSION_MATRIX        TEXT NOT NULL,\n",
    "#     CLASSIFICATION_REPORT   TEXT NOT NULL,\n",
    "#     BEST_PARAMS             TEXT NOT NULL,                 \n",
    "#     EVALUATION_TIMESTAMP    DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "# )\n",
    "# ''')\n",
    "\n",
    "###### Model Results Applied to Historical ######\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS CLASSIFICATION_MODEL_PREDICTIONS''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS CLASSIFICATION_MODEL_PREDICTIONS (\n",
    "#     SYMBOL                      TEXT,\n",
    "#     TIMESTAMP_EST               TIMESTAMP,\n",
    "#     DATE                        DATE,\n",
    "#     TRADING_HOURS_IND           BOOLEAN,\n",
    "#     TRADING_HOURS_IND_EXT       BOOLEAN,\n",
    "#     CLOSE                       DECIMAL,\n",
    "#     HIGH                        DECIMAL,\n",
    "#     LOW                         DECIMAL,\n",
    "#     TRADE_COUNT                 INTEGER,\n",
    "#     OPEN                        DECIMAL,\n",
    "#     VOLUME                      INTEGER,\n",
    "#     VWAP                        DECIMAL,\n",
    "#     SMA_15                      DECIMAL,\n",
    "#     SMA_30                      DECIMAL,\n",
    "#     SMA_120                     DECIMAL,\n",
    "#     EMA_15                      DECIMAL,\n",
    "#     EMA_30                      DECIMAL,\n",
    "#     EMA_120                     DECIMAL,\n",
    "#     SMA_15_120_PCT_DIFF         DECIMAL,                    \n",
    "#     EMA_15_120_PCT_DIFF         DECIMAL,\n",
    "#     MACD                        DECIMAL,\n",
    "#     MACD_SIGNAL                 DECIMAL,\n",
    "#     MACD_DIFF                   DECIMAL,\n",
    "#     ADX                         DECIMAL,\n",
    "#     RSI                         DECIMAL,                      \n",
    "#     STOCHASTIC_K                DECIMAL,\n",
    "#     STOCHASTIC_D                DECIMAL,\n",
    "#     CCI                         DECIMAL,\n",
    "#     WILLIAMS_R                  DECIMAL, \n",
    "#     BOLLINGER_HBAND             DECIMAL,\n",
    "#     BOLLINGER_LBAND             DECIMAL,\n",
    "#     BOLLINGER_MBAND             DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT     DECIMAL,\n",
    "#     BOLLINGER_POSITION          DECIMAL,\n",
    "#     ATR                         DECIMAL,\n",
    "#     OBV                         INTEGER, \n",
    "#     ICHIMOKU_TENKAN_SEN         DECIMAL, \n",
    "#     ICHIMOKU_SENKOU_SPAN_A      DECIMAL,\n",
    "#     ICHIMOKU_SENKOU_SPAN_B      DECIMAL,\n",
    "#     RVI                         DECIMAL,\n",
    "#     FORCE_INDEX                 DECIMAL,\n",
    "#     AO_IND                      DECIMAL,\n",
    "#     RSI_LAG1                    DECIMAL,\n",
    "#     MACD_DIFF_LAG1              DECIMAL,\n",
    "#     CCI_LAG1                    DECIMAL,\n",
    "#     ADX_LAG1                    DECIMAL, \n",
    "#     CLOSE_LAG1                  DECIMAL,\n",
    "#     VOLUME_LAG1                 DECIMAL,\n",
    "#     VWAP_LAG1                   DECIMAL,\n",
    "#     ATR_LAG1                    DECIMAL,\n",
    "#     RSI_LAG2                    DECIMAL,\n",
    "#     MACD_DIFF_LAG2              DECIMAL,\n",
    "#     CCI_LAG2                    DECIMAL,\n",
    "#     ADX_LAG2                    DECIMAL,\n",
    "#     CLOSE_LAG2                  DECIMAL,\n",
    "#     VOLUME_LAG2                 DECIMAL,\n",
    "#     VWAP_LAG2                   DECIMAL,\n",
    "#     ATR_LAG2                    DECIMAL,\n",
    "#     RSI_LAG3                    DECIMAL,\n",
    "#     MACD_DIFF_LAG3              DECIMAL,\n",
    "#     CCI_LAG3                    DECIMAL,\n",
    "#     ADX_LAG3                    DECIMAL,\n",
    "#     CLOSE_LAG3                  DECIMAL,\n",
    "#     VOLUME_LAG3                 DECIMAL,\n",
    "#     VWAP_LAG3                   DECIMAL,\n",
    "#     ATR_LAG3                    DECIMAL,                      \n",
    "#     RSI_LAG4                    DECIMAL,\n",
    "#     MACD_DIFF_LAG4              DECIMAL,\n",
    "#     CCI_LAG4                    DECIMAL,\n",
    "#     ADX_LAG4                    DECIMAL,\n",
    "#     CLOSE_LAG4                  DECIMAL,\n",
    "#     VOLUME_LAG4                 DECIMAL,\n",
    "#     VWAP_LAG4                   DECIMAL,\n",
    "#     ATR_LAG4                    DECIMAL,                           \n",
    "#     RSI_LAG5                    DECIMAL,\n",
    "#     MACD_DIFF_LAG5              DECIMAL,\n",
    "#     CCI_LAG5                    DECIMAL,\n",
    "#     ADX_LAG5                    DECIMAL,\n",
    "#     CLOSE_LAG5                  DECIMAL,\n",
    "#     VOLUME_LAG5                 DECIMAL,\n",
    "#     VWAP_LAG5                   DECIMAL,\n",
    "#     ATR_LAG5                    DECIMAL,\n",
    "#     RSI_LAG10                   DECIMAL,\n",
    "#     MACD_DIFF_LAG10             DECIMAL,\n",
    "#     CCI_LAG10                   DECIMAL,\n",
    "#     ADX_LAG10                   DECIMAL,\n",
    "#     CLOSE_LAG10                 DECIMAL,\n",
    "#     VOLUME_LAG10                DECIMAL,\n",
    "#     VWAP_LAG10                  DECIMAL,\n",
    "#     ATR_LAG10                   DECIMAL,    \n",
    "#     RSI_LAG20                   DECIMAL,\n",
    "#     MACD_DIFF_LAG20             DECIMAL,\n",
    "#     CCI_LAG20                   DECIMAL,\n",
    "#     ADX_LAG20                   DECIMAL,\n",
    "#     CLOSE_LAG20                 DECIMAL,\n",
    "#     VOLUME_LAG20                DECIMAL,\n",
    "#     VWAP_LAG20                  DECIMAL,\n",
    "#     ATR_LAG20                   DECIMAL,                     \n",
    "#     RSI_LAG30                   DECIMAL,\n",
    "#     MACD_DIFF_LAG30             DECIMAL,\n",
    "#     CCI_LAG30                   DECIMAL,\n",
    "#     ADX_LAG30                   DECIMAL,\n",
    "#     CLOSE_LAG30                 DECIMAL,\n",
    "#     VOLUME_LAG30                DECIMAL,\n",
    "#     VWAP_LAG30                  DECIMAL,\n",
    "#     ATR_LAG30                   DECIMAL,\n",
    "#     RSI_SQUARED                 DECIMAL,\n",
    "#     CCI_SQUARED                 DECIMAL,\n",
    "#     MACD_SQUARED                DECIMAL,\n",
    "#     ADX_SQUARED                 DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT_SQUARED         DECIMAL,\n",
    "#     SMA_30_RSI                  DECIMAL,\n",
    "#     MACD_STOCHASTIC_K           DECIMAL,\n",
    "#     EMA_30_ATR                  DECIMAL,\n",
    "#     RSI_MACD_DIFF_INTERACTION   DECIMAL,\n",
    "#     SMA_30_ADX_INTERACTION      DECIMAL,\n",
    "#     BOLLINGER_POSITION_ATR_INTERACTION      DECIMAL,\n",
    "#     DAY_OF_WEEK                 INTEGER,\n",
    "#     HOUR_OF_DAY                 INTEGER,\n",
    "#     TIME_OF_DAY                 TIME,                      \n",
    "#     DAILY_OPENING_PRICE         DECIMAL,\n",
    "#     DAILY_MOVEMENT_SINCE_OPEN   DECIMAL,\n",
    "#     WEEKLY_OPENING_PRICE        DECIMAL,\n",
    "#     WEEKLY_MOVEMENT_SINCE_OPEN  DECIMAL,\n",
    "#     PCT_MOVEMENT_20_PERIODS     DECIMAL,\n",
    "#     PCT_MOVEMENT_60_PERIODS     DECIMAL,\n",
    "#     TARGET_30_MIN               DECIMAL,\n",
    "#     TARGET_MOVEMENT_PCT         DECIMAL,\n",
    "#     TARGET_MOVEMENT_SIGNAL      INTEGER,\n",
    "#     RANDOM_FOREST_PREDICTION        INTEGER,                \n",
    "#     XGBOOST_PREDICTION              INTEGER,     \n",
    "#     GRADIENT_BOOSTING_PREDICTION    INTEGER,     \n",
    "#     NEURAL_NETWORK_PREDICTION       INTEGER,     \n",
    "#     PRIMARY KEY (SYMBOL, TIMESTAMP_EST)\n",
    "# )\n",
    "# ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape & Store Data In Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped for 2024-06-05 through 11:59:59Z...\n",
      "Data scraped for 2024-06-05 through 23:59:59Z...\n",
      "Total API calls made so far: 2\n",
      "Pausing for 16.4940822757135 after scraping data for 2024-06-05...\n",
      "__________________________________\n",
      "Scraping Bar Data complete for timerange: 2024-06-05 08:00:00+00:00  -  2024-06-05 23:59:00+00:00\n",
      "Total count of data records queried: 785\n",
      "Count of fresh data records actually inserted into STG_SYMBOL_DATA: 0\n"
     ]
    }
   ],
   "source": [
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# Define the list of stocks\n",
    "stocks = [\"SPY\"]\n",
    "\n",
    "# Define the time range for historical data\n",
    "start_date = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "end_date = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Download and store historical data in the staging table\n",
    "data = {}\n",
    "for stock in stocks:\n",
    "    # Download historical data\n",
    "    stock_data = atb.download_bar_data(stock, TimeFrame(1, TimeFrameUnit.Minute), start_date, end_date)\n",
    "    data[stock] = stock_data\n",
    "\n",
    "# Store all downloaded data in the staging table\n",
    "for stock, stock_data in data.items():\n",
    "    atb.db_append_no_duplicates('STG_SYMBOL_DATA', stock_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing SPY data for classification modeling.\n",
      "Data fetched for SPY from table STG_SYMBOL_DATA.\n",
      "Timestamps converted to EST.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bryan\\AppData\\Local\\Temp\\ipykernel_25628\\3824202878.py:412: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  input_data['WEEKLY_MOVEMENT_SINCE_OPEN'] = input_data['CLOSE'] - input_data.groupby(input_data['timestamp_est'].dt.to_period('W'))['CLOSE'].transform('first')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical indicators created.\n",
      "Dropped columns: {'timestamp_utc', 'VWAP', 'TIMESTAMP'}\n",
      "Dataframe columns reordered to match MODEL_TRAINING_DATA.\n",
      "Truncated table MODEL_TRAINING_DATA.\n",
      "Prepared model data stored in MODEL_TRAINING_DATA. Number of rows inserted: 916895.\n",
      "__________________________________\n"
     ]
    }
   ],
   "source": [
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# Define the list of stocks\n",
    "stocks = [\"SPY\"]\n",
    "\n",
    "# Prepare data for traditional classification modeling\n",
    "for stock in stocks:\n",
    "    atb.modeling_prepare('STG_SYMBOL_DATA', 'MODEL_TRAINING_DATA', stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Execute Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing symbol: SPY\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and use the trading bot\n",
    "atb = AlpacaTradingBot()\n",
    "\n",
    "\"\"\"\n",
    "We will run our predictive models using the following parameters (note that classification models don't care about the sequence_window):\n",
    "\n",
    "Parameters:\n",
    "- models: A list of model names to be used for prediction.\n",
    "- table_name: The name of the table used for classification modeling.\n",
    "- features: A list of feature names to be used for prediction.\n",
    "- symbols: A list of symbols for which the models will be run.\n",
    "- target_threshold: The threshold value for the target variable.\n",
    "- sequence_window: The window size for the sequential models.\n",
    "- n_iter: The number of iterations for model training.\n",
    "- cv: The number of cross-validation folds.\n",
    "- start_date: Optional parameter to train on smaller dataset.. really just have this here to test out code before running on a big timeframe\n",
    "\"\"\"\n",
    "\n",
    "table_name = 'MODEL_TRAINING_DATA'\n",
    "target_threshold = .0015\n",
    "sequence_window = 10\n",
    "n_iter=20\n",
    "cv=3\n",
    "start_date = '2024-01-01'\n",
    "\n",
    "symbols = [\n",
    "    'SPY'\n",
    "    ]\n",
    "models = [\n",
    "    'LSTM', 'RNN', 'CNN',\n",
    "    'K-Nearest Neighbors', 'XGBoost', 'Logistic Regression', \n",
    "    'Random Forest', 'Neural Network', 'Gradient Boosting', 'Support Vector Machine'\n",
    "    ]\n",
    "features = [\n",
    "    'CLOSE', 'HIGH', 'LOW', 'OPEN', 'VOLUME', 'VWAP', 'SMA_15', 'SMA_120', \n",
    "    'EMA_15', 'EMA_120', 'SMA_15_120_PCT_DIFF', 'EMA_15_120_PCT_DIFF', \n",
    "    'MACD_DIFF', 'ADX', 'RSI', 'STOCHASTIC_K', 'STOCHASTIC_D', 'CCI', \n",
    "    'WILLIAMS_R','BOLLINGER_WIDTH_PERCENT', 'BOLLINGER_POSITION', 'ATR', \n",
    "    'ICHIMOKU_TENKAN_SEN', 'RVI', 'FORCE_INDEX', 'AO_IND', 'HIGH_LOW_RANGE', \n",
    "    'CLOSE_PERCENT_RANGE', 'DAY_OF_WEEK', 'HOUR_OF_DAY', 'DAILY_MOVEMENT_SINCE_OPEN',\n",
    "    'WEEKLY_MOVEMENT_SINCE_OPEN', 'PCT_MOVEMENT_20_PERIODS', 'PCT_MOVEMENT_60_PERIODS' \n",
    "    ]\n",
    "\n",
    "    # Unused features\n",
    "    # 'TRADE_COUNT', 'SMA_30', 'EMA_30', 'MACD', 'MACD_SIGNAL', 'BOLLINGER_HBAND', 'OBV', 'ICHIMOKU_SENKOU_SPAN_A', 'ICHIMOKU_SENKOU_SPAN_B', \n",
    "    # 'BOLLINGER_LBAND', 'BOLLINGER_MBAND', 'RSI_LAG1', 'MACD_DIFF_LAG1', 'CCI_LAG1', 'ADX_LAG1', \n",
    "    # 'CLOSE_LAG1', 'VOLUME_LAG1', 'VWAP_LAG1', 'ATR_LAG1', 'RSI_LAG2', \n",
    "    # 'MACD_DIFF_LAG2', 'CCI_LAG2', 'ADX_LAG2', 'CLOSE_LAG2', 'VOLUME_LAG2', \n",
    "    # 'VWAP_LAG2', 'ATR_LAG2', 'RSI_LAG3', 'MACD_DIFF_LAG3', 'CCI_LAG3', \n",
    "    # 'ADX_LAG3', 'CLOSE_LAG3', 'VOLUME_LAG3', 'VWAP_LAG3', 'ATR_LAG3', \n",
    "    # 'RSI_LAG4', 'MACD_DIFF_LAG4', 'CCI_LAG4', 'ADX_LAG4', 'CLOSE_LAG4', \n",
    "    # 'VOLUME_LAG4', 'VWAP_LAG4', 'ATR_LAG4', 'RSI_LAG5', 'MACD_DIFF_LAG5', \n",
    "    # 'CCI_LAG5', 'ADX_LAG5', 'CLOSE_LAG5', 'VOLUME_LAG5', 'VWAP_LAG5', \n",
    "    # 'ATR_LAG5', 'RSI_LAG10', 'MACD_DIFF_LAG10', 'CCI_LAG10', 'ADX_LAG10', \n",
    "    # 'CLOSE_LAG10', 'VOLUME_LAG10', 'VWAP_LAG10', 'ATR_LAG10', 'RSI_LAG20', \n",
    "    # 'MACD_DIFF_LAG20', 'CCI_LAG20', 'ADX_LAG20', 'CLOSE_LAG20', 'VOLUME_LAG20', \n",
    "    # 'VWAP_LAG20', 'ATR_LAG20', 'RSI_LAG30', 'MACD_DIFF_LAG30', 'CCI_LAG30', \n",
    "    # 'ADX_LAG30', 'CLOSE_LAG30', 'VOLUME_LAG30', 'VWAP_LAG30', 'ATR_LAG30', \n",
    "    # 'RSI_SQUARED', 'CCI_SQUARED', 'MACD_SQUARED', 'ADX_SQUARED', \n",
    "    # 'MACD_DIFF_SQUARED', 'STOCHASTIC_D_SQUARED', 'STOCHASTIC_K_SQUARED', \n",
    "    # 'WILLIAMS_R_SQUARED', 'OBV_SQUARED', 'BOLLINGER_WIDTH_PERCENT_SQUARED', \n",
    "    # 'SMA_30_RSI', 'MACD_STOCHASTIC_K', 'EMA_30_ATR', 'RSI_MACD_DIFF_INTERACTION', \n",
    "    # 'SMA_30_ADX_INTERACTION', 'BOLLINGER_POSITION_ATR_INTERACTION', 'TIME_OF_DAY', \n",
    "\n",
    "# Run sequential models\n",
    "for symbol in symbols:\n",
    "    print(f\"Processing symbol: {symbol}\")\n",
    "    atb.modeling_build(models, table_name, features, symbol, target_threshold, sequence_window, n_iter, cv, start_date)\n",
    "    print(f\"Completed modeling for symbol: {symbol}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Query current day's data, insert to stage, add indicators & add model predictions based on already created models.\n",
    "\n",
    "\"\"\"\n",
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# Define the list of stocks\n",
    "stocks = [\"SPY\"]\n",
    "\n",
    "# Just query current day's data\n",
    "# start_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Step 1: Download ticks and add to stage table\n",
    "# data = {}\n",
    "# for stock in stocks:\n",
    "    # Download historical data\n",
    "    # stock_data = atb.download_bar_data(stock, TimeFrame(1, TimeFrameUnit.Minute), start_date, end_date)\n",
    "    # data[stock] = stock_data\n",
    "\n",
    "# Store all downloaded data in the staging table\n",
    "# for stock, stock_data in data.items():\n",
    "#     atb.db_append_no_duplicates('STG_SYMBOL_DATA', stock_data)\n",
    "#     print(f\"Stage table updated with data for {stock}\")\n",
    "\n",
    "# Step 2: Transfer data from the staging table to the modeling staging table\n",
    "for stock in stocks:\n",
    "    atb.classification_modeling_apply('STG_SYMBOL_DATA', stock, \"CLASSIFICATION_MODEL_PREDICTIONS\")\n",
    "    print(f\"Model predictions for {stock} stored in CLASSIFICATION_MODEL_PREDICTIONS.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
