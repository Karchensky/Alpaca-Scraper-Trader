{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpaca Trading Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data & API packages\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import TimeFrame, TimeFrameUnit\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import ta\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "#modeling packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, SimpleRNN, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "class AlpacaTradingBot:\n",
    "    \"\"\"\n",
    "\n",
    "    This class can be used to scrape stock data from the Alpaca API, store it locally in a SQLite Database,\n",
    "    add indicators & perform predective analytics.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, keys_file_path='alpaca_keys.txt', base_url='https://paper-api.alpaca.markets', database_path=r'D:\\Scripts\\alpaca\\alpaca_algo_trading\\alpaca_data.db'):\n",
    "        with open(keys_file_path, 'r') as file:\n",
    "            self.api_key = file.readline().strip()\n",
    "            self.api_secret = file.readline().strip()\n",
    "        self.base_url = base_url\n",
    "        self.api = tradeapi.REST(self.api_key, self.api_secret, base_url=base_url)\n",
    "        self.database_path = database_path\n",
    "        self.api_call_count = 0\n",
    "\n",
    "    ######################################################  API Scraping methods ######################################################\n",
    "    def download_bar_data(self, stock, timeframe, start_date, end_date, pause=True):\n",
    "        \"\"\"\n",
    "        \n",
    "        This method will be used to scrape bar data from the Alpaca API. \n",
    "        Provide a ticker symbol, an interval, and a beginning and end date.\n",
    "\n",
    "        \"\"\"\n",
    "        all_data = []\n",
    "\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            # Check if it's a weekend (Saturday or Sunday) & skip, if so\n",
    "            # if  datetime.strptime(current_date, \"%Y-%m-%d\").weekday() >= 5:\n",
    "            #     current_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "            #     continue\n",
    "\n",
    "            # Split the day into two segments: midnight to noon, and noon to end of day because the API can only return 1,000 data points a day\n",
    "            segments = [\n",
    "                (f\"{current_date}T00:00:00Z\", f\"{current_date}T11:59:59Z\"),\n",
    "                (f\"{current_date}T12:00:00Z\", f\"{current_date}T23:59:59Z\")\n",
    "            ]\n",
    "\n",
    "            for start_time, end_time in segments:\n",
    "                try:\n",
    "                    # Get data for the current day segment\n",
    "                    bars = self.api.get_bars(stock, timeframe, start=start_time, end=end_time, limit=1000).df\n",
    "                    self.api_call_count += 1  # Increment the API call counter\n",
    "                    if not bars.empty:\n",
    "                        bars['symbol'] = stock  # Add the stock symbol column\n",
    "                        all_data.append(bars)\n",
    "                        print(f\"Data scraped for {current_date} through {end_time.split('T')[1]}...\")\n",
    "                    else:\n",
    "                        print(f\"No data available for segment: {current_date}: {start_time.split('T')[1]} - {end_time.split('T')[1]}\")\n",
    "                except tradeapi.rest.APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    pause_duration = random.uniform(120, 240)  # Longer pause if an API error occurs\n",
    "                    time.sleep(pause_duration)\n",
    "                    continue\n",
    "                \n",
    "            # Random pause after each scrape.. API rate limits need to be considered. Only a problem for the main original pull to populate the database.\n",
    "            if pause and (datetime.strptime(current_date, \"%Y-%m-%d\") - datetime.strptime(start_date, \"%Y-%m-%d\")).days % 2 == 0:\n",
    "                print(f\"Total API calls made so far: {self.api_call_count}\")\n",
    "                pause_duration = random.uniform(10, 30)\n",
    "                print(f\"Pausing for {pause_duration} after scraping data for {current_date}...\")\n",
    "                print(f\"__________________________________\")\n",
    "                time.sleep(pause_duration)\n",
    "\n",
    "            # Move to the next day\n",
    "            current_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        if all_data:\n",
    "            combined_data = pd.concat(all_data)\n",
    "            combined_data = combined_data.reset_index()  # Ensure the index is reset to have 'timestamp' as a column\n",
    "            print(\"Scraping Bar Data complete for timerange:\", combined_data['timestamp'].min(), \" - \", combined_data['timestamp'].max())\n",
    "            return combined_data[['symbol'] + [col for col in combined_data.columns if col not in ['symbol']]]\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    ######################################################  Modeling methods ######################################################     \n",
    "    def sequence_modeling_prep(self, input_table, symbol, output_table=None):\n",
    "        \"\"\"\n",
    "\n",
    "        This method cleans the scraped raw stage data, adds indicator calculations on it inserts the otuput into a fresh table for modeling.\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Read the entire table\n",
    "            input_data = pd.read_sql(f\"SELECT distinct * FROM {input_table} WHERE SYMBOL = '{symbol}'\" , conn)\n",
    "\n",
    "            # Check if the input data is empty\n",
    "            if input_data.empty:\n",
    "                raise ValueError(f\"No data found for symbol {symbol} in table {input_table}\")\n",
    "            \n",
    "            # Convert 'TIMESTAMP' to datetime and localize it to UTC\n",
    "            input_data['timestamp_utc'] = pd.to_datetime(input_data['TIMESTAMP'], utc=True)\n",
    "            input_data['timestamp_est'] = input_data['timestamp_utc'].dt.tz_convert('US/Eastern')\n",
    "            input_data['trading_hours_ind'] = (input_data['timestamp_est'].dt.time >= datetime.strptime('09:30', '%H:%M').time()) & \\\n",
    "                                            (input_data['timestamp_est'].dt.time <= datetime.strptime('16:00', '%H:%M').time())\n",
    "            input_data['trading_hours_ind_ext'] = (input_data['timestamp_est'].dt.time >= datetime.strptime('10:30', '%H:%M').time()) & \\\n",
    "                                                    (input_data['timestamp_est'].dt.time <= datetime.strptime('15:00', '%H:%M').time())\n",
    "            input_data['date'] = input_data['timestamp_est'].dt.date\n",
    "\n",
    "            # Some technical indicators to test\n",
    "            input_data['rsi'] = ta.momentum.rsi(input_data['CLOSE'], window=30)\n",
    "            input_data['atr'] = ta.volatility.average_true_range(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=30)\n",
    "            input_data['cci'] = ta.trend.cci(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=20)\n",
    "            bb_indicator = ta.volatility.BollingerBands(input_data['CLOSE'], window=30, window_dev=2)\n",
    "            input_data['bollinger_width_percent'] = (bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()) / input_data['CLOSE']\n",
    "            input_data['bollinger_position'] = bb_indicator.bollinger_pband()\n",
    "\n",
    "            # Targets\n",
    "            input_data['target_30_min'] = input_data['CLOSE'].shift(-30)\n",
    "            input_data['target_movement_pct'] = ((input_data['target_30_min'] - input_data['CLOSE']).abs() / input_data['CLOSE']) * 100\n",
    "            input_data['target_movement_signal'] = input_data.apply(\n",
    "                lambda row: 1 if (row['target_30_min'] - row['CLOSE']) > 0.001 * row['CLOSE'] else \n",
    "                            (-1 if (row['target_30_min'] - row['CLOSE']) <= -0.001 * row['CLOSE'] else 0), \n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Drop some columns and reorder\n",
    "            input_data = input_data.drop(columns=['TIMESTAMP', 'timestamp_utc'])\n",
    "            input_data = input_data[['SYMBOL', 'timestamp_est', 'date', 'trading_hours_ind', 'trading_hours_ind_ext'] + [col for col in input_data.columns if col not in ['SYMBOL', 'timestamp_est', 'date', 'trading_hours_ind', 'trading_hours_ind_ext']]]\n",
    "\n",
    "            # Store the final DataFrame into the output table\n",
    "            if output_table is not None:\n",
    "                # Truncate the existing table\n",
    "                self.db_write(f\"DELETE FROM {output_table}\")\n",
    "                print(f\"Truncated table {output_table}.\")\n",
    "\n",
    "                # Store the DataFrame in the output table\n",
    "                self.db_append(output_table, input_data)\n",
    "                print(f\"Prepared sequence model data stored in {output_table}. Number of rows inserted: {len(input_data)}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing symbol {symbol}: {e}\")\n",
    "    \n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        return input_data\n",
    "    \n",
    "    def sequence_modeling_build(self, features, target, symbol, window):\n",
    "        \"\"\"\n",
    "\n",
    "        This method runs our LSTM/RNN/CNN models on our prepped-data\n",
    "\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.database_path)\n",
    "        df = pd.read_sql(f\"SELECT * FROM SYMBOL_DATA_MODELING WHERE SYMBOL = '{symbol}' AND TRADING_HOURS_IND_EXT = 1\", conn)\n",
    "        conn.close()\n",
    "\n",
    "        # Split data by unique dates\n",
    "        unique_dates = df['DATE'].unique()\n",
    "        train_dates, val_dates = train_test_split(unique_dates, test_size=0.3, random_state=42)\n",
    "\n",
    "        train_data = df[df['DATE'].isin(train_dates)]\n",
    "        val_data = df[df['DATE'].isin(val_dates)]\n",
    "\n",
    "        # Ensure data is sorted by time\n",
    "        train_data.sort_values('TIMESTAMP_EST', inplace=True)\n",
    "        val_data.sort_values('TIMESTAMP_EST', inplace=True)\n",
    "\n",
    "        # Create the sequences for each of our features\n",
    "        X_train, y_train = self.create_sequences(train_data, features, target, int(window))\n",
    "        X_val, y_val = self.create_sequences(val_data, features, target, int(window))\n",
    "\n",
    "        # Define models with parameter grids for hyperparameter tuning\n",
    "        models = {\n",
    "            \"LSTM\": (self.build_lstm_model, {\n",
    "                'units': [50, 100],\n",
    "                'activation': ['relu', 'tanh'],\n",
    "                'optimizer': ['adam', 'rmsprop'],\n",
    "                'epochs': [10, 20],\n",
    "                'batch_size': [32, 64]\n",
    "            }),\n",
    "            \"RNN\": (self.build_rnn_model, {\n",
    "                'units': [50, 100],\n",
    "                'activation': ['relu', 'tanh'],\n",
    "                'optimizer': ['adam', 'rmsprop'],\n",
    "                'epochs': [10, 20],\n",
    "                'batch_size': [32, 64]\n",
    "            }),\n",
    "            \"CNN\": (self.build_cnn_model, {\n",
    "                'filters': [32, 64],\n",
    "                'kernel_size': [3, 5],\n",
    "                'activation': ['relu', 'tanh'],\n",
    "                'optimizer': ['adam', 'rmsprop'],\n",
    "                'epochs': [10, 20],\n",
    "                'batch_size': [32, 64]\n",
    "            })\n",
    "        }\n",
    "\n",
    "        for model_name, (model_builder, params) in models.items():\n",
    "            print(f\"Training {model_name} model for symbol: {symbol}\")\n",
    "\n",
    "            keras_model = KerasClassifier(build_fn=model_builder, input_shape=X_train.shape[1:], output_units=1)\n",
    "            grid_search = RandomizedSearchCV(estimator=keras_model, param_distributions=params, n_iter=10, cv=2, n_jobs=-1, scoring='f1_weighted', random_state=42, verbose=3)\n",
    "\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "\n",
    "            self.evaluate_model(best_model, model_name, X_val, y_val, symbol)\n",
    "            print(f\"Completed training for {model_name} model\\n\")\n",
    "\n",
    "    # Support Function to create sequences\n",
    "    def create_sequences(self, df, feature_columns, target_column, sequence_length):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        for i in range(len(df) - sequence_length):\n",
    "            seq_features = df[feature_columns].iloc[i:i+sequence_length].values\n",
    "            seq_target = df[target_column].iloc[i + sequence_length]\n",
    "            sequences.append(seq_features)\n",
    "            targets.append(seq_target)\n",
    "        return np.array(sequences), np.array(targets)\n",
    "\n",
    "    # Support Function for LTSM\n",
    "    def build_lstm_model(self, input_shape, output_units, units=50, activation='relu', optimizer='adam'):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units, activation=activation, input_shape=input_shape))\n",
    "        model.add(Dense(output_units, activation='linear'))\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "    # Support Function for RNN\n",
    "    def build_rnn_model(self, input_shape, output_units, units=50, activation='relu', optimizer='adam'):\n",
    "        model = Sequential()\n",
    "        model.add(SimpleRNN(units, activation=activation, input_shape=input_shape))\n",
    "        model.add(Dense(output_units, activation='linear'))\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "    # Support Function for CNN\n",
    "    def build_cnn_model(self, input_shape, output_units, filters=64, kernel_size=3, activation='relu', optimizer='adam'):\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=input_shape))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation=activation))\n",
    "        model.add(Dense(output_units, activation='linear'))\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "        return model\n",
    "    \n",
    "    # Model evaluation\n",
    "    def evaluate_model(self, model, model_name, X_val, y_val, symbol, features):\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
    "        f1 = f1_score(y_val, y_pred_classes, average='weighted', zero_division=0)\n",
    "        cm = confusion_matrix(y_val, y_pred_classes)\n",
    "        cr = classification_report(y_val, y_pred_classes, zero_division=0)\n",
    "\n",
    "        print(f\"{model_name} - F1 Score: {f1}\")\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "        print(\"Classification Report:\\n\", cr)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.title(f'Confusion Matrix: {model_name}')\n",
    "        plt.show()\n",
    "        print(f\"__________________________________\")\n",
    "\n",
    "        # Save the model with a unique filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model.model.save(f'{symbol}_{model_name}_{timestamp}.h5')\n",
    "\n",
    "        # Log results to the database\n",
    "        evaluation_results = {\n",
    "            'SYMBOL': symbol,\n",
    "            'MODEL_NAME': model_name,\n",
    "            'F1_SCORE': f1,\n",
    "            'CONFUSION_MATRIX': str(cm),\n",
    "            'CLASSIFICATION_REPORT': cr,\n",
    "            'BEST_PARAMS': str(model.get_params())\n",
    "        }\n",
    "        results_df = pd.DataFrame([evaluation_results])\n",
    "        self.db_append('CLASSIFICATION_MODEL_RESULTS', results_df)\n",
    "\n",
    "        # Feature Importance or Coefficients\n",
    "        if hasattr(model, \"feature_importances_\"):\n",
    "            importances = model.feature_importances_\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'feature': features,\n",
    "                'importance': importances\n",
    "            }).sort_values(by='importance', ascending=False)\n",
    "            print(f\"Feature importances for {model_name}:\")\n",
    "            print(feature_importance_df)\n",
    "        elif hasattr(model, \"coef_\"):\n",
    "            coefs = model.coef_[0]\n",
    "            coef_df = pd.DataFrame({\n",
    "                'feature': features,\n",
    "                'coefficient': coefs\n",
    "            }).sort_values(by='coefficient', ascending=False)\n",
    "            print(f\"Coefficients for {model_name}:\")\n",
    "            print(coef_df)\n",
    "        else:\n",
    "            print(f\"No feature importance or coefficients for {model_name}\")\n",
    "        print(f\"____________________________________________\")\n",
    "        print(f\"____________________________________________\")\n",
    "\n",
    "    ######################################################  (Old) Modeling methods.. these didn't produce great results ######################################################  \n",
    "    def classification_modeling_prep(self, input_table, symbol, output_table=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        This method cleans the scraped raw stage data, adds indicator calculations on it inserts the otuput into a fresh table.\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Read the entire table\n",
    "            input_data = pd.read_sql(f\"SELECT distinct * FROM {input_table} WHERE SYMBOL = '{symbol}'\" , conn)\n",
    "\n",
    "            # Check if the input data is empty\n",
    "            if input_data.empty:\n",
    "                raise ValueError(f\"No data found for symbol {symbol} in table {input_table}\")\n",
    "\n",
    "            # Convert 'TIMESTAMP' to datetime and localize it to UTC\n",
    "            input_data['timestamp_utc'] = pd.to_datetime(input_data['TIMESTAMP'], utc=True)\n",
    "            input_data['timestamp_est'] = input_data['timestamp_utc'].dt.tz_convert('US/Eastern')\n",
    "            input_data['trading_hours_ind'] = (input_data['timestamp_est'].dt.time >= datetime.strptime('09:30', '%H:%M').time()) & \\\n",
    "                                            (input_data['timestamp_est'].dt.time <= datetime.strptime('16:00', '%H:%M').time())\n",
    "            input_data['trading_hours_ind_ext'] = (input_data['timestamp_est'].dt.time >= datetime.strptime('10:30', '%H:%M').time()) & \\\n",
    "                                                    (input_data['timestamp_est'].dt.time <= datetime.strptime('15:00', '%H:%M').time())\n",
    "\n",
    "            # Calculate indicators\n",
    "            # Trend Indicators\n",
    "            input_data['sma_15'] = ta.trend.sma_indicator(input_data['CLOSE'], window=15)\n",
    "            input_data['sma_30'] = ta.trend.sma_indicator(input_data['CLOSE'], window=30)\n",
    "            input_data['sma_120'] = ta.trend.sma_indicator(input_data['CLOSE'], window=120)\n",
    "            input_data['ema_15'] = ta.trend.ema_indicator(input_data['CLOSE'], window=15)\n",
    "            input_data['ema_30'] = ta.trend.ema_indicator(input_data['CLOSE'], window=30)\n",
    "            input_data['ema_120'] = ta.trend.ema_indicator(input_data['CLOSE'], window=120)\n",
    "            input_data['sma_15_120_pct_diff'] = ((input_data['sma_15'] - input_data['sma_120']) / input_data['sma_120']) * 100\n",
    "            input_data['ema_15_120_pct_diff'] = ((input_data['ema_15'] - input_data['ema_120']) / input_data['ema_120']) * 100\n",
    "        \n",
    "            # 30-minute SMA (Simple Moving Average) for medium-term trend analysis\n",
    "            # 30-minute EMA for a responsive short to medium-term trend\n",
    "\n",
    "            input_data['macd'] = ta.trend.macd(input_data['CLOSE'], window_slow=26, window_fast=12)\n",
    "            input_data['macd_signal'] = ta.trend.macd_signal(input_data['CLOSE'], window_slow=26, window_fast=12, window_sign=9)\n",
    "            input_data['macd_diff'] = input_data['macd'] - input_data['macd_signal']  # MACD Histogram\n",
    "            input_data['adx'] = ta.trend.adx(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=14)\n",
    "            # Standard MACD (Moving Average Convergence Divergence) settings for capturing trend and momentum changes\n",
    "\n",
    "            # Momentum Indicators\n",
    "            input_data['rsi'] = ta.momentum.rsi(input_data['CLOSE'], window=14)\n",
    "            input_data['stochastic_k'] = ta.momentum.stoch(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=14, smooth_window=3)\n",
    "            input_data['stochastic_d'] = ta.momentum.stoch_signal(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=14, smooth_window=3)\n",
    "            input_data['cci'] = ta.trend.cci(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=20)\n",
    "            input_data['williams_r'] = ta.momentum.williams_r(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], lbp=14)\n",
    "            # 14-minute RSI (Relative Strength Index) for overbought/oversold conditions\n",
    "            # 14-minute Stochastic Oscillator for short-term momentum\n",
    "            # 20-minute CCI for identifying cyclical trends\n",
    "\n",
    "            # Volatility Indicators\n",
    "            bb_indicator = ta.volatility.BollingerBands(input_data['CLOSE'], window=20, window_dev=2)\n",
    "            input_data['bollinger_hband'] = bb_indicator.bollinger_hband()\n",
    "            input_data['bollinger_lband'] = bb_indicator.bollinger_lband()\n",
    "            input_data['bollinger_mband'] = bb_indicator.bollinger_mavg()\n",
    "            input_data['bollinger_width_percent'] = (input_data['bollinger_hband'] - input_data['bollinger_lband']) / input_data['CLOSE']\n",
    "            input_data['bollinger_position'] = bb_indicator.bollinger_pband()\n",
    "            input_data['atr'] = ta.volatility.average_true_range(input_data['HIGH'], input_data['LOW'], input_data['CLOSE'], window=14)\n",
    "            # Bollinger Bands with standard deviation score for precise volatility measure\n",
    "            # 14-minute ATR (Average True Range) for short-term volatility\n",
    "\n",
    "            # Volume-Based Indicators\n",
    "            input_data['obv'] = ta.volume.on_balance_volume(input_data['CLOSE'], input_data['VOLUME'])\n",
    "            # OBV (On-Balance Volume) for volume-based trend prediction\n",
    "\n",
    "            # Calculate VWAP for each day\n",
    "            input_data['vwap'] = input_data.groupby(input_data['timestamp_est'].dt.date).apply(\n",
    "                lambda x: (x['CLOSE'] * x['VOLUME']).cumsum() / x['VOLUME'].cumsum()\n",
    "            ).reset_index(level=0, drop=True)\n",
    "\n",
    "            # Additional Technical Indicators\n",
    "            input_data['ichimoku_tenkan_sen'] = ta.trend.ichimoku_conversion_line(input_data['HIGH'], input_data['LOW'], window1=9, window2=26)\n",
    "            input_data['ichimoku_senkou_span_a'] = ta.trend.ichimoku_a(input_data['HIGH'], input_data['LOW'])\n",
    "            input_data['ichimoku_senkou_span_b'] = ta.trend.ichimoku_b(input_data['HIGH'], input_data['LOW'])\n",
    "            input_data['rvi'] = ta.trend.stc(input_data['CLOSE'])\n",
    "            input_data['force_index'] = ta.volume.force_index(input_data['CLOSE'], input_data['VOLUME'])\n",
    "            ao_indicator = ta.momentum.AwesomeOscillatorIndicator(input_data['HIGH'], input_data['LOW'])\n",
    "            input_data['ao_ind'] = ao_indicator.awesome_oscillator()  # Extract the values and store them in the DataFrame\n",
    "            # Ichimoku Tenkan-sen (Conversion Line): The average of the highest high and the lowest low over the last 9 periods.\n",
    "            # Ichimoku Senkou Span A (Leading Span A): The average of the Tenkan-sen and the Kijun-sen, plotted 26 periods ahead.\n",
    "            # Ichimoku Senkou Span B (Leading Span B): The average of the highest high and the lowest low over the last 52 periods, plotted 26 periods ahead.\n",
    "            # Relative Vigor Index (RVI): Measures the conviction of a recent price action and the likelihood that it will continue.\n",
    "            # Force Index: Combines price change and volume to measure the strength of bulls and bears in the market.\n",
    "            # The Awesome Oscillator is an indicator used to measure market momentum. AO calculates the difference of a 34 Period and 5 Period Simple Moving Averages.\n",
    "\n",
    "            # Lagged Features\n",
    "            lagged_features = []\n",
    "\n",
    "            for lag in [1, 2, 3, 4, 5, 10, 20, 30]:\n",
    "                lagged_features.append(input_data['rsi'].shift(lag))\n",
    "                lagged_features.append(input_data['macd_diff'].shift(lag))\n",
    "                lagged_features.append(input_data['cci'].shift(lag))\n",
    "                lagged_features.append(input_data['adx'].shift(lag))\n",
    "                lagged_features.append(input_data['CLOSE'].shift(lag))\n",
    "                lagged_features.append(input_data['VOLUME'].shift(lag))\n",
    "                lagged_features.append(input_data['vwap'].shift(lag))\n",
    "                lagged_features.append(input_data['atr'].shift(lag))\n",
    "\n",
    "            # Concatenate all lagged features\n",
    "            lagged_features_df = pd.concat(lagged_features, axis=1)\n",
    "\n",
    "            # Rename columns appropriately\n",
    "            lagged_feature_columns = [f'{col}_lag{lag}' for lag in [1, 2, 3, 4, 5, 10, 20, 30] for col in ['rsi', 'macd_diff', 'cci', 'adx', 'CLOSE', 'VOLUME', 'vwap', 'atr']]\n",
    "            lagged_features_df.columns = lagged_feature_columns\n",
    "\n",
    "            # Concatenate lagged features with input_data\n",
    "            input_data = pd.concat([input_data, lagged_features_df], axis=1)\n",
    "\n",
    "            # Polynomial Features (example for RSI and CCI)\n",
    "            # You can concatenate them like this:\n",
    "            squared_columns = pd.concat([\n",
    "                input_data['rsi'] ** 2,\n",
    "                input_data['cci'] ** 2,\n",
    "                input_data['macd_diff'] ** 2,\n",
    "                input_data['adx'] ** 2,\n",
    "                input_data['bollinger_width_percent'] ** 2\n",
    "            ], axis=1)\n",
    "            squared_columns.columns = ['rsi_squared', 'cci_squared', 'macd_squared', 'adx_squared', 'bollinger_width_percent_squared']\n",
    "            input_data = pd.concat([input_data, squared_columns], axis=1)\n",
    "\n",
    "            # Interaction Features\n",
    "            input_data['sma_30_rsi'] = input_data['sma_30'] * input_data['rsi']\n",
    "            input_data['macd_stochastic_k'] = input_data['macd_diff'] * input_data['stochastic_k']\n",
    "            input_data['ema_30_atr'] = input_data['ema_30'] * input_data['atr']\n",
    "            input_data['rsi_macd_diff_interaction'] = input_data['rsi'] * input_data['macd_diff']\n",
    "            input_data['sma_30_adx_interaction'] = input_data['sma_30'] * input_data['adx']\n",
    "            input_data['bollinger_position_atr_interaction'] = input_data['bollinger_position'] * input_data['atr']\n",
    "\n",
    "            # Calculate additional features\n",
    "            input_data['date'] = input_data['timestamp_est'].dt.date\n",
    "            input_data['day_of_week'] = input_data['timestamp_est'].dt.dayofweek\n",
    "            input_data['hour_of_day'] = input_data['timestamp_est'].dt.hour\n",
    "            input_data['time_of_day'] = input_data['timestamp_est'].dt.time\n",
    "\n",
    "            # Movement in-day & in-week as percentage\n",
    "            input_data['daily_opening_price'] = input_data.groupby(input_data['timestamp_est'].dt.date)['OPEN'].transform('first')\n",
    "            input_data['daily_movement_since_open'] = (input_data['CLOSE'] - input_data['daily_opening_price']) / input_data['daily_opening_price']\n",
    "\n",
    "            input_data['weekly_opening_price'] = input_data.groupby(input_data['timestamp_est'].dt.isocalendar().week)['OPEN'].transform('first')\n",
    "            input_data['weekly_movement_since_open'] = (input_data['CLOSE'] - input_data['weekly_opening_price']) / input_data['weekly_opening_price']\n",
    "\n",
    "            # Calculate percentage movement over the last 20 periods (20 minutes)\n",
    "            input_data['pct_movement_20_periods'] = input_data['CLOSE'].pct_change(periods=20)\n",
    "\n",
    "            # Calculate percentage movement over the last 60 periods (1 hour)\n",
    "            input_data['pct_movement_60_periods'] = input_data['CLOSE'].pct_change(periods=60)\n",
    "\n",
    "            # Targets\n",
    "            input_data['target_30_min'] = input_data['CLOSE'].shift(-30)\n",
    "            # Target for the closing price 30 minutes from the current time, aligning with the 30 minute trading strategy\n",
    "            # Calculate movement_pct and signal\n",
    "            input_data['target_movement_pct'] = ((input_data['target_30_min'] - input_data['CLOSE']).abs() / input_data['CLOSE']) * 100\n",
    "            input_data['target_movement_signal'] = input_data.apply(\n",
    "                lambda row: 1 if (row['target_30_min'] - row['CLOSE']) > 0.001 * row['CLOSE'] else \n",
    "                            (-1 if (row['target_30_min'] - row['CLOSE']) <= -0.001 * row['CLOSE'] else 0), \n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Drop some columns and reorder\n",
    "            input_data = input_data.drop(columns=['TIMESTAMP', 'timestamp_utc'])\n",
    "            input_data = input_data[['SYMBOL', 'timestamp_est', 'date', 'trading_hours_ind', 'trading_hours_ind_ext'] + [col for col in input_data.columns if col not in ['SYMBOL', 'timestamp_est', 'date', 'trading_hours_ind', 'trading_hours_ind_ext']]]\n",
    "\n",
    "            # Step 4: Store the final DataFrame into the output table\n",
    "            if output_table is not None:\n",
    "                # Truncate the existing table\n",
    "                self.db_write(f\"DELETE FROM {output_table}\")\n",
    "                print(f\"Truncated table {output_table}.\")\n",
    "\n",
    "                # Store the DataFrame in the output table\n",
    "                self.db_append(output_table, input_data)\n",
    "                print(f\"Prepared model data stored in {output_table}. Number of rows inserted: {len(input_data)}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing symbol {symbol}: {e}\")\n",
    "    \n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        return input_data\n",
    "\n",
    "    def classification_modeling_build(self, features, target, symbol):\n",
    "        \"\"\"\n",
    "        Splits the data into training and testing sets based on provided features and target variable for a specific symbol.\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.database_path)\n",
    "        df = pd.read_sql(f\"SELECT * FROM SYMBOL_DATA_MODELING WHERE SYMBOL = '{symbol}' AND TRADING_HOURS_IND_EXT = 1\", conn)\n",
    "        conn.close()\n",
    "\n",
    "        unique_dates = df['DATE'].unique()\n",
    "        train_dates, val_dates = train_test_split(unique_dates, test_size=0.3, random_state=42)\n",
    "\n",
    "        train_data = df[df['DATE'].isin(train_dates)]\n",
    "        val_data = df[df['DATE'].isin(val_dates)]\n",
    "\n",
    "        X_train = train_data[features]\n",
    "        y_train = train_data[target]\n",
    "        X_val = val_data[features]\n",
    "        y_val = val_data[target]\n",
    "\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        models = {\n",
    "            \"K-Nearest Neighbors\": (KNeighborsClassifier(), {\n",
    "                'n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "            }),\n",
    "            \"XGBoost\": (XGBClassifier(eval_metric='mlogloss'), {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'reg_alpha': [0, 0.1, 1],\n",
    "                'reg_lambda': [1, 1.5, 2]\n",
    "            }),\n",
    "            \"Logistic Regression\": (LogisticRegression(max_iter=1000, random_state=42), {\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'C': [0.1, 1, 10],\n",
    "                'solver': ['liblinear', 'saga'],\n",
    "                'class_weight': ['balanced', None]\n",
    "            }),\n",
    "            \"Random Forest\": (RandomForestClassifier(), {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [None, 10, 20, 30],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'bootstrap': [True, False],\n",
    "                'class_weight': ['balanced', 'balanced_subsample', None]\n",
    "            }),\n",
    "            \"Neural Network\": (MLPClassifier(max_iter=1000, random_state=42), {\n",
    "                'hidden_layer_sizes': [(50,), (100,), (150,), (100, 50), (100, 100)],\n",
    "                'activation': ['relu', 'tanh', 'logistic'],\n",
    "                'solver': ['adam', 'sgd', 'lbfgs'],\n",
    "                'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "                'learning_rate': ['constant', 'adaptive'],\n",
    "                'learning_rate_init': [0.001, 0.01, 0.1]\n",
    "            }),\n",
    "            \"Gradient Boosting\": (GradientBoostingClassifier(), {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 4, 5, 6],\n",
    "                'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }),\n",
    "            \"Support Vector Machine\": (SVC(), {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'class_weight': ['balanced', None]\n",
    "            })\n",
    "        }\n",
    "\n",
    "        for model_name, (model, params) in models.items():\n",
    "            print(f\"Processing {model_name} for symbol: {symbol}\")\n",
    "\n",
    "            # Apply standardization only for SVM and Neural Network\n",
    "            if model_name in [\"Support Vector Machine\", \"Neural Network\"]:\n",
    "                scaler = StandardScaler()\n",
    "                X_train_smote = scaler.fit_transform(X_train_smote)\n",
    "                X_val = scaler.transform(X_val)\n",
    "\n",
    "            # These are mostly performance modifications by model type... faster = can do more combos of parameters\n",
    "            grid_search = RandomizedSearchCV(estimator=model, param_distributions=params, n_iter=10, cv=2, n_jobs=-1, scoring='f1_weighted', random_state=42,verbose=3)\n",
    "\n",
    "            if model_name == \"XGBoost\":\n",
    "                y_train_transformed = y_train_smote.replace({-1: 0, 0: 1, 1: 2})\n",
    "                grid_search.fit(X_train_smote, y_train_transformed)\n",
    "                best_model = grid_search.best_estimator_\n",
    "                y_pred_transformed = best_model.predict(X_val)\n",
    "                y_pred = pd.Series(y_pred_transformed).replace({0: -1, 1: 0, 2: 1})\n",
    "            else:\n",
    "                grid_search.fit(X_train_smote, y_train_smote)\n",
    "                best_model = grid_search.best_estimator_\n",
    "                y_pred = best_model.predict(X_val)\n",
    "\n",
    "            f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "            cm = confusion_matrix(y_val, y_pred)\n",
    "            cr = classification_report(y_val, y_pred, zero_division=0)\n",
    "\n",
    "            # visualize & print results\n",
    "            print(f\"{model_name} - F1 Score: {f1}\")\n",
    "            print(\"Confusion Matrix:\\n\", cm)\n",
    "            print(\"Classification Report:\\n\", cr)\n",
    "            print(\"Best Hyperparameters:\\n\", grid_search.best_params_)\n",
    "            sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='d')\n",
    "            plt.title(f'Confusion Matrix: {model_name}')\n",
    "            plt.show()\n",
    "            print(f\"__________________________________\")\n",
    "            # Save the model with a unique filename\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            joblib.dump(best_model, f'{symbol}_{model_name}_{timestamp}.pkl')\n",
    "\n",
    "            evaluation_results = {\n",
    "                'SYMBOL': symbol,\n",
    "                'MODEL_NAME': model_name,\n",
    "                'F1_SCORE': f1,\n",
    "                'CONFUSION_MATRIX': str(cm),\n",
    "                'CLASSIFICATION_REPORT': cr,\n",
    "                'BEST_PARAMS': str(grid_search.best_params_)\n",
    "            }\n",
    "            results_df = pd.DataFrame([evaluation_results])\n",
    "            self.db_append('CLASSIFICATION_MODEL_RESULTS', results_df)\n",
    "\n",
    "            # Feature Importance or Coefficients\n",
    "            if hasattr(best_model, \"feature_importances_\"):\n",
    "                importances = best_model.feature_importances_\n",
    "                feature_importance_df = pd.DataFrame({\n",
    "                    'feature': features,\n",
    "                    'importance': importances\n",
    "                }).sort_values(by='importance', ascending=False)\n",
    "                print(f\"Feature importances for {model_name}:\")\n",
    "                print(feature_importance_df)\n",
    "            elif hasattr(best_model, \"coef_\"):\n",
    "                coefs = best_model.coef_[0]\n",
    "                coef_df = pd.DataFrame({\n",
    "                    'feature': features,\n",
    "                    'coefficient': coefs\n",
    "                }).sort_values(by='coefficient', ascending=False)\n",
    "                print(f\"Coefficients for {model_name}:\")\n",
    "                print(coef_df)\n",
    "            else:\n",
    "                print(f\"No feature importance or coefficients for {model_name}\")\n",
    "            print(f\"____________________________________________\")\n",
    "            print(f\"____________________________________________\")\n",
    "\n",
    "    def classification_modeling_apply(self, input_table, symbol, output_table=None):\n",
    "        \"\"\"\n",
    "        This method reads data from the stage table, adds indicators, makes predictions using models,\n",
    "        and stores the results in a new table.\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        try:\n",
    "            # Step 1: Transfer stage data and add indicators\n",
    "            indicators_data = self.transfer_stage_modeling(input_table, symbol)\n",
    "            indicators_data.columns = indicators_data.columns.str.upper()\n",
    "            indicators_data = indicators_data[indicators_data['TRADING_HOURS_IND_EXT'] == 1]\n",
    "\n",
    "            if indicators_data.empty:\n",
    "                raise ValueError(f\"No data found for symbol {symbol} in table {input_table}\")\n",
    "\n",
    "            # Extract feature columns\n",
    "            feature_columns = ['CLOSE','HIGH','LOW','TRADE_COUNT','OPEN','VOLUME','VWAP','SMA_15','SMA_30','SMA_120','EMA_15','EMA_30','EMA_120','SMA_15_120_PCT_DIFF','EMA_15_120_PCT_DIFF',\n",
    "            'MACD','MACD_SIGNAL','MACD_DIFF','ADX','RSI','STOCHASTIC_K','STOCHASTIC_D','CCI','WILLIAMS_R','BOLLINGER_WIDTH_PERCENT','BOLLINGER_POSITION','ATR','OBV',\n",
    "            'ICHIMOKU_TENKAN_SEN','ICHIMOKU_SENKOU_SPAN_A','ICHIMOKU_SENKOU_SPAN_B','RVI','FORCE_INDEX','AO_IND','RSI_LAG1','MACD_DIFF_LAG1','CCI_LAG1','ADX_LAG1','CLOSE_LAG1',\n",
    "            'VOLUME_LAG1','VWAP_LAG1','ATR_LAG1','RSI_LAG2','MACD_DIFF_LAG2','CCI_LAG2','ADX_LAG2','CLOSE_LAG2','VOLUME_LAG2','VWAP_LAG2','ATR_LAG2','RSI_LAG3','MACD_DIFF_LAG3','CCI_LAG3',\n",
    "            'ADX_LAG3','CLOSE_LAG3','VOLUME_LAG3','VWAP_LAG3','ATR_LAG3','RSI_LAG4','MACD_DIFF_LAG4','CCI_LAG4','ADX_LAG4','CLOSE_LAG4','VOLUME_LAG4','VWAP_LAG4','ATR_LAG4','RSI_LAG5',\n",
    "            'MACD_DIFF_LAG5','CCI_LAG5','ADX_LAG5','CLOSE_LAG5','VOLUME_LAG5','VWAP_LAG5','ATR_LAG5','RSI_LAG10','MACD_DIFF_LAG10','CCI_LAG10','ADX_LAG10','CLOSE_LAG10','VOLUME_LAG10',\n",
    "            'VWAP_LAG10','ATR_LAG10','RSI_LAG20','MACD_DIFF_LAG20','CCI_LAG20','ADX_LAG20','CLOSE_LAG20','VOLUME_LAG20','VWAP_LAG20','ATR_LAG20','RSI_LAG30','MACD_DIFF_LAG30','CCI_LAG30',\n",
    "            'ADX_LAG30','CLOSE_LAG30','VOLUME_LAG30','VWAP_LAG30','ATR_LAG30','RSI_SQUARED','CCI_SQUARED','MACD_SQUARED','ADX_SQUARED','BOLLINGER_WIDTH_PERCENT_SQUARED','SMA_30_RSI',\n",
    "            'MACD_STOCHASTIC_K','EMA_30_ATR','RSI_MACD_DIFF_INTERACTION','SMA_30_ADX_INTERACTION','BOLLINGER_POSITION_ATR_INTERACTION','DAY_OF_WEEK','HOUR_OF_DAY',\n",
    "            'DAILY_MOVEMENT_SINCE_OPEN','WEEKLY_MOVEMENT_SINCE_OPEN','PCT_MOVEMENT_20_PERIODS','PCT_MOVEMENT_60_PERIODS' ]\n",
    "            \n",
    "            # Ensure only the feature columns are used for prediction\n",
    "            feature_data = indicators_data[feature_columns]\n",
    "\n",
    "            # Step 2: Load models\n",
    "            models = {\n",
    "                \"Random_Forest\": joblib.load('SPY_Random Forest_20240603_103726.pkl'),\n",
    "                \"XGBoost\": joblib.load('SPY_XGBoost_20240603_135615.pkl'),\n",
    "                \"Gradient_Boosting\": joblib.load('SPY_Gradient Boosting_20240603_173344.pkl'),\n",
    "                \"Neural_Network\": joblib.load('SPY_Neural Network_20240604_104103.pkl'),\n",
    "            }\n",
    "\n",
    "            # Step 3: Make predictions using each model and store the results in the DataFrame\n",
    "            for model_name, model in models.items():\n",
    "                print(f\"Predicting with {model_name} model.\")\n",
    "                if model_name == \"XGBoost\":\n",
    "                    predictions = model.predict(feature_data)\n",
    "                    # Inverse transform the predictions for XGBoost\n",
    "                    predictions = pd.Series(predictions).replace({0: -1, 1: 0, 2: 1}).values\n",
    "                elif model_name == \"Neural_Network\":\n",
    "                    scaler = StandardScaler()\n",
    "                    X_scaled = scaler.fit_transform(feature_data)\n",
    "                    predictions = model.predict(X_scaled)\n",
    "                else:\n",
    "                    predictions = model.predict(feature_data)\n",
    "                \n",
    "                indicators_data[f'{model_name}_prediction'] = predictions\n",
    "                print(f\"{model_name} prediction computed for {symbol}.\")\n",
    "                print(f\"Unique predictions for {model_name}: {pd.Series(predictions).unique()}\")\n",
    "\n",
    "            # Step 4: Store the final DataFrame into the output table\n",
    "            if output_table is not None:\n",
    "                # Truncate the existing table\n",
    "                self.db_write(f\"DELETE FROM {output_table}\")\n",
    "                print(f\"Truncated table {output_table}.\")\n",
    "\n",
    "                # Store the DataFrame in the output table\n",
    "                self.db_append(output_table, indicators_data)\n",
    "                print(f\"Model predictions stored in {output_table}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing symbol {symbol}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        return indicators_data\n",
    "    \n",
    "    ######################################################  Database write/read methods ######################################################\n",
    "    def db_write(self, sql_statement):\n",
    "        \"\"\"\n",
    "\n",
    "        Basic functionality to execute an SQL statement against our database\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database & create a cursor object\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Execute the SQL statement\n",
    "        cur.execute(sql_statement)\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def db_append(self, table_name, data_frame):\n",
    "        \"\"\"\n",
    "\n",
    "        Basic functionality to append/insert a dataframe into a specified table in our database\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        # Append our dataframe into our table\n",
    "        data_frame.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "    def db_append_no_duplicates(self, table_name, data_frame):\n",
    "        \"\"\"\n",
    "\n",
    "        Variation of db_append that makes sure we aren't inserting duplicates by first checking against the primary key of the table records are being inserted into\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Get the primary key column names\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        table_info = cursor.fetchall()\n",
    "\n",
    "        # Identify columns that are part of the primary key\n",
    "        primary_key_columns = [column[1] for column in table_info if column[5] > 0]\n",
    "\n",
    "        # If no primary key columns found, fall back to db_append method\n",
    "        if not primary_key_columns:\n",
    "            data_frame.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "            \n",
    "            # Commit the changes & close the connection\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return\n",
    "    \n",
    "        # Convert primary key columns and DataFrame columns to uppercase for case-insensitive comparison\n",
    "        primary_key_columns_upper = [col.upper() for col in primary_key_columns]\n",
    "        data_frame.columns = data_frame.columns.str.upper()\n",
    "\n",
    "        # Ensure the DataFrame contains all primary key columns\n",
    "        for column in primary_key_columns_upper:\n",
    "            if column not in data_frame.columns:\n",
    "                raise KeyError(f\"Column '{column}' is missing in the DataFrame\")\n",
    "\n",
    "        # Standardize the timestamp format in the DataFrame\n",
    "        timestamp_column = None\n",
    "        for col in primary_key_columns_upper:\n",
    "            if 'TIMESTAMP' in col:\n",
    "                timestamp_column = col\n",
    "                break\n",
    "        \n",
    "        if timestamp_column:\n",
    "            data_frame[timestamp_column] = pd.to_datetime(data_frame[timestamp_column], utc=True)\n",
    "\n",
    "        # Construct the SELECT statement to fetch existing primary keys from the table\n",
    "        existing_keys_query = f\"SELECT {', '.join(primary_key_columns)} FROM {table_name}\"\n",
    "        existing_keys_df = pd.read_sql(existing_keys_query, conn)\n",
    "\n",
    "        # Standardize the timestamp format in the existing keys DataFrame\n",
    "        if timestamp_column:\n",
    "            existing_keys_df[timestamp_column] = pd.to_datetime(existing_keys_df[timestamp_column], utc=True)\n",
    "\n",
    "        # Construct the composite primary key from the existing keys DataFrame\n",
    "        existing_keys_df['COMPOSITE_KEY'] = existing_keys_df.apply(lambda row: tuple(row), axis=1)\n",
    "        existing_keys = set(existing_keys_df['COMPOSITE_KEY'])\n",
    "\n",
    "        # Print count of rows in the DataFrame before filtering\n",
    "        print(f\"Total count of data records queried: {len(data_frame)}\")\n",
    "\n",
    "        # Construct the composite primary key for new records\n",
    "        data_frame['COMPOSITE_KEY'] = data_frame.apply(lambda row: tuple(row[primary_key_columns_upper]), axis=1)\n",
    "\n",
    "        # Filter out rows with primary keys that already exist in the table\n",
    "        data_frame_new_records = data_frame[~data_frame['COMPOSITE_KEY'].isin(existing_keys)]\n",
    "\n",
    "        # Drop the composite key column\n",
    "        data_frame_new_records = data_frame_new_records.drop(columns=['COMPOSITE_KEY'])\n",
    "\n",
    "        # Append only the new rows into our table\n",
    "        if not data_frame_new_records.empty:\n",
    "            data_frame_new_records.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "\n",
    "        # Print count of rows that will be inserted\n",
    "        print(f\"Count of fresh data records actually inserted into {table_name}: {len(data_frame_new_records)}\")\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema DDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Schema DDL - creates the tables in our database that we need for storing the raw scraped data & analyzing it.\n",
    "\n",
    "\"\"\"\n",
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# This table will house the raw scraped data\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS STG_SYMBOL_DATA''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS STG_SYMBOL_DATA (\n",
    "#     SYMBOL                  TEXT,\n",
    "#     TIMESTAMP               TIMESTAMP,\n",
    "#     CLOSE                   DECIMAL,\n",
    "#     HIGH                    DECIMAL,\n",
    "#     LOW                     DECIMAL,\n",
    "#     TRADE_COUNT             INTEGER,\n",
    "#     OPEN                    DECIMAL,\n",
    "#     VOLUME                  INTEGER,\n",
    "#     VWAP                    DECIMAL,\n",
    "#     PRIMARY KEY (SYMBOL, TIMESTAMP)\n",
    "# )\n",
    "# ''')\n",
    "\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS SYMBOL_DATA_MODELING''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS SYMBOL_DATA_MODELING (\n",
    "#     SYMBOL                      TEXT,\n",
    "#     TIMESTAMP_EST               TIMESTAMP,\n",
    "#     DATE                        DATE,\n",
    "#     TRADING_HOURS_IND           BOOLEAN,\n",
    "#     TRADING_HOURS_IND_EXT       BOOLEAN,\n",
    "#     CLOSE                       DECIMAL,\n",
    "#     HIGH                        DECIMAL,\n",
    "#     LOW                         DECIMAL,\n",
    "#     TRADE_COUNT                 INTEGER,\n",
    "#     OPEN                        DECIMAL,\n",
    "#     VOLUME                      INTEGER,\n",
    "#     VWAP                        DECIMAL,\n",
    "#     SMA_15                      DECIMAL,\n",
    "#     SMA_30                      DECIMAL,\n",
    "#     SMA_120                     DECIMAL,\n",
    "#     EMA_15                      DECIMAL,\n",
    "#     EMA_30                      DECIMAL,\n",
    "#     EMA_120                     DECIMAL,\n",
    "#     SMA_15_120_PCT_DIFF         DECIMAL,                    \n",
    "#     EMA_15_120_PCT_DIFF         DECIMAL,\n",
    "#     MACD                        DECIMAL,\n",
    "#     MACD_SIGNAL                 DECIMAL,\n",
    "#     MACD_DIFF                   DECIMAL,\n",
    "#     ADX                         DECIMAL,\n",
    "#     RSI                         DECIMAL,                      \n",
    "#     STOCHASTIC_K                DECIMAL,\n",
    "#     STOCHASTIC_D                DECIMAL,\n",
    "#     CCI                         DECIMAL,\n",
    "#     WILLIAMS_R                  DECIMAL, \n",
    "#     BOLLINGER_HBAND             DECIMAL,\n",
    "#     BOLLINGER_LBAND             DECIMAL,\n",
    "#     BOLLINGER_MBAND             DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT     DECIMAL,\n",
    "#     BOLLINGER_POSITION          DECIMAL,\n",
    "#     ATR                         DECIMAL,\n",
    "#     OBV                         INTEGER, \n",
    "#     ICHIMOKU_TENKAN_SEN         DECIMAL, \n",
    "#     ICHIMOKU_SENKOU_SPAN_A      DECIMAL,\n",
    "#     ICHIMOKU_SENKOU_SPAN_B      DECIMAL,\n",
    "#     RVI                         DECIMAL,\n",
    "#     FORCE_INDEX                 DECIMAL,\n",
    "#     AO_IND                      DECIMAL,\n",
    "#     RSI_LAG1                    DECIMAL,\n",
    "#     MACD_DIFF_LAG1              DECIMAL,\n",
    "#     CCI_LAG1                    DECIMAL,\n",
    "#     ADX_LAG1                    DECIMAL, \n",
    "#     CLOSE_LAG1                  DECIMAL,\n",
    "#     VOLUME_LAG1                 DECIMAL,\n",
    "#     VWAP_LAG1                   DECIMAL,\n",
    "#     ATR_LAG1                    DECIMAL,\n",
    "#     RSI_LAG2                    DECIMAL,\n",
    "#     MACD_DIFF_LAG2              DECIMAL,\n",
    "#     CCI_LAG2                    DECIMAL,\n",
    "#     ADX_LAG2                    DECIMAL,\n",
    "#     CLOSE_LAG2                  DECIMAL,\n",
    "#     VOLUME_LAG2                 DECIMAL,\n",
    "#     VWAP_LAG2                   DECIMAL,\n",
    "#     ATR_LAG2                    DECIMAL,\n",
    "#     RSI_LAG3                    DECIMAL,\n",
    "#     MACD_DIFF_LAG3              DECIMAL,\n",
    "#     CCI_LAG3                    DECIMAL,\n",
    "#     ADX_LAG3                    DECIMAL,\n",
    "#     CLOSE_LAG3                  DECIMAL,\n",
    "#     VOLUME_LAG3                 DECIMAL,\n",
    "#     VWAP_LAG3                   DECIMAL,\n",
    "#     ATR_LAG3                    DECIMAL,                      \n",
    "#     RSI_LAG4                    DECIMAL,\n",
    "#     MACD_DIFF_LAG4              DECIMAL,\n",
    "#     CCI_LAG4                    DECIMAL,\n",
    "#     ADX_LAG4                    DECIMAL,\n",
    "#     CLOSE_LAG4                  DECIMAL,\n",
    "#     VOLUME_LAG4                 DECIMAL,\n",
    "#     VWAP_LAG4                   DECIMAL,\n",
    "#     ATR_LAG4                    DECIMAL,                           \n",
    "#     RSI_LAG5                    DECIMAL,\n",
    "#     MACD_DIFF_LAG5              DECIMAL,\n",
    "#     CCI_LAG5                    DECIMAL,\n",
    "#     ADX_LAG5                    DECIMAL,\n",
    "#     CLOSE_LAG5                  DECIMAL,\n",
    "#     VOLUME_LAG5                 DECIMAL,\n",
    "#     VWAP_LAG5                   DECIMAL,\n",
    "#     ATR_LAG5                    DECIMAL,\n",
    "#     RSI_LAG10                   DECIMAL,\n",
    "#     MACD_DIFF_LAG10             DECIMAL,\n",
    "#     CCI_LAG10                   DECIMAL,\n",
    "#     ADX_LAG10                   DECIMAL,\n",
    "#     CLOSE_LAG10                 DECIMAL,\n",
    "#     VOLUME_LAG10                DECIMAL,\n",
    "#     VWAP_LAG10                  DECIMAL,\n",
    "#     ATR_LAG10                   DECIMAL,    \n",
    "#     RSI_LAG20                   DECIMAL,\n",
    "#     MACD_DIFF_LAG20             DECIMAL,\n",
    "#     CCI_LAG20                   DECIMAL,\n",
    "#     ADX_LAG20                   DECIMAL,\n",
    "#     CLOSE_LAG20                 DECIMAL,\n",
    "#     VOLUME_LAG20                DECIMAL,\n",
    "#     VWAP_LAG20                  DECIMAL,\n",
    "#     ATR_LAG20                   DECIMAL,                     \n",
    "#     RSI_LAG30                   DECIMAL,\n",
    "#     MACD_DIFF_LAG30             DECIMAL,\n",
    "#     CCI_LAG30                   DECIMAL,\n",
    "#     ADX_LAG30                   DECIMAL,\n",
    "#     CLOSE_LAG30                 DECIMAL,\n",
    "#     VOLUME_LAG30                DECIMAL,\n",
    "#     VWAP_LAG30                  DECIMAL,\n",
    "#     ATR_LAG30                   DECIMAL,\n",
    "#     RSI_SQUARED                 DECIMAL,\n",
    "#     CCI_SQUARED                 DECIMAL,\n",
    "#     MACD_SQUARED                DECIMAL,\n",
    "#     ADX_SQUARED                 DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT_SQUARED         DECIMAL,\n",
    "#     SMA_30_RSI                  DECIMAL,\n",
    "#     MACD_STOCHASTIC_K           DECIMAL,\n",
    "#     EMA_30_ATR                  DECIMAL,\n",
    "#     RSI_MACD_DIFF_INTERACTION   DECIMAL,\n",
    "#     SMA_30_ADX_INTERACTION      DECIMAL,\n",
    "#     BOLLINGER_POSITION_ATR_INTERACTION      DECIMAL,\n",
    "#     DAY_OF_WEEK                 INTEGER,\n",
    "#     HOUR_OF_DAY                 INTEGER,\n",
    "#     TIME_OF_DAY                 TIME,                      \n",
    "#     DAILY_OPENING_PRICE         DECIMAL,\n",
    "#     DAILY_MOVEMENT_SINCE_OPEN   DECIMAL,\n",
    "#     WEEKLY_OPENING_PRICE        DECIMAL,\n",
    "#     WEEKLY_MOVEMENT_SINCE_OPEN  DECIMAL,\n",
    "#     PCT_MOVEMENT_20_PERIODS     DECIMAL,\n",
    "#     PCT_MOVEMENT_60_PERIODS     DECIMAL,\n",
    "#     TARGET_30_MIN               DECIMAL,\n",
    "#     TARGET_MOVEMENT_PCT         DECIMAL,\n",
    "#     TARGET_MOVEMENT_SIGNAL      INTEGER,\n",
    "#     PRIMARY KEY (SYMBOL, TIMESTAMP_EST)\n",
    "# )\n",
    "# ''')\n",
    "\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS CLASSIFICATION_MODEL_RESULTS''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS CLASSIFICATION_MODEL_RESULTS  (\n",
    "#     ID                      INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "#     SYMBOL                  TEXT NOT NULL,\n",
    "#     MODEL_NAME              TEXT NOT NULL,\n",
    "#     F1_SCORE                REAL NOT NULL,\n",
    "#     CONFUSION_MATRIX        TEXT NOT NULL,\n",
    "#     CLASSIFICATION_REPORT   TEXT NOT NULL,\n",
    "#     BEST_PARAMS             TEXT NOT NULL,                 \n",
    "#     EVALUATION_TIMESTAMP    DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "# )\n",
    "# ''')\n",
    "\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS CLASSIFICATION_MODEL_PREDICTIONS''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS CLASSIFICATION_MODEL_PREDICTIONS (\n",
    "#     SYMBOL                      TEXT,\n",
    "#     TIMESTAMP_EST               TIMESTAMP,\n",
    "#     DATE                        DATE,\n",
    "#     TRADING_HOURS_IND           BOOLEAN,\n",
    "#     TRADING_HOURS_IND_EXT       BOOLEAN,\n",
    "#     CLOSE                       DECIMAL,\n",
    "#     HIGH                        DECIMAL,\n",
    "#     LOW                         DECIMAL,\n",
    "#     TRADE_COUNT                 INTEGER,\n",
    "#     OPEN                        DECIMAL,\n",
    "#     VOLUME                      INTEGER,\n",
    "#     VWAP                        DECIMAL,\n",
    "#     SMA_15                      DECIMAL,\n",
    "#     SMA_30                      DECIMAL,\n",
    "#     SMA_120                     DECIMAL,\n",
    "#     EMA_15                      DECIMAL,\n",
    "#     EMA_30                      DECIMAL,\n",
    "#     EMA_120                     DECIMAL,\n",
    "#     SMA_15_120_PCT_DIFF         DECIMAL,                    \n",
    "#     EMA_15_120_PCT_DIFF         DECIMAL,\n",
    "#     MACD                        DECIMAL,\n",
    "#     MACD_SIGNAL                 DECIMAL,\n",
    "#     MACD_DIFF                   DECIMAL,\n",
    "#     ADX                         DECIMAL,\n",
    "#     RSI                         DECIMAL,                      \n",
    "#     STOCHASTIC_K                DECIMAL,\n",
    "#     STOCHASTIC_D                DECIMAL,\n",
    "#     CCI                         DECIMAL,\n",
    "#     WILLIAMS_R                  DECIMAL, \n",
    "#     BOLLINGER_HBAND             DECIMAL,\n",
    "#     BOLLINGER_LBAND             DECIMAL,\n",
    "#     BOLLINGER_MBAND             DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT     DECIMAL,\n",
    "#     BOLLINGER_POSITION          DECIMAL,\n",
    "#     ATR                         DECIMAL,\n",
    "#     OBV                         INTEGER, \n",
    "#     ICHIMOKU_TENKAN_SEN         DECIMAL, \n",
    "#     ICHIMOKU_SENKOU_SPAN_A      DECIMAL,\n",
    "#     ICHIMOKU_SENKOU_SPAN_B      DECIMAL,\n",
    "#     RVI                         DECIMAL,\n",
    "#     FORCE_INDEX                 DECIMAL,\n",
    "#     AO_IND                      DECIMAL,\n",
    "#     RSI_LAG1                    DECIMAL,\n",
    "#     MACD_DIFF_LAG1              DECIMAL,\n",
    "#     CCI_LAG1                    DECIMAL,\n",
    "#     ADX_LAG1                    DECIMAL, \n",
    "#     CLOSE_LAG1                  DECIMAL,\n",
    "#     VOLUME_LAG1                 DECIMAL,\n",
    "#     VWAP_LAG1                   DECIMAL,\n",
    "#     ATR_LAG1                    DECIMAL,\n",
    "#     RSI_LAG2                    DECIMAL,\n",
    "#     MACD_DIFF_LAG2              DECIMAL,\n",
    "#     CCI_LAG2                    DECIMAL,\n",
    "#     ADX_LAG2                    DECIMAL,\n",
    "#     CLOSE_LAG2                  DECIMAL,\n",
    "#     VOLUME_LAG2                 DECIMAL,\n",
    "#     VWAP_LAG2                   DECIMAL,\n",
    "#     ATR_LAG2                    DECIMAL,\n",
    "#     RSI_LAG3                    DECIMAL,\n",
    "#     MACD_DIFF_LAG3              DECIMAL,\n",
    "#     CCI_LAG3                    DECIMAL,\n",
    "#     ADX_LAG3                    DECIMAL,\n",
    "#     CLOSE_LAG3                  DECIMAL,\n",
    "#     VOLUME_LAG3                 DECIMAL,\n",
    "#     VWAP_LAG3                   DECIMAL,\n",
    "#     ATR_LAG3                    DECIMAL,                      \n",
    "#     RSI_LAG4                    DECIMAL,\n",
    "#     MACD_DIFF_LAG4              DECIMAL,\n",
    "#     CCI_LAG4                    DECIMAL,\n",
    "#     ADX_LAG4                    DECIMAL,\n",
    "#     CLOSE_LAG4                  DECIMAL,\n",
    "#     VOLUME_LAG4                 DECIMAL,\n",
    "#     VWAP_LAG4                   DECIMAL,\n",
    "#     ATR_LAG4                    DECIMAL,                           \n",
    "#     RSI_LAG5                    DECIMAL,\n",
    "#     MACD_DIFF_LAG5              DECIMAL,\n",
    "#     CCI_LAG5                    DECIMAL,\n",
    "#     ADX_LAG5                    DECIMAL,\n",
    "#     CLOSE_LAG5                  DECIMAL,\n",
    "#     VOLUME_LAG5                 DECIMAL,\n",
    "#     VWAP_LAG5                   DECIMAL,\n",
    "#     ATR_LAG5                    DECIMAL,\n",
    "#     RSI_LAG10                   DECIMAL,\n",
    "#     MACD_DIFF_LAG10             DECIMAL,\n",
    "#     CCI_LAG10                   DECIMAL,\n",
    "#     ADX_LAG10                   DECIMAL,\n",
    "#     CLOSE_LAG10                 DECIMAL,\n",
    "#     VOLUME_LAG10                DECIMAL,\n",
    "#     VWAP_LAG10                  DECIMAL,\n",
    "#     ATR_LAG10                   DECIMAL,    \n",
    "#     RSI_LAG20                   DECIMAL,\n",
    "#     MACD_DIFF_LAG20             DECIMAL,\n",
    "#     CCI_LAG20                   DECIMAL,\n",
    "#     ADX_LAG20                   DECIMAL,\n",
    "#     CLOSE_LAG20                 DECIMAL,\n",
    "#     VOLUME_LAG20                DECIMAL,\n",
    "#     VWAP_LAG20                  DECIMAL,\n",
    "#     ATR_LAG20                   DECIMAL,                     \n",
    "#     RSI_LAG30                   DECIMAL,\n",
    "#     MACD_DIFF_LAG30             DECIMAL,\n",
    "#     CCI_LAG30                   DECIMAL,\n",
    "#     ADX_LAG30                   DECIMAL,\n",
    "#     CLOSE_LAG30                 DECIMAL,\n",
    "#     VOLUME_LAG30                DECIMAL,\n",
    "#     VWAP_LAG30                  DECIMAL,\n",
    "#     ATR_LAG30                   DECIMAL,\n",
    "#     RSI_SQUARED                 DECIMAL,\n",
    "#     CCI_SQUARED                 DECIMAL,\n",
    "#     MACD_SQUARED                DECIMAL,\n",
    "#     ADX_SQUARED                 DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT_SQUARED         DECIMAL,\n",
    "#     SMA_30_RSI                  DECIMAL,\n",
    "#     MACD_STOCHASTIC_K           DECIMAL,\n",
    "#     EMA_30_ATR                  DECIMAL,\n",
    "#     RSI_MACD_DIFF_INTERACTION   DECIMAL,\n",
    "#     SMA_30_ADX_INTERACTION      DECIMAL,\n",
    "#     BOLLINGER_POSITION_ATR_INTERACTION      DECIMAL,\n",
    "#     DAY_OF_WEEK                 INTEGER,\n",
    "#     HOUR_OF_DAY                 INTEGER,\n",
    "#     TIME_OF_DAY                 TIME,                      \n",
    "#     DAILY_OPENING_PRICE         DECIMAL,\n",
    "#     DAILY_MOVEMENT_SINCE_OPEN   DECIMAL,\n",
    "#     WEEKLY_OPENING_PRICE        DECIMAL,\n",
    "#     WEEKLY_MOVEMENT_SINCE_OPEN  DECIMAL,\n",
    "#     PCT_MOVEMENT_20_PERIODS     DECIMAL,\n",
    "#     PCT_MOVEMENT_60_PERIODS     DECIMAL,\n",
    "#     TARGET_30_MIN               DECIMAL,\n",
    "#     TARGET_MOVEMENT_PCT         DECIMAL,\n",
    "#     TARGET_MOVEMENT_SIGNAL      INTEGER,\n",
    "#     RANDOM_FOREST_PREDICTION        INTEGER,                \n",
    "#     XGBOOST_PREDICTION              INTEGER,     \n",
    "#     GRADIENT_BOOSTING_PREDICTION    INTEGER,     \n",
    "#     NEURAL_NETWORK_PREDICTION       INTEGER,     \n",
    "#     PRIMARY KEY (SYMBOL, TIMESTAMP_EST)\n",
    "# )\n",
    "# ''')\n",
    "\n",
    "# sql_statement = atb.db_write('''DROP TABLE IF EXISTS SYMBOL_SEQUENCE_MODELING''')\n",
    "# sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS SYMBOL_SEQUENCE_MODELING (\n",
    "#     SYMBOL                      TEXT,\n",
    "#     TIMESTAMP_EST               TIMESTAMP,\n",
    "#     DATE                        DATE,\n",
    "#     TRADING_HOURS_IND           BOOLEAN,\n",
    "#     TRADING_HOURS_IND_EXT       BOOLEAN,\n",
    "#     CLOSE                       DECIMAL,\n",
    "#     HIGH                        DECIMAL,\n",
    "#     LOW                         DECIMAL,\n",
    "#     TRADE_COUNT                 INTEGER,\n",
    "#     OPEN                        DECIMAL,\n",
    "#     VOLUME                      INTEGER,\n",
    "#     VWAP                        DECIMAL,\n",
    "#     RSI                         DECIMAL,\n",
    "#     ATR                         DECIMAL,\n",
    "#     CCI                         DECIMAL,\n",
    "#     BOLLINGER_WIDTH_PERCENT     DECIMAL,\n",
    "#     BOLLINGER_POSITION          DECIMAL,\n",
    "#     TARGET_30_MIN               DECIMAL,\n",
    "#     TARGET_MOVEMENT_PCT         DECIMAL,\n",
    "#     TARGET_MOVEMENT_SIGNAL      INTEGER,\n",
    "#     PRIMARY KEY (SYMBOL, TIMESTAMP_EST)\n",
    "# )\n",
    "# ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape & Store Data In Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped for 2024-06-05 through 11:59:59Z...\n",
      "Data scraped for 2024-06-05 through 23:59:59Z...\n",
      "Total API calls made so far: 2\n",
      "Pausing for 16.4940822757135 after scraping data for 2024-06-05...\n",
      "__________________________________\n",
      "Scraping Bar Data complete for timerange: 2024-06-05 08:00:00+00:00  -  2024-06-05 23:59:00+00:00\n",
      "Total count of data records queried: 785\n",
      "Count of fresh data records actually inserted into STG_SYMBOL_DATA: 0\n"
     ]
    }
   ],
   "source": [
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# Define the list of stocks\n",
    "stocks = [\"SPY\"]\n",
    "\n",
    "# Define the time range for historical data\n",
    "start_date = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "end_date = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Download and store historical data in the staging table\n",
    "data = {}\n",
    "for stock in stocks:\n",
    "    # Download historical data\n",
    "    stock_data = atb.download_bar_data(stock, TimeFrame(1, TimeFrameUnit.Minute), start_date, end_date)\n",
    "    data[stock] = stock_data\n",
    "\n",
    "# Store all downloaded data in the staging table\n",
    "for stock, stock_data in data.items():\n",
    "    atb.db_append_no_duplicates('STG_SYMBOL_DATA', stock_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated table SYMBOL_SEQUENCE_MODELING.\n",
      "Prepared sequence model data stored in SYMBOL_SEQUENCE_MODELING. Number of rows inserted: 916895.\n"
     ]
    }
   ],
   "source": [
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# Define the list of stocks\n",
    "stocks = [\"SPY\"]\n",
    "\n",
    "# Step 2b: Transfer data from the staging table to the modeling table\n",
    "for stock in stocks:\n",
    "    atb.sequence_modeling_prep('STG_SYMBOL_DATA', stock, 'SYMBOL_SEQUENCE_MODELING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing symbol: SPY\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "AlpacaTradingBot.create_sequences() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m symbol \u001b[38;5;129;01min\u001b[39;00m symbols:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing symbol: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     \u001b[43matb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_modeling_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted processing for symbol: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 196\u001b[0m, in \u001b[0;36mAlpacaTradingBot.sequence_modeling_build\u001b[1;34m(self, features, target, symbol, window)\u001b[0m\n\u001b[0;32m    193\u001b[0m val_data \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(val_dates)]\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Create the sequences for each of our features\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m X_val, y_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_sequences(val_data, features, target, \u001b[38;5;28mint\u001b[39m(window))\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Define models with parameter grids for hyperparameter tuning\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: AlpacaTradingBot.create_sequences() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "# Instantiate and use the trading bot\n",
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# Select features, target, symbol and sequence window to run our classifaction models on \n",
    "symbols = ['SPY']\n",
    "target = 'TARGET_MOVEMENT_SIGNAL'\n",
    "features = ['CLOSE','HIGH','LOW','TRADE_COUNT','OPEN','VOLUME','VWAP','RSI','ATR','CCI','BOLLINGER_WIDTH_PERCENT','BOLLINGER_POSITION']\n",
    "window = 30\n",
    "\n",
    "# Run models for each symbol\n",
    "for symbol in symbols:\n",
    "    print(f\"Processing symbol: {symbol}\")\n",
    "    atb.sequence_modeling_build(features, target, symbol, window)\n",
    "    print(f\"Completed processing for symbol: {symbol}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with Random_Forest model.\n",
      "Error processing symbol SPY: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- ADX\n",
      "- ADX_LAG1\n",
      "- ADX_LAG10\n",
      "- ADX_LAG2\n",
      "- ADX_LAG20\n",
      "- ...\n",
      "\n",
      "Model predictions for SPY stored in CLASSIFICATION_MODEL_PREDICTIONS.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Query current day's data, insert to stage, add indicators & add model predictions based on already created models.\n",
    "\n",
    "\"\"\"\n",
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# Define the list of stocks\n",
    "stocks = [\"SPY\"]\n",
    "\n",
    "# Just query current day's data\n",
    "# start_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Step 1: Download ticks and add to stage table\n",
    "# data = {}\n",
    "# for stock in stocks:\n",
    "    # Download historical data\n",
    "    # stock_data = atb.download_bar_data(stock, TimeFrame(1, TimeFrameUnit.Minute), start_date, end_date)\n",
    "    # data[stock] = stock_data\n",
    "\n",
    "# Store all downloaded data in the staging table\n",
    "# for stock, stock_data in data.items():\n",
    "#     atb.db_append_no_duplicates('STG_SYMBOL_DATA', stock_data)\n",
    "#     print(f\"Stage table updated with data for {stock}\")\n",
    "\n",
    "# Step 2: Transfer data from the staging table to the modeling staging table\n",
    "for stock in stocks:\n",
    "    atb.classification_modeling_apply('STG_SYMBOL_DATA', stock, \"CLASSIFICATION_MODEL_PREDICTIONS\")\n",
    "    print(f\"Model predictions for {stock} stored in CLASSIFICATION_MODEL_PREDICTIONS.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
