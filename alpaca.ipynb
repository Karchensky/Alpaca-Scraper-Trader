{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpaca Trading Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import TimeFrame, TimeFrameUnit\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import ta\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class AlpacaTradingBot:\n",
    "    \"\"\"\n",
    "\n",
    "    This class can be used to scrape stock data from the Alpaca API, store it locally in a SQLite Database,\n",
    "    add indicators & perform predective analytics.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, keys_file_path='alpaca_keys.txt', base_url='https://paper-api.alpaca.markets', database_path=r'D:\\Scripts\\alpaca\\alpaca_algo_trading\\alpaca_data.db'):\n",
    "        with open(keys_file_path, 'r') as file:\n",
    "            self.api_key = file.readline().strip()\n",
    "            self.api_secret = file.readline().strip()\n",
    "        self.base_url = base_url\n",
    "        self.api = tradeapi.REST(self.api_key, self.api_secret, base_url=base_url)\n",
    "        self.database_path = database_path\n",
    "        self.api_call_count = 0\n",
    "\n",
    "    def download_bar_data(self, stock, timeframe, start_date, end_date):\n",
    "        \"\"\"\n",
    "        \n",
    "        This method will be used to scrape bar data from the Alpaca API. \n",
    "        Provide a ticker symbol, an interval, and a beginning and end date.\n",
    "\n",
    "        \"\"\"\n",
    "        all_data = []\n",
    "\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            # Check if it's a weekend (Saturday or Sunday) & skip, if so\n",
    "            # if  datetime.strptime(current_date, \"%Y-%m-%d\").weekday() >= 5:\n",
    "            #     current_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "            #     continue\n",
    "\n",
    "            # Split the day into two segments: midnight to noon, and noon to end of day because the API can only return 1,000 data points a day\n",
    "            segments = [\n",
    "                (f\"{current_date}T00:00:00Z\", f\"{current_date}T11:59:59Z\"),\n",
    "                (f\"{current_date}T12:00:00Z\", f\"{current_date}T23:59:59Z\")\n",
    "            ]\n",
    "\n",
    "            for start_time, end_time in segments:\n",
    "                try:\n",
    "                    # Get data for the current day segment\n",
    "                    bars = self.api.get_bars(stock, timeframe, start=start_time, end=end_time, limit=1000).df\n",
    "                    self.api_call_count += 1  # Increment the API call counter\n",
    "                    if not bars.empty:\n",
    "                        bars['symbol'] = stock  # Add the stock symbol column\n",
    "                        all_data.append(bars)\n",
    "                        print(f\"Data scraped for {current_date} through {end_time.split('T')[1]}...\")\n",
    "                    else:\n",
    "                        print(f\"No data available for segment: {current_date}: {start_time.split('T')[1]} - {end_time.split('T')[1]}\")\n",
    "                except tradeapi.rest.APIError as e:\n",
    "                    print(f\"API Error: {e}\")\n",
    "                    pause_duration = random.uniform(120, 240)  # Longer pause if an API error occurs\n",
    "                    time.sleep(pause_duration)\n",
    "                    continue\n",
    "                \n",
    "            # Random pause after each scrape.. API rate limits need to be considered. Only a problem for the main original pull to populate the database.\n",
    "            if (datetime.strptime(current_date, \"%Y-%m-%d\") - datetime.strptime(start_date, \"%Y-%m-%d\")).days % 1 == 0:\n",
    "                print(f\"Total API calls made so far: {self.api_call_count}\")\n",
    "                pause_duration = random.uniform(10, 30)\n",
    "                print(f\"Pausing for {pause_duration} after scraping data for {current_date}...\")\n",
    "                print(f\"__________________________________\")\n",
    "                time.sleep(pause_duration)\n",
    "\n",
    "            # Move to the next day\n",
    "            current_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        if all_data:\n",
    "            combined_data = pd.concat(all_data)\n",
    "            combined_data = combined_data.reset_index()  # Ensure the index is reset to have 'timestamp' as a column\n",
    "            print(\"Scraping Bar Data complete for timerange:\", combined_data['timestamp'].min(), \" - \", combined_data['timestamp'].max())\n",
    "            return combined_data[['symbol'] + [col for col in combined_data.columns if col not in ['symbol']]]\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    def calculate_indicators_modeling(self, stock_data):\n",
    "        \"\"\"\n",
    "        This method calculates various indicators on a dataframe tailored for a 1-hour trading strategy.\n",
    "        \"\"\"\n",
    "\n",
    "        # Trend Indicators\n",
    "        stock_data['sma_30'] = ta.trend.sma_indicator(stock_data['CLOSE'], window=30)\n",
    "        # 30-minute SMA (Simple Moving Average) for medium-term trend analysis\n",
    "\n",
    "        stock_data['ema_30'] = ta.trend.ema_indicator(stock_data['CLOSE'], window=30)\n",
    "        # 30-minute EMA for a responsive short to medium-term trend\n",
    "\n",
    "        stock_data['macd'] = ta.trend.macd(stock_data['CLOSE'], window_slow=26, window_fast=12)\n",
    "        stock_data['macd_signal'] = ta.trend.macd_signal(stock_data['CLOSE'], window_slow=26, window_fast=12, window_sign=9)\n",
    "        # Standard MACD (Moving Average Convergence Divergence) settings for capturing trend and momentum changes\n",
    "\n",
    "        # Momentum Indicators\n",
    "        stock_data['rsi'] = ta.momentum.rsi(stock_data['CLOSE'], window=14)\n",
    "        # 14-minute RSI (Relative Strength Index) for overbought/oversold conditions\n",
    "\n",
    "        stock_data['stochastic_k'] = ta.momentum.stoch(stock_data['HIGH'], stock_data['LOW'], stock_data['CLOSE'], window=14, smooth_window=3)\n",
    "        # 14-minute Stochastic Oscillator for short-term momentum\n",
    "\n",
    "        stock_data['cci'] = ta.trend.cci(stock_data['HIGH'], stock_data['LOW'], stock_data['CLOSE'], window=20)\n",
    "        # 20-minute CCI for identifying cyclical trends\n",
    "\n",
    "        # Volatility Indicators\n",
    "        bb_indicator = ta.volatility.BollingerBands(stock_data['CLOSE'], window=20, window_dev=2)\n",
    "        stock_data['bollinger_hband'] = bb_indicator.bollinger_hband()\n",
    "        stock_data['bollinger_lband'] = bb_indicator.bollinger_lband()\n",
    "        stock_data['bollinger_mband'] = bb_indicator.bollinger_mavg()\n",
    "        stock_data['bollinger_width_percent'] = (stock_data['bollinger_hband'] - stock_data['bollinger_lband']) / stock_data['CLOSE']\n",
    "        stock_data['bollinger_position'] = bb_indicator.bollinger_pband()\n",
    "        # Bollinger Bands with standard deviation score for precise volatility measure\n",
    "\n",
    "        stock_data['atr'] = ta.volatility.average_true_range(stock_data['HIGH'], stock_data['LOW'], stock_data['CLOSE'], window=14)\n",
    "        # 14-minute ATR (Average True Range) for short-term volatility\n",
    "\n",
    "        # Volume-Based Indicators\n",
    "        stock_data['obv'] = ta.volume.on_balance_volume(stock_data['CLOSE'], stock_data['VOLUME'])\n",
    "        # OBV (On-Balance Volume) for volume-based trend prediction\n",
    "\n",
    "        # Calculate VWAP for each day\n",
    "        stock_data['vwap'] = stock_data.groupby(stock_data['timestamp_est'].dt.date).apply(\n",
    "            lambda x: (x['CLOSE'] * x['VOLUME']).cumsum() / x['VOLUME'].cumsum()\n",
    "        ).reset_index(level=0, drop=True)\n",
    "\n",
    "        return stock_data\n",
    "\n",
    "    def transfer_stage_modeling(self, input_table, symbol, output_table):\n",
    "        \"\"\"\n",
    "        \n",
    "        This method cleans the scraped raw stage data, adds indicator calculations on it inserts the otuput into a fresh table.\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Read the entire table\n",
    "            input_data = pd.read_sql(f\"SELECT distinct * FROM {input_table} WHERE SYMBOL = '{symbol}'\" , conn)\n",
    "\n",
    "            # Check if the input data is empty\n",
    "            if input_data.empty:\n",
    "                raise ValueError(f\"No data found for symbol {symbol} in table {input_table}\")\n",
    "\n",
    "            # Convert 'TIMESTAMP' to datetime and localize it to UTC\n",
    "            input_data['timestamp_utc'] = pd.to_datetime(input_data['TIMESTAMP'], utc=True)\n",
    "            input_data['timestamp_est'] = input_data['timestamp_utc'].dt.tz_convert('US/Eastern')\n",
    "            input_data['trading_hours_ind'] = (input_data['timestamp_est'].dt.time >= datetime.strptime('09:30', '%H:%M').time()) & \\\n",
    "                                            (input_data['timestamp_est'].dt.time <= datetime.strptime('16:00', '%H:%M').time())\n",
    "            input_data['trading_hours_ind_ext'] = (input_data['timestamp_est'].dt.time >= datetime.strptime('10:30', '%H:%M').time()) & \\\n",
    "                                                    (input_data['timestamp_est'].dt.time <= datetime.strptime('15:00', '%H:%M').time())\n",
    "\n",
    "            # Calculate indicators\n",
    "            indicators_data = self.calculate_indicators_modeling(input_data)\n",
    "\n",
    "            # Calculate additional features\n",
    "            indicators_data['date'] = indicators_data['timestamp_est'].dt.date\n",
    "            indicators_data['day_of_week'] = indicators_data['timestamp_est'].dt.dayofweek\n",
    "            indicators_data['hour_of_day'] = indicators_data['timestamp_est'].dt.hour\n",
    "            indicators_data['time_of_day'] = indicators_data['timestamp_est'].dt.time\n",
    "\n",
    "            # Movement in-day & in-week as percentage\n",
    "            indicators_data['daily_opening_price'] = indicators_data.groupby(indicators_data['timestamp_est'].dt.date)['OPEN'].transform('first')\n",
    "            indicators_data['daily_movement_since_open'] = (indicators_data['CLOSE'] - indicators_data['daily_opening_price']) / indicators_data['daily_opening_price']\n",
    "\n",
    "            indicators_data['weekly_opening_price'] = indicators_data.groupby(indicators_data['timestamp_est'].dt.isocalendar().week)['OPEN'].transform('first')\n",
    "            indicators_data['weekly_movement_since_open'] = (indicators_data['CLOSE'] - indicators_data['weekly_opening_price']) / indicators_data['weekly_opening_price']\n",
    "\n",
    "            # Calculate percentage movement over the last 20 periods (20 minutes)\n",
    "            indicators_data['pct_movement_20_periods'] = indicators_data['CLOSE'].pct_change(periods=20)\n",
    "\n",
    "            # Calculate percentage movement over the last 60 periods (1 hour)\n",
    "            indicators_data['pct_movement_60_periods'] = indicators_data['CLOSE'].pct_change(periods=60)\n",
    "\n",
    "            # Targets\n",
    "            indicators_data['target_1_hour'] = indicators_data['CLOSE'].shift(-60)\n",
    "            # Target for the closing price 1 hour from the current time, aligning with the 1-hour trading strategy\n",
    "            # Calculate movement_pct and signal\n",
    "            indicators_data['target_movement_pct'] = ((indicators_data['target_1_hour'] - indicators_data['CLOSE']).abs() / indicators_data['CLOSE']) * 100\n",
    "            indicators_data['target_movement_signal'] = indicators_data.apply(\n",
    "                lambda row: 1 if (row['target_1_hour'] - row['CLOSE']) > 0.001 * row['CLOSE'] else \n",
    "                            (-1 if (row['target_1_hour'] - row['CLOSE']) <= -0.001 * row['CLOSE'] else 0), \n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Drop some columns and reorder\n",
    "            indicators_data = indicators_data.drop(columns=['TIMESTAMP', 'timestamp_utc'])\n",
    "            indicators_data = indicators_data[['SYMBOL', 'timestamp_est', 'date', 'trading_hours_ind', 'trading_hours_ind_ext'] + [col for col in indicators_data.columns if col not in ['SYMBOL', 'timestamp_est', 'date', 'trading_hours_ind', 'trading_hours_ind_ext']]]\n",
    "\n",
    "            # Upload data to output table\n",
    "            self.db_append(output_table, indicators_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing symbol {symbol}: {e}\")\n",
    "    \n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        return indicators_data\n",
    "\n",
    "    def db_write(self, sql_statement):\n",
    "        \"\"\"\n",
    "\n",
    "        Basic functionality to execute an SQL statement against our database\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database & create a cursor object\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Execute the SQL statement\n",
    "        cur.execute(sql_statement)\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def db_append(self, table_name, data_frame):\n",
    "        \"\"\"\n",
    "\n",
    "        Basic functionality to append/insert a dataframe into a specified table in our database\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        # Append our dataframe into our table\n",
    "        data_frame.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "    def db_append_no_duplicates(self, table_name, data_frame):\n",
    "        \"\"\"\n",
    "\n",
    "        Variation of db_append that makes sure we aren't inserting duplicates by first checking against the primary key of the table records are being inserted into\n",
    "\n",
    "        \"\"\"\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Get the primary key column names\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        table_info = cursor.fetchall()\n",
    "        primary_key_columns = [column[1] for column in table_info if column[5] == 1]\n",
    "\n",
    "        # If no primary key columns found, fall back to db_append method\n",
    "        if not primary_key_columns:\n",
    "            data_frame.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "\n",
    "            # Commit the changes & close the connection\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return\n",
    "\n",
    "        # Construct the SELECT statement to fetch existing primary keys from the table\n",
    "        existing_keys_query = f\"SELECT {', '.join(primary_key_columns)} FROM {table_name}\"\n",
    "        existing_keys_df = pd.read_sql(existing_keys_query, conn)\n",
    "\n",
    "        # Construct the composite primary key from the existing keys DataFrame\n",
    "        existing_keys_df['COMPOSITE_KEY'] = existing_keys_df.apply(lambda row: tuple(row), axis=1)\n",
    "        existing_keys = set(existing_keys_df['COMPOSITE_KEY'])\n",
    "\n",
    "        # Construct the composite primary key for new records\n",
    "        data_frame['COMPOSITE_KEY'] = data_frame.apply(lambda row: tuple(row[primary_key_columns]), axis=1)\n",
    "\n",
    "        # Filter out rows with primary keys that already exist in the table\n",
    "        data_frame_new_records = data_frame[~data_frame['COMPOSITE_KEY'].isin(existing_keys)]\n",
    "\n",
    "        # Drop the composite key column\n",
    "        data_frame_new_records = data_frame_new_records.drop(columns=['COMPOSITE_KEY'])\n",
    "\n",
    "        # Append only the new rows into our table\n",
    "        data_frame_new_records.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema DDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Schema DDL - creates the tables in our database that we need for storing the raw scraped data & analyzing it.\n",
    "\n",
    "\"\"\"\n",
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# This table will house the raw scraped data\n",
    "sql_statement = atb.db_write('''DROP TABLE IF EXISTS STG_SYMBOL_DATA''')\n",
    "sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS STG_SYMBOL_DATA (\n",
    "    SYMBOL                  TEXT        PRIMARY_KEY,\n",
    "    TIMESTAMP               TIMESTAMP   PRIMARY_KEY,\n",
    "    CLOSE                   DECIMAL,\n",
    "    HIGH                    DECIMAL,\n",
    "    LOW                     DECIMAL,\n",
    "    TRADE_COUNT             INTEGER,\n",
    "    OPEN                    DECIMAL,\n",
    "    VOLUME                  INTEGER,\n",
    "    VWAP                    DECIMAL\n",
    "\n",
    ")\n",
    "''')\n",
    "\n",
    "sql_statement = atb.db_write('''DROP TABLE IF EXISTS SYMBOL_DATA_MODELING''')\n",
    "sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS SYMBOL_DATA_MODELING (\n",
    "    SYMBOL                      TEXT            PRIMARY_KEY,\n",
    "    TIMESTAMP_EST               TIMESTAMP       PRIMARY_KEY,\n",
    "    DATE                        DATE,\n",
    "    TRADING_HOURS_IND           BOOLEAN,\n",
    "    TRADING_HOURS_IND_EXT       BOOLEAN,\n",
    "    CLOSE                       DECIMAL,\n",
    "    HIGH                        DECIMAL,\n",
    "    LOW                         DECIMAL,\n",
    "    TRADE_COUNT                 INTEGER,\n",
    "    OPEN                        DECIMAL,\n",
    "    VOLUME                      INTEGER,\n",
    "    VWAP                        DECIMAL,\n",
    "    RSI                         DECIMAL,\n",
    "    SMA_30                      DECIMAL,\n",
    "    EMA_30                      DECIMAL,\n",
    "    MACD                        DECIMAL,\n",
    "    MACD_SIGNAL                 DECIMAL,\n",
    "    STOCHASTIC_K                DECIMAL,\n",
    "    CCI                         DECIMAL,\n",
    "    BOLLINGER_HBAND             DECIMAL,\n",
    "    BOLLINGER_LBAND             DECIMAL,\n",
    "    BOLLINGER_MBAND             DECIMAL,\n",
    "    BOLLINGER_WIDTH_PERCENT     DECIMAL,\n",
    "    BOLLINGER_POSITION          DECIMAL,\n",
    "    ATR                         DECIMAL,\n",
    "    OBV                         INTEGER,\n",
    "    TARGET_1_HOUR               DECIMAL,\n",
    "    TARGET_MOVEMENT_PCT         DECIMAL,        \n",
    "    TARGET_MOVEMENT_SIGNAL      INTEGER,\n",
    "    DAY_OF_WEEK                 INTEGER,\n",
    "    HOUR_OF_DAY                 INTEGER,\n",
    "    TIME_OF_DAY                 TIME,\n",
    "    DAILY_OPENING_PRICE         DECIMAL,\n",
    "    DAILY_MOVEMENT_SINCE_OPEN   DECIMAL,\n",
    "    WEEKLY_OPENING_PRICE        DECIMAL,\n",
    "    WEEKLY_MOVEMENT_SINCE_OPEN  DECIMAL,\n",
    "    PCT_MOVEMENT_20_PERIODS     DECIMAL,\n",
    "    PCT_MOVEMENT_60_PERIODS     DECIMAL\n",
    "\n",
    ")\n",
    "''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Historical Data to Stage & Prepare for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped for 2020-01-01 through 11:59:59Z...\n",
      "No data available for segment: 2020-01-01: 12:00:00Z - 23:59:59Z\n",
      "Total API calls made so far: 2\n",
      "Pausing for 25.231668200102845 after scraping data for 2020-01-01...\n",
      "__________________________________\n",
      "Data scraped for 2020-01-02 through 11:59:59Z...\n",
      "Data scraped for 2020-01-02 through 23:59:59Z...\n",
      "Total API calls made so far: 4\n",
      "Pausing for 11.816046099309968 after scraping data for 2020-01-02...\n",
      "__________________________________\n",
      "Scraping Bar Data complete for timerange: 2020-01-01 00:01:00+00:00  -  2020-01-02 23:57:00+00:00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Query & store historical data in out stage table. Only need to run this on the first run.\n",
    "\n",
    "\"\"\"\n",
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# Define the list of stocks\n",
    "stocks = [\"SPY\"]\n",
    "\n",
    "# Define the time range for historical data\n",
    "start_date = \"2020-01-01\"\n",
    "# end_date = \"2020-01-02\"\n",
    "end_date = datetime.now().strftime(\"%Y-%m-%d\") - timedelta(days=1).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Step 1: Download and store historical data in the staging table\n",
    "data = {}\n",
    "for stock in stocks:\n",
    "    # Download historical data\n",
    "    stock_data = atb.download_bar_data(stock, TimeFrame(1, TimeFrameUnit.Minute), start_date, end_date)\n",
    "    data[stock] = stock_data\n",
    "\n",
    "# Store all downloaded data in the staging table\n",
    "for stock, stock_data in data.items():\n",
    "    atb.db_append('STG_SYMBOL_DATA', stock_data)\n",
    "\n",
    "# Step 2: Transfer data from the staging table to the modeling table\n",
    "for stock in stocks:\n",
    "    atb.transfer_stage_modeling('STG_SYMBOL_DATA', stock, 'SYMBOL_DATA_MODELING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
