{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpaca Trading Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import TimeFrame, TimeFrameUnit\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import ta\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class AlpacaTradingBot:\n",
    "    def __init__(self, keys_file_path='alpaca_keys.txt', base_url='https://paper-api.alpaca.markets', database_path=r'D:\\Scripts\\alpaca\\alpaca_algo_trading\\alpaca_data.db'):\n",
    "        with open(keys_file_path, 'r') as file:\n",
    "            self.api_key = file.readline().strip()\n",
    "            self.api_secret = file.readline().strip()\n",
    "        self.base_url = base_url\n",
    "        self.api = tradeapi.REST(self.api_key, self.api_secret, base_url=base_url)\n",
    "        self.database_path = database_path\n",
    "\n",
    "\n",
    "    def download_bar_data(self, stock, timeframe, start_date, end_date):\n",
    "        all_data = []\n",
    "\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            # Check if it's a weekend (Saturday or Sunday) & skip, if so\n",
    "            if  datetime.strptime(current_date, \"%Y-%m-%d\").weekday() >= 5:\n",
    "                current_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "                continue\n",
    "\n",
    "            # Get data for the current day\n",
    "            bars = self.api.get_bars(stock, timeframe, start=current_date, end=current_date, limit=1000).df\n",
    "            if bars.empty:\n",
    "                continue\n",
    "\n",
    "            bars['symbol'] = stock  # Add the stock symbol column\n",
    "            all_data.append(bars)\n",
    "\n",
    "            # Move to the next day\n",
    "            current_date = (datetime.strptime(current_date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            # Random pause after each scrape.. API rate limits need to be considered. Only a problem for the main original pull to populate the database.\n",
    "            if (datetime.strptime(current_date, \"%Y-%m-%d\") - datetime.strptime(start_date, \"%Y-%m-%d\")).days % 5 == 0:\n",
    "                pause_duration = random.uniform(3, 5)\n",
    "                time.sleep(pause_duration)\n",
    "                print(f\"Scraped a 1 day increment, pausing for {pause_duration} seconds...\")\n",
    "\n",
    "        if all_data:\n",
    "            combined_data = pd.concat(all_data)\n",
    "            combined_data = combined_data.reset_index()  # Ensure the index is reset to have 'timestamp' as a column\n",
    "            return combined_data[['symbol'] + [col for col in combined_data.columns if col not in ['symbol']]]\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    # This method calculates various indicators on a dataframe\n",
    "    def calculate_indicators(self, stock_data):\n",
    "        stock_data['rsi'] = ta.momentum.rsi(stock_data['CLOSE'], window=14)\n",
    "        stock_data['sma_50'] = ta.trend.sma_indicator(stock_data['CLOSE'], window=50)\n",
    "        stock_data['sma_200'] = ta.trend.sma_indicator(stock_data['CLOSE'], window=200)\n",
    "        stock_data['bollinger_hband'] = ta.volatility.bollinger_hband(stock_data['CLOSE'], window=20, window_dev=2)\n",
    "        stock_data['bollinger_lband'] = ta.volatility.bollinger_lband(stock_data['CLOSE'], window=20, window_dev=2)\n",
    "        return stock_data\n",
    "\n",
    "    # This method cleans the scraped data & runs indicator calculations on it\n",
    "    def transfer_stage(self, input_table, output_table):\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database & create a cursor object\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        # Read the entire table\n",
    "        input_data = pd.read_sql(f\"SELECT * FROM {input_table}\", conn)\n",
    "\n",
    "        # Convert 'TIMESTAMP' to datetime and localize it to UTC\n",
    "        input_data['timestamp_utc'] = pd.to_datetime(input_data['TIMESTAMP'], utc=True)\n",
    "        input_data['timestamp_est'] = input_data['timestamp_utc'].dt.tz_convert('US/Eastern')\n",
    "        input_data['trading_hours_ind'] = (input_data['timestamp_est'].dt.time >= datetime.strptime('09:30', '%H:%M').time()) & \\\n",
    "                                    (input_data['timestamp_est'].dt.time <= datetime.strptime('16:00', '%H:%M').time())\n",
    "\n",
    "        # Calculate indicators\n",
    "        indicators_data = self.calculate_indicators(input_data)\n",
    "\n",
    "        # Drop some columns a reorder\n",
    "        indicators_data = indicators_data.drop(columns=['TIMESTAMP','timestamp_utc'])\n",
    "        indicators_data[['symbol','timestamp_est','trading_hours_ind'] + [col for col in indicators_data.columns if col not in ['symbol','timestamp_est','trading_hours_ind']]]\n",
    "\n",
    "        # Upload data to output table\n",
    "        self.db_append_no_duplicates(output_table, indicators_data)\n",
    "\n",
    "        return indicators_data\n",
    "\n",
    "    # This method executes a SQL statement against our database.\n",
    "    def db_write(self, sql_statement):\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database & create a cursor object\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Execute the SQL statement\n",
    "        cur.execute(sql_statement)\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    # This method appends records to our database.\n",
    "    def db_append(self, table_name, data_frame):\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        # Append our dataframe into our table\n",
    "        data_frame.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    # Modification of db_append that checks primary key & ensures we aren't inserting a duplicate value\n",
    "    def db_append_no_duplicates(self, table_name, data_frame):\n",
    "        db_path = self.database_path\n",
    "\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Get the primary key column names\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        table_info = cursor.fetchall()\n",
    "        primary_key_columns = [column[1] for column in table_info if column[5] == 1]\n",
    "\n",
    "        # If no primary key columns found, fall back to db_append method\n",
    "        if not primary_key_columns:\n",
    "            data_frame.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "\n",
    "            # Commit the changes & close the connection\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return\n",
    "\n",
    "        # Construct the SELECT statement to fetch existing primary keys from the table\n",
    "        existing_keys_query = f\"SELECT {', '.join(primary_key_columns)} FROM {table_name}\"\n",
    "        existing_keys_df = pd.read_sql(existing_keys_query, conn)\n",
    "\n",
    "        # Construct the composite primary key from the existing keys DataFrame\n",
    "        existing_keys_df['COMPOSITE_KEY'] = existing_keys_df.apply(lambda row: tuple(row), axis=1)\n",
    "        existing_keys = set(existing_keys_df['COMPOSITE_KEY'])\n",
    "\n",
    "        # Construct the composite primary key for new records\n",
    "        data_frame['COMPOSITE_KEY'] = data_frame.apply(lambda row: tuple(row[primary_key_columns]), axis=1)\n",
    "\n",
    "        # Filter out rows with primary keys that already exist in the table\n",
    "        data_frame_new_records = data_frame[~data_frame['COMPOSITE_KEY'].isin(existing_keys)]\n",
    "\n",
    "        # Drop the composite key column\n",
    "        data_frame_new_records = data_frame_new_records.drop(columns=['COMPOSITE_KEY'])\n",
    "\n",
    "        # Append only the new rows into our table\n",
    "        data_frame_new_records.to_sql(table_name, conn, schema='main', if_exists='append', index=False)\n",
    "\n",
    "        # Commit the changes & close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema DDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema DDL\n",
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# This table will house the raw scraped data\n",
    "sql_statement = atb.db_write('''DROP TABLE IF EXISTS STG_SYMBOL_DATA''')\n",
    "sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS STG_SYMBOL_DATA (\n",
    "    SYMBOL                  TEXT        PRIMARY_KEY,\n",
    "    TIMESTAMP               TIMESTAMP   PRIMARY_KEY,\n",
    "    CLOSE                   DECIMAL,\n",
    "    HIGH                    DECIMAL,\n",
    "    LOW                     DECIMAL,\n",
    "    TRADE_COUNT             INTEGER,\n",
    "    OPEN                    DECIMAL,\n",
    "    VOLUME                  INTEGER,\n",
    "    VWAP                    DECIMAL\n",
    "\n",
    ")\n",
    "''')\n",
    "\n",
    "# This table will house the scraped data + computed indicators\n",
    "sql_statement = atb.db_write('''DROP TABLE IF EXISTS SYMBOL_DATA''')\n",
    "sql_statement = atb.db_write('''CREATE TABLE IF NOT EXISTS SYMBOL_DATA (\n",
    "    SYMBOL                  TEXT        PRIMARY_KEY,\n",
    "    TIMESTAMP_EST           TIMESTAMP   PRIMARY_KEY,\n",
    "    TRADING_HOURS_IND       BOOLEAN,                   \n",
    "    CLOSE                   DECIMAL,\n",
    "    HIGH                    DECIMAL,\n",
    "    LOW                     DECIMAL,\n",
    "    TRADE_COUNT             INTEGER,\n",
    "    OPEN                    DECIMAL,\n",
    "    VOLUME                  INTEGER,\n",
    "    VWAP                    DECIMAL,\n",
    "    RSI                     DECIMAL,\n",
    "    SMA_50                  DECIMAL,\n",
    "    SMA_200                 DECIMAL,\n",
    "    BOLLINGER_HBAND         DECIMAL,\n",
    "    BOLLINGER_LBAND         DECIMAL\n",
    ")\n",
    "''')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped a 5 day increment, pausing for 8.823637889260677 seconds...\n",
      "Scraped a 5 day increment, pausing for 7.297139144290808 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SPY/bars 3 more time(s)...\n",
      "sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SPY/bars 3 more time(s)...\n",
      "sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SPY/bars 3 more time(s)...\n",
      "sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SPY/bars 3 more time(s)...\n",
      "sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SPY/bars 3 more time(s)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Scripts\\alpaca\\.venv\\Lib\\site-packages\\alpaca_trade_api\\rest.py:243\u001b[0m, in \u001b[0;36mREST._one_request\u001b[1;34m(self, method, url, opts, retry)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# retry if we hit Rate Limit\u001b[39;00m\n",
      "File \u001b[1;32md:\\Scripts\\alpaca\\.venv\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://data.alpaca.markets/v2/stocks/SPY/bars?timeframe=5Min&adjustment=raw&start=2020-01-20&end=2020-01-20&limit=1000",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRetryException\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32md:\\Scripts\\alpaca\\.venv\\Lib\\site-packages\\alpaca_trade_api\\rest.py:222\u001b[0m, in \u001b[0;36mREST._request\u001b[1;34m(self, method, path, data, base_url, api_version)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RetryException:\n",
      "File \u001b[1;32md:\\Scripts\\alpaca\\.venv\\Lib\\site-packages\\alpaca_trade_api\\rest.py:247\u001b[0m, in \u001b[0;36mREST._one_request\u001b[1;34m(self, method, url, opts, retry)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m retry_codes \u001b[38;5;129;01mand\u001b[39;00m retry \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryException()\n\u001b[0;32m    248\u001b[0m raise_api_error(resp, http_error)\n",
      "\u001b[1;31mRetryException\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stock \u001b[38;5;129;01min\u001b[39;00m stocks:\n\u001b[1;32m---> 14\u001b[0m     stock_data \u001b[38;5;241m=\u001b[39m \u001b[43matb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_bar_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTimeFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTimeFrameUnit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMinute\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     data[stock] \u001b[38;5;241m=\u001b[39m stock_data\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Store data locally\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[104], line 31\u001b[0m, in \u001b[0;36mAlpacaTradingBot.download_bar_data\u001b[1;34m(self, stock, timeframe, start_date, end_date)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Get data for the current day\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m bars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_bars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdf\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bars\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Scripts\\alpaca\\.venv\\Lib\\site-packages\\alpaca_trade_api\\rest.py:735\u001b[0m, in \u001b[0;36mREST.get_bars\u001b[1;34m(self, symbol, timeframe, start, end, adjustment, limit, feed, asof, sort)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bars\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    725\u001b[0m              symbol: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m    726\u001b[0m              timeframe: TimeFrame,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    733\u001b[0m              sort: Optional[Sort] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    734\u001b[0m              ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BarsV2:\n\u001b[1;32m--> 735\u001b[0m     bars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_bars_iter(symbol,\n\u001b[0;32m    736\u001b[0m                                    timeframe,\n\u001b[0;32m    737\u001b[0m                                    start,\n\u001b[0;32m    738\u001b[0m                                    end,\n\u001b[0;32m    739\u001b[0m                                    adjustment,\n\u001b[0;32m    740\u001b[0m                                    limit,\n\u001b[0;32m    741\u001b[0m                                    feed\u001b[38;5;241m=\u001b[39mfeed,\n\u001b[0;32m    742\u001b[0m                                    asof\u001b[38;5;241m=\u001b[39masof,\n\u001b[0;32m    743\u001b[0m                                    sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    744\u001b[0m                                    raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BarsV2(bars)\n",
      "File \u001b[1;32md:\\Scripts\\alpaca\\.venv\\Lib\\site-packages\\alpaca_trade_api\\rest.py:718\u001b[0m, in \u001b[0;36mREST.get_bars_iter\u001b[1;34m(self, symbol, timeframe, start, end, adjustment, limit, feed, asof, sort, raw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bars_iter\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    698\u001b[0m                   symbol: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m    699\u001b[0m                   timeframe: TimeFrame,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    706\u001b[0m                   sort: Optional[Sort] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m                   raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BarIterator:\n\u001b[0;32m    708\u001b[0m     bars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_get(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbars\u001b[39m\u001b[38;5;124m'\u001b[39m, symbol,\n\u001b[0;32m    709\u001b[0m                           timeframe\u001b[38;5;241m=\u001b[39mtimeframe,\n\u001b[0;32m    710\u001b[0m                           adjustment\u001b[38;5;241m=\u001b[39madjustment,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    716\u001b[0m                           sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    717\u001b[0m                           )\n\u001b[1;32m--> 718\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbars\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbar\u001b[49m\n",
      "File \u001b[1;32md:\\Scripts\\alpaca\\.venv\\Lib\\site-packages\\alpaca_trade_api\\rest.py:594\u001b[0m, in \u001b[0;36mREST._data_get\u001b[1;34m(self, endpoint, symbol_or_symbols, api_version, endpoint_base, resp_grouped_by_symbol, page_limit, feed, asof, loc, **kwargs)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m endpoint:\n\u001b[0;32m    593\u001b[0m     path \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 594\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resp_grouped_by_symbol:\n\u001b[0;32m    597\u001b[0m     k \u001b[38;5;241m=\u001b[39m endpoint \u001b[38;5;129;01mor\u001b[39;00m endpoint_base\n",
      "File \u001b[1;32md:\\Scripts\\alpaca\\.venv\\Lib\\site-packages\\alpaca_trade_api\\rest.py:274\u001b[0m, in \u001b[0;36mREST.data_get\u001b[1;34m(self, path, data, feed, api_version)\u001b[0m\n\u001b[0;32m    272\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    273\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m feed\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Scripts\\alpaca\\.venv\\Lib\\site-packages\\alpaca_trade_api\\rest.py:229\u001b[0m, in \u001b[0;36mREST._request\u001b[1;34m(self, method, path, data, base_url, api_version)\u001b[0m\n\u001b[0;32m    224\u001b[0m retry_wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_wait\n\u001b[0;32m    225\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msleep \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds and retrying \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m more time(s)...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    228\u001b[0m         retry_wait, url, retry))\n\u001b[1;32m--> 229\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(retry_wait)\n\u001b[0;32m    230\u001b[0m retry \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "atb = AlpacaTradingBot()\n",
    "\n",
    "# Define the list of stocks\n",
    "stocks = [\"SPY\"]\n",
    "\n",
    "# Define the time range for historical data\n",
    "start_date = \"2020-01-01\"\n",
    "# end_date = \"2020-01-15\"\n",
    "end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Download historical data\n",
    "data = {}\n",
    "for stock in stocks:\n",
    "    stock_data = atb.download_bar_data(stock, TimeFrame(5, TimeFrameUnit.Minute), start_date, end_date)\n",
    "    data[stock] = stock_data\n",
    "\n",
    "# Store data locally\n",
    "for stock, stock_data in data.items():\n",
    "    atb.db_append_no_duplicates('STG_SYMBOL_DATA', stock_data)\n",
    "\n",
    "# atb.transfer_stage('STG_SYMBOL_DATA','SYMBOL_DATA')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
